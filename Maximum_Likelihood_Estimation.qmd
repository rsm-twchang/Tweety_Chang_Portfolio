---
title: "Blueprinty Case Study"
format: html
author: "Tweety"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---

## Introduction

[Blueprinty](https://cedreo.com/floor-plans/blueprint-maker/?utm_source=google&utm_medium=ppc&utm_campaign=ur_g_s_us_nb_floorplan&utm_content=blueprint_maker&utm_term=blueprint%20software&hsa_cam=14665050709&hsa_grp=139247217835&hsa_src=g&hsa_acc=6133015900&hsa_ver=3&hsa_ad=608477980296&hsa_tgt=kwd-510681492&hsa_kw=blueprint%20software&hsa_mt=e&hsa_net=adwords&gad_source=1&gad_campaignid=14665050709&gbraid=0AAAAADDYjZHKIFo4OUxk1pYoWm-0VeD_s&gclid=Cj0KCQiA7rDMBhCjARIsAGDBuEAKQsXUPEdiVMgsJIRjOqUONUA1ywGjZu_IEoYDQ6wlEgoxkeuFLx0aAkS4EALw_wcB)  is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.

Instead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users.
---

## Objective

The objective of this project is to assess whether engineering firms that use Blueprinty’s software produce more patents than firms that do not, after accounting for firm age and regional location. The analysis aims to test whether the available data support Blueprinty’s claim that its software is associated with stronger patent performance.
---

## Data Description
```{python}
#| warning: false
#| echo: false
import warnings
warnings.filterwarnings("ignore")
```

```{python}
#| echo: false
#| output: false
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Read the data 
df = pd.read_csv('blueprinty.csv')

# Check structure of the data
df.head()
```

```{python}
#| echo: false
#| output: false
# Calculate mean number of patents for each group (assuming 'iscustomer' is the same as 'uses_blueprinty')
mean_patents = df.groupby('iscustomer')['patents'].mean().reset_index(name='MeanPatents')
# Print the results
print(mean_patents)
```

### Variables

| Variable   | Description |
|-----------|-------------|
| `patents`  | Number of patents awarded in the last 5 years (count outcome) |
| `region`   | Firm geographic region (categorical: Midwest, Southwest, Northwest, Northeast, etc.) |
| `age`      | Years since incorporation (continuous) |
| `iscustomer` | Blueprinty software user indicator (1 = customer, 0 = non-customer) |

### Descriptive comparison: patents by customer status

| Customer status | Group        | Mean patents (5 years) |
|---------------:|--------------|------------------------:|
| 0              | Non-customer | 3.473                   |
| 1              | Customer     | 4.133                   |

Blueprinty customers have a higher average number of patents awarded over the past five years (4.133) than non-customers (3.473). This difference is descriptive and does not account for other factors such as firm age or region, which are addressed in the regression analysis.

```{python}
#| echo: false
#| output: false
# Plot histograms to compare number of patents
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='patents', hue='iscustomer', multiple='dodge', binwidth=1)
plt.title('Distribution of Patents by Usage of Blueprinty Software')
plt.xlabel('Number of Patents')
plt.ylabel('Frequency')
plt.legend(title='Uses Blueprinty')
plt.tight_layout()
plt.show()
```

**Distribution of Patent Counts by Blueprinty Usage**
![Figure 1. Distribution of Patent Counts by Blueprinty Usage](fig1_blueprinty_patent_hist.png){#fig-blueprinty-patent-hist}

The histogram shows a right-skewed distribution of patent counts, which is typical for count outcomes. Firms using Blueprinty’s software show a modest rightward shift, suggesting higher patent counts are somewhat more common among customers.

Descriptively, Blueprinty customers average 4.133 patents over five years versus 3.473 for non-customers. This comparison is correlational: customers are not randomly selected, and differences in firm age or region may explain part of the gap. The regression analysis tests whether the association remains after controlling for these factors.

```{python}
#| echo: false
#| output: false
# Group summary statistics for age
age_summary = df.groupby('iscustomer')['age'].describe()
print("\nAge Summary by Customer Status:")
age_summary
```
**Firm age by Blueprinty customer status**

| iscustomer | N    | Mean age | Median age | SD   |
|----------:|-----:|---------:|-----------:|-----:|
| 0         | 1019 | 26.10    | 25.50      | 6.95 |
| 1         | 481  | 26.90    | 26.50      | 7.81 |

Customer and non-customer firms have similar age distributions: customers are slightly older on average (26.90 vs 26.10 years) and have a slightly higher median age (26.5 vs 25.5). Age is therefore included as a control in the regression.

**Region by Blueprinty customer status (counts)**

| Region     | Non-customer (0) | Customer (1) | Total |
|-----------|------------------:|-------------:|------:|
| Midwest   | 187               | 37           | 224   |
| Northeast | 273               | 328          | 601   |
| Northwest | 158               | 29           | 187   |
| South     | 156               | 35           | 191   |
| Southwest | 245               | 52           | 297   |
| Total     | 1019              | 481          | 1500  |

Customer status varies by region, with a large share of customers located in the Northeast. Because region is related to adoption, it is controlled for in the regression to reduce confounding.
```{python}
#| echo: false
#| output: false
# Cross-tabulation of region by customer status
region_counts = pd.crosstab(df['region'], df['iscustomer'], margins=True)
print("\nRegion Distribution by Customer Status:")
region_counts
```
------

## Poisson Model Specification and Estimation

We model the number of patents awarded to each firm over a fixed five-year period. Because **patents** is a nonnegative count outcome, a Poisson model provides a natural starting point.

Let \(Y_i\) denote the number of patents awarded to firm \(i\) in the last five years. We assume:

\[
Y_i \sim \text{Poisson}(\lambda)
\]

where \(\lambda\) is the expected number of patents per firm over the five-year window.

For a sample of \(n\) independent firms with observed counts \(y_1, y_2, \ldots, y_n\), the log-likelihood function is:

\[
\ell(\lambda \mid y_1,\ldots,y_n) 
= -n\lambda + \left(\sum_{i=1}^n y_i\right)\log(\lambda) - \sum_{i=1}^n \log(y_i!)
\]

We estimate \(\lambda\) using maximum likelihood (MLE). This baseline model provides a benchmark before adding firm characteristics (e.g., Blueprinty usage, firm age, and region) in a regression framework.

```{python}
#| echo: false
#| output: false
from scipy.special import gammaln  

def poisson_loglikelihood(lmbda, Y):
    """
    Computes the log-likelihood of a Poisson model.
    
    Parameters:
    - lmbda: scalar or array-like (same length as Y), expected rate parameter(s)
    - Y: array-like, observed count data

    Returns:
    - log-likelihood value (scalar)
    """
    lmbda = np.asarray(lmbda)
    Y = np.asarray(Y)
    
    # Ensure shape compatibility
    if np.isscalar(lmbda):
        lmbda = np.full_like(Y, lmbda, dtype=np.float64)
    
    # Compute log-likelihood
    loglik = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))
    return loglik
```

```{python}
#| echo: false
#| output: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('blueprinty.csv')
Y = df['patents'].values  # observed count data
# Evaluate log-likelihood over a range of lambda values
lambda_values = np.linspace(0.1, 10, 200)  # avoid zero to prevent log(0)
loglik_values = [poisson_loglikelihood(lmb, Y) for lmb in lambda_values]

# Plotting
plt.figure(figsize=(8, 5))
plt.plot(lambda_values, loglik_values, label='Log-Likelihood')
plt.axvline(np.mean(Y), color='red', linestyle='--', label='Sample Mean (MLE)')
plt.title('Poisson Log-Likelihood vs. Lambda')
plt.xlabel('Lambda')
plt.ylabel('Log-Likelihood')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

### Poisson log-likelihood and the MLE

**Poisson Log-Likelihood vs. \(\lambda\)**  
![Figure 2. Poisson Log-Likelihood vs. Lambda](fig2_poisson_loglik_vs_lambda.png){#fig-poisson-loglik}

This figure plots the Poisson log-likelihood as a function of \(\lambda\) for the observed patent counts. The curve reaches its maximum at the sample mean (shown by the dashed vertical line), which is the maximum likelihood estimate under the Poisson model:
\[
\hat{\lambda} = \bar{Y}.
\]

To estimate the Poisson rate parameter \(\lambda\), we use maximum likelihood estimation (MLE). For independent observations \(y_1,\ldots,y_n\), the Poisson log-likelihood is:

\[
\ell(\lambda) = -n\lambda + \left(\sum_{i=1}^n y_i\right)\log(\lambda) - \sum_{i=1}^n \log(y_i!)
\]

Taking the derivative and setting it to zero:

\[
\frac{\partial \ell}{\partial \lambda} = -n + \frac{\sum_{i=1}^n y_i}{\lambda} = 0
\quad \Rightarrow \quad
\hat{\lambda} = \frac{1}{n}\sum_{i=1}^n y_i = \bar{y}.
\]

Thus, the MLE for \(\lambda\) is the sample mean.

A numerical maximization of the Poisson log-likelihood gives \(\hat{\lambda}=3.6847\), which matches the sample mean \(\bar{y}=3.6847\) (up to rounding), as expected for a Poisson model.

```{python}
#| echo: false
#| output: false
from scipy.optimize import minimize
from scipy.special import gammaln

# Define the negative log-likelihood function
def neg_log_likelihood(lmbda, Y):
    lmbda = lmbda[0]  # Extract scalar from array
    if lmbda <= 0:
        return np.inf
    return -np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))
# Initial guess (sample mean)
initial_lambda = np.array([np.mean(Y)])
# Perform the optimization
result = minimize(fun=neg_log_likelihood, x0=initial_lambda, args=(Y,), method='BFGS')
# Extract results
lambda_mle = result.x[0]
# Print the MLE
print(f"The MLE of lambda is: {lambda_mle:.4f}")
print(np.mean(Y), lambda_mle)
```
------

## Poisson Regression Model

Next, we model patent counts as a function of firm characteristics using a Poisson regression. Let \(Y_i\) be the number of patents awarded to firm \(i\) over the last five years:

\[
Y_i \sim \text{Poisson}(\lambda_i), \qquad \lambda_i = \exp(X_i^\top \beta).
\]

Here, \(\lambda_i\) is the expected five-year patent count for firm \(i\), and \(X_i\) includes firm age, age squared, region indicators, and whether the firm uses Blueprinty’s software.

```{python}
#| echo: false
#| output: false
def poisson_regression_neg_loglikelihood(beta, Y, X):
    """
    Computes the negative log-likelihood for Poisson regression.
    
    Parameters:
    - beta: array-like, shape (p,), model coefficients
    - Y: array-like, shape (n,), observed counts
    - X: array-like, shape (n, p), design matrix of covariates
    
    Returns:
    - Negative log-likelihood (scalar)
    """
    
    beta = np.asarray(beta)
    X = np.asarray(X)
    Y = np.asarray(Y)    
    # Compute lambda_i = exp(X_i^T * beta)
    lambda_ = np.exp(X @ beta)    
    # Compute log-likelihood
    log_likelihood = np.sum(Y * np.log(lambda_) - lambda_ - gammaln(Y + 1))   
    return -log_likelihood  
```
```{python}
#| echo: false
#| output: false
import pandas as pd
import statsmodels.formula.api as smf
import statsmodels.api as sm

df = pd.read_csv("blueprinty.csv")

# Center age to avoid numerical issues with age^2
df["age_c"] = df["age"] - df["age"].mean()
df["age_c2"] = df["age_c"]**2

model = smf.glm(
    "patents ~ iscustomer + C(region) + age_c + age_c2",
    data=df,
    family=sm.families.Poisson()
).fit()

print(model.summary())
```

**Poisson regression (GLM) results**

| Variable | Coefficient | Std. Error | z | p-value |
|---------|------------:|-----------:|--:|--------:|
| Intercept | 1.3447 | 0.038 | 35.059 | <0.001 |
| Northeast (vs Midwest) | 0.0292 | 0.044 | 0.669 | 0.504 |
| Northwest (vs Midwest) | -0.0176 | 0.054 | -0.327 | 0.744 |
| South (vs Midwest) | 0.0566 | 0.053 | 1.074 | 0.283 |
| Southwest (vs Midwest) | 0.0506 | 0.047 | 1.072 | 0.284 |
| Blueprinty customer (`iscustomer`) | 0.2076 | 0.031 | 6.719 | <0.001 |
| Age (centered) `age_c` | -0.0080 | 0.002 | -3.843 | <0.001 |
| Age\(^2\) (centered) `age_c2` | -0.0030 | 0.000 | -11.513 | <0.001 |

*Notes:* Reference region is **Midwest**. Age is centered at the sample mean; `age_c2 = age_c^2`.

Holding firm age and region constant, Blueprinty customers have higher expected patent counts. The coefficient on `iscustomer` is 0.2076 (p < 0.001), implying an expected increase of about \(e^{0.2076}-1 \approx 23\%\) in patents for customers relative to non-customers.

Age shows a concave pattern: the negative linear and squared centered terms indicate that expected patent output declines with age and falls faster at higher ages (relative to the sample mean). Regional indicators are not statistically significant, suggesting no meaningful regional differences once firm characteristics are controlled for.

```{python}
#| echo: false
#| output: false
# Assuming 'df' is your original DataFrame
df['age_squared'] = df['age'] ** 2

# Create dummy variables for region (drop one to avoid multicollinearity)
X = df[['age', 'age_squared', 'region', 'iscustomer']]
X = pd.get_dummies(X, columns=['region'], drop_first=True)

# Ensure 'iscustomer' is numeric (0/1)
X['iscustomer'] = X['iscustomer'].astype(int)

# Add intercept
X = sm.add_constant(X)

# Define target
Y = df['patents']
```
```{python}
#| echo: false
#| output: false
# Convert boolean dummy columns to integers (0/1)
for col in X.select_dtypes(include='bool').columns:
    X[col] = X[col].astype(int)

# Now fit the model
model = sm.GLM(Y, X, family=sm.families.Poisson()).fit()
print(model.summary())
```


```{python}
#| echo: false
#| output: false
#| fig-cap: "Figure X. Poisson regression coefficients with 95% confidence intervals"
#| label: fig-poisson-coefs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

coefs = pd.DataFrame({
    "term": ["iscustomer","age","age_squared","region_Northeast","region_Northwest","region_South","region_Southwest"],
    "coef": [0.2076, 0.1486, -0.0030, 0.0292, -0.0176, 0.0566, 0.0506],
    "lo":   [0.147,  0.121,  -0.003,  -0.056, -0.123, -0.047, -0.042],
    "hi":   [0.268,  0.176,  -0.002,   0.115,  0.088,  0.160,  0.143],
})

# sort for nicer display
coefs = coefs.sort_values("coef")
y = np.arange(len(coefs))

plt.figure(figsize=(8, 4.8))
plt.hlines(y, coefs["lo"], coefs["hi"])
plt.plot(coefs["coef"], y, marker="o", linestyle="None")
plt.axvline(0, linestyle="--")
plt.yticks(y, coefs["term"])
plt.xlabel("Coefficient (log scale)")
plt.tight_layout()
plt.show()
```

**Incidence rate ratios (IRR)**
![Figure 3. Poisson Regression Coefficients (95% CI)](fig3_poisson_coef_plot.png){#fig-poisson-coef-plot}

The figure shows Poisson regression coefficient estimates with 95% confidence intervals (dashed line at 0 indicates “no effect” on the log scale). The `iscustomer` estimate is positive and its confidence interval does not cross zero, indicating a statistically significant association between Blueprinty usage and higher expected patent counts. Regional coefficients cluster near zero and their intervals overlap zero, suggesting no meaningful regional differences after controlling for other variables. The negative squared-age term indicates a concave relationship between age and patent output: patenting increases with age but at a diminishing rate.

```{python}
#| echo: false
#| output: false
import pandas as pd

# Extract coefficient summary from the fitted Poisson model
summary_table = model.summary2().tables[1]

# Optionally round and rename columns
summary_table = model.summary2().tables[1]
summary_table = summary_table.rename(columns={
    "Coef.": "Estimate",
    "Std.Err.": "Std. Error",
    "z": "z value",
    "P>|z|": "Pr(>|z|)"
}).round(4)

from IPython.display import display, HTML
display(HTML(summary_table.to_html(index=True)))
```

### Poisson Regression Results

| Term | Estimate | Std. Error | z value | p-value | 2.5% | 97.5% |
|------|---------:|-----------:|--------:|--------:|-----:|------:|
| const | -0.5089 | 0.1832 | -2.7783 | 0.0055 | -0.8679 | -0.1499 |
| age | 0.1486 | 0.0139 | 10.7162 | <0.001 | 0.1214 | 0.1758 |
| age_squared | -0.0030 | 0.0003 | -11.5132 | <0.001 | -0.0035 | -0.0025 |
| iscustomer | 0.2076 | 0.0309 | 6.7192 | <0.001 | 0.1470 | 0.2681 |
| region_Northeast | 0.0292 | 0.0436 | 0.6686 | 0.5037 | -0.0563 | 0.1147 |
| region_Northwest | -0.0176 | 0.0538 | -0.3268 | 0.7438 | -0.1230 | 0.0878 |
| region_South | 0.0566 | 0.0527 | 1.0740 | 0.2828 | -0.0467 | 0.1598 |
| region_Southwest | 0.0506 | 0.0472 | 1.0716 | 0.2839 | -0.0419 | 0.1431 |

*Note:* Coefficients are on the log scale. Exponentiating a coefficient gives the incidence rate ratio (IRR).

Figure X plots each coefficient estimate with its 95% confidence interval. The dashed vertical line at 0 represents no association on the log scale. Coefficients whose intervals do not cross 0 are statistically distinguishable from 0 at the 5% level.

The `iscustomer` effect is clearly positive and statistically significant, indicating higher expected patent counts for Blueprinty customers after controlling for age and region. Regional effects are small and not statistically significant. The positive `age` term and negative `age_squared` term indicate a concave relationship: expected patenting increases with age but the marginal gain declines at higher ages.

------

## Model Implications
```{python}
#| echo: false
#| output: false
#| fig-cap: "Figure X. Predicted five-year patent counts for Blueprinty customers vs non-customers across firm age (holding region at the reference category)."
#| label: fig-predicted-patents
import numpy as np
import matplotlib.pyplot as plt

# Coefficients from the fitted model
b = model.params

# Choose a baseline region (reference category: Midwest => all region dummies = 0)
ages = np.array([15, 25, 35, 45], dtype=float)

def mu_pred(age, iscustomer):
    eta = (
        b["const"]
        + b["age"] * age
        + b["age_squared"] * (age**2)
        + b["iscustomer"] * iscustomer
    )
    return np.exp(eta)

mu_non = np.array([mu_pred(a, 0) for a in ages])
mu_cust = np.array([mu_pred(a, 1) for a in ages])

plt.figure(figsize=(7.5, 4.8))
plt.plot(ages, mu_non, marker="o", label="Non-customer")
plt.plot(ages, mu_cust, marker="o", label="Customer")
plt.xlabel("Firm age (years)")
plt.ylabel("Predicted patents (5 years)")
plt.title("Predicted patents by Blueprinty usage")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()
```

#### Model fit and assumptions
The Pearson chi-square statistic relative to degrees of freedom is approximately \(2070/1492 \approx 1.39\), suggesting mild overdispersion. As a robustness check, we report results using robust (sandwich) standard errors; the main conclusion for `iscustomer` is unchanged.

**Predicted Patents by Blueprinty Usage**
![Figure 4. Predicted Patents by Blueprinty Usage](fig4_predicted_patents.png){#fig-predicted-patents}

This figure translates the Poisson regression into predicted five-year patent counts at representative firm ages. Across all ages shown, Blueprinty customers are predicted to produce more patents than comparable non-customers (holding other covariates fixed). The gap is largest in the mid-age range, consistent with the nonlinear age pattern captured by the positive age and negative age_squared terms.

------

## Conclusion
Overall, these findings reinforce the value of Blueprinty’s software as a meaningful contributor to patent productivity. At the same time, they highlight the role of firm maturity in shaping innovation outcomes. While regional variation appears limited, the analysis underscores the importance of targeting firms at the right stage of development and aligning product value with their innovation capacity. These insights can guide Blueprinty’s strategic messaging and outreach efforts, particularly when engaging established firms seeking to strengthen their patent portfolios.
