{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "format: html\n",
        "author: \"Tweety\"\n",
        "date: 2025-05-20\n",
        "callout-appearance: minimal # this hides the blue \"i\" icon on .callout-notes\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}\n",
        "## Simulating the Data\n"
      ],
      "id": "f2ee4bd9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "message": false
      },
      "source": [
        "#| echo: true\n",
        "#| warning: false\n",
        "#| code-overflow: wrap\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import product\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "# Attribute Levels \n",
        "brand_levels = [\"N\", \"P\", \"H\"]        \n",
        "ad_levels = [\"Yes\", \"No\"]             \n",
        "price_levels = np.arange(4, 33, 4)    \n",
        "\n",
        "# All Possible Profiles \n",
        "profiles = pd.DataFrame(\n",
        "    list(product(brand_levels, ad_levels, price_levels)),\n",
        "    columns=[\"brand\", \"ad\", \"price\"]\n",
        ")\n",
        "\n",
        "# Part-Worth Utilities \n",
        "b_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}        \n",
        "a_util = {\"Yes\": -0.8, \"No\": 0.0}           \n",
        "price_util = lambda p: -0.1 * p               \n",
        "\n",
        "# Simulation Parameters \n",
        "n_peeps = 100     \n",
        "n_tasks = 10      \n",
        "n_alts = 3      \n",
        "\n",
        "# Simulate One Respondent’s Data \n",
        "def simulate_one(respondent_id: int) -> pd.DataFrame:\n",
        "    tasks = []\n",
        "\n",
        "    for task_no in range(1, n_tasks + 1):\n",
        "        dat = profiles.sample(n=n_alts).copy()\n",
        "        dat.insert(0, \"task\", task_no)\n",
        "        dat.insert(0, \"resp\", respondent_id)\n",
        "\n",
        "        # Deterministic utility\n",
        "        dat[\"v\"] = (\n",
        "            dat[\"brand\"].map(b_util)\n",
        "            + dat[\"ad\"].map(a_util)\n",
        "            + price_util(dat[\"price\"])\n",
        "        )\n",
        "\n",
        "        # Gumbel-distributed noise\n",
        "        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n",
        "        dat[\"u\"] = dat[\"v\"] + e\n",
        "\n",
        "        # Determine choice\n",
        "        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n",
        "\n",
        "        tasks.append(dat)\n",
        "\n",
        "    return pd.concat(tasks, ignore_index=True)\n",
        "\n",
        "# Simulate All Respondents\n",
        "conjoint_data = pd.concat(\n",
        "    [simulate_one(i) for i in range(1, n_peeps + 1)],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Keep Only Observable Data \n",
        "conjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n",
        "\n",
        "# Display Result Summary \n",
        "rows, cols = conjoint_data.shape\n",
        "print(f\"The simulated dataset contains {rows} rows and {cols} columns \"\n",
        "      f\"({n_peeps} respondents × {n_tasks} tasks × {n_alts} alternatives).\")\n",
        "\n",
        "# Peek at Data \n",
        "print(conjoint_data.head())"
      ],
      "id": "792a36ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: \n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n"
      ],
      "id": "49c911bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "# Load the simulated conjoint dataset\n",
        "df = pd.read_csv(\"conjoint_data.csv\")\n",
        "\n",
        "# Create dummy indicators for brand (baseline: Hulu) and ad (baseline: No-ad)\n",
        "df[\"brand_N\"] = (df[\"brand\"] == \"N\").astype(int)  \n",
        "df[\"brand_P\"] = (df[\"brand\"] == \"P\").astype(int)  \n",
        "df[\"ad_yes\"] = (df[\"ad\"] == \"Yes\").astype(int) \n",
        "\n",
        "# Create a unique task identifier for each choice task (for grouping alternatives)\n",
        "df[\"task_id\"] = (df[\"resp\"] - 1) * 10 + (df[\"task\"] - 1)\n",
        "\n",
        "print(df.head())"
      ],
      "id": "3b58f14e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each task_id represents a choice task with three alternatives. For example, task_id = 0 (resp = 1, task = 1) includes three options from brands N, H, and P, all with ads and different prices. The first row (brand = N, price = 28) has choice = 1, indicating it was selected. Dummy variables for brand and ad presence are created, with Hulu and ad-free as baselines. The data is now formatted for model estimation.\n",
        "\n",
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        ":::: {.callout-note collapse=\"true\"}\n",
        "## Simulating the Data\n"
      ],
      "id": "1c8ee716"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "X = df[[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]].values\n",
        "y = df[\"choice\"].values\n",
        "groups = df[\"task_id\"].values\n",
        "\n",
        "# MNL log-likelihood\n",
        "def mnl_log_likelihood(beta, X, y, task_ids):\n",
        "    v = X @ beta\n",
        "    exp_v = np.exp(v)\n",
        "    df_v = pd.DataFrame({\"task_id\": task_ids, \"exp_v\": exp_v})\n",
        "    denom = df_v.groupby(\"task_id\")[\"exp_v\"].transform(\"sum\").values\n",
        "    log_probs = v - np.log(denom)\n",
        "    return -np.sum(y * log_probs)\n",
        "\n",
        "# MLE estimation\n",
        "beta_init = np.zeros(X.shape[1])\n",
        "result = minimize(\n",
        "    mnl_log_likelihood,\n",
        "    beta_init,\n",
        "    args=(X, y, groups),\n",
        "    method=\"BFGS\"\n",
        ")\n",
        "beta_hat = result.x\n",
        "se = np.sqrt(np.diag(result.hess_inv))\n",
        "z = 1.96\n",
        "ci_lower = beta_hat - z * se\n",
        "ci_upper = beta_hat + z * se\n",
        "\n",
        "# MLE results\n",
        "mle_summary = pd.DataFrame({\n",
        "    \"MLE Coefficient\": beta_hat,\n",
        "    \"Std. Error\": se,\n",
        "    \"95% CI Lower\": ci_lower,\n",
        "    \"95% CI Upper\": ci_upper\n",
        "}, index=[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"])"
      ],
      "id": "6b7db1c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: \n"
      ],
      "id": "5c0a7a90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "mle_summary"
      ],
      "id": "7d026fdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MLE estimates are:\n",
        "\n",
        "- **brand_N**: 0.94  \n",
        "- **brand_P**: 0.50  \n",
        "- **ad_yes**: -0.73  \n",
        "- **price**: -0.099  \n",
        "\n",
        "Standard errors range from 0.006 to 0.12, and all 95% confidence intervals exclude zero. The model converged successfully and parameters were estimated with high precision.\n",
        "\n",
        "\n",
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "::::{.callout-note collapse=\"true\"}\n",
        "## Simulating the Data\n"
      ],
      "id": "26f26da6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "proposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n",
        "\n",
        "# Log-prior: N(0, 5^2) for binary vars, N(0, 1^2) for price\n",
        "def log_prior(beta):\n",
        "    return -0.5 * (np.sum(beta[:3]**2 / 25) + beta[3]**2)\n",
        "\n",
        "# Log-posterior: log-likelihood + log-prior\n",
        "def log_posterior(beta, X, y, task_ids):\n",
        "    v = X @ beta\n",
        "    exp_v = np.exp(v)\n",
        "    df_v = pd.DataFrame({\"task_id\": task_ids, \"exp_v\": exp_v, \"v\": v, \"y\": y})\n",
        "    denom = df_v.groupby(\"task_id\")[\"exp_v\"].transform(\"sum\").values\n",
        "    log_probs = v - np.log(denom)\n",
        "    ll = np.sum(y * log_probs)\n",
        "    return ll + log_prior(beta)\n",
        "\n",
        "# MCMC sampler\n",
        "def metropolis_sampler(X, y, task_ids, n_iter=11000, burn_in=1000):\n",
        "    n_params = X.shape[1]\n",
        "    samples = np.zeros((n_iter, n_params))\n",
        "    beta_current = np.zeros(n_params)\n",
        "    log_post_current = log_posterior(beta_current, X, y, task_ids)\n",
        "\n",
        "    for i in range(1, n_iter):\n",
        "        beta_proposal = beta_current + np.random.normal(0, proposal_sd)\n",
        "        log_post_proposal = log_posterior(beta_proposal, X, y, task_ids)\n",
        "        log_accept_ratio = log_post_proposal - log_post_current\n",
        "\n",
        "        if np.log(np.random.rand()) < log_accept_ratio:\n",
        "            beta_current = beta_proposal\n",
        "            log_post_current = log_post_proposal\n",
        "\n",
        "        samples[i] = beta_current\n",
        "\n",
        "    return samples[burn_in:]\n",
        "\n",
        "# Run sampler and summarize\n",
        "posterior_samples = metropolis_sampler(X, y, groups)\n",
        "\n",
        "posterior_summary = pd.DataFrame({\n",
        "    \"Posterior Mean\": posterior_samples.mean(axis=0),\n",
        "    \"Posterior Std\": posterior_samples.std(axis=0),\n",
        "    \"95% CI Lower\": np.percentile(posterior_samples, 2.5, axis=0),\n",
        "    \"95% CI Upper\": np.percentile(posterior_samples, 97.5, axis=0)\n",
        "}, index=[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"])"
      ],
      "id": "638ad846",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::\n"
      ],
      "id": "fdd27a71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "posterior_summary"
      ],
      "id": "d1b724f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The posterior means are:\n",
        "\n",
        "- **brand_N**: 0.95  \n",
        "- **brand_P**: 0.50  \n",
        "- **ad_yes**: -0.74  \n",
        "- **price**: -0.100  \n",
        "\n",
        "Posterior standard deviations closely match the MLE standard errors (within ±0.001). The 95% credible intervals are nearly identical to the MLE confidence intervals. These results reflect strong agreement between the two methods given the large sample size and weakly informative priors.\n"
      ],
      "id": "6a36ce50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: \"Posterior Histogram for brand_N\"\n",
        "#| fig-align: \"center\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "param_index = 0\n",
        "param_name = \"brand_N\"\n",
        "samples = posterior_samples[:, param_index]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(samples, bins=30, kde=True, stat=\"density\", color=\"steelblue\", alpha=0.7)\n",
        "plt.axvline(np.mean(samples), color='red', linestyle='--', linewidth=1)\n",
        "plt.title(\"Posterior Histogram: brand_N\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "6ea9bdd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The posterior histogram is approximately normal and centered near 0.95. The distribution is symmetric and unimodal, with most density between 0.75 and 1.15. This indicates a well-identified parameter with low uncertainty.\n"
      ],
      "id": "2808a120"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: \"Trace Plot for brand_N\"\n",
        "#| fig-align: \"center\"\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(samples, color=\"steelblue\", linewidth=0.6)\n",
        "plt.axhline(np.mean(samples), color='red', linestyle='--', linewidth=1)\n",
        "plt.title(\"Trace Plot for brand_N\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Sampled Value\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e6ee6f21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trace plot shows consistent sampling around a stable mean with no visible drift or trends. The values fluctuate tightly across the 10,000 post-burn-in iterations, suggesting good mixing and convergence of the Metropolis-Hastings sampler.\n",
        "\n",
        "::::{.callout-note collapse=\"true\"}\n",
        "## Simulating the Data\n"
      ],
      "id": "da2b1e9b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bayes_summary = posterior_summary.copy()\n",
        "bayes_summary.columns = [\"Bayes Mean\", \"Bayes Std. Dev.\", \"Bayes CI Lower\", \"Bayes CI Upper\"]\n",
        "\n",
        "mle_summary.columns = [\"MLE Mean\", \"MLE Std. Error\", \"MLE CI Lower\", \"MLE CI Upper\"]\n",
        "\n",
        "comparison_df = pd.concat([bayes_summary, mle_summary], axis=1)"
      ],
      "id": "524fc27c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: \n"
      ],
      "id": "d228b705"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "comparison_df"
      ],
      "id": "9d98b765",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bayesian posterior means closely align with the MLE point estimates across all four parameters. Differences are minimal:\n",
        "\n",
        "- **brand_N**: Posterior mean is 0.95; MLE is 0.94. Both intervals fully overlap.\n",
        "- **brand_P**: Posterior mean is 0.51 vs. MLE of 0.50.\n",
        "- **ad_yes**: Both methods estimate the effect around -0.73, with nearly identical uncertainty.\n",
        "- **price**: Posterior mean is -0.0997; MLE is -0.0995, with near-identical intervals.\n",
        "\n",
        "Posterior standard deviations match MLE standard errors within 0.002 across all parameters. Given the large sample and weak priors, the Bayesian and MLE results are functionally equivalent and reinforce the same conclusions: consumers prefer Netflix and Prime, dislike ads, and are price sensitive.\n",
        "\n",
        "## 6. Discussion\n",
        "\n",
        "If this were real data, the parameter estimates would reveal consumers’ underlying preferences.\n",
        "\n",
        "- **β_Netflix > β_Prime** implies that, all else equal, consumers prefer Netflix to Prime Video. This reflects a higher perceived utility from Netflix’s offering.\n",
        "\n",
        "- **β_price < 0** is consistent with economic theory: as price increases, utility decreases, making the product less likely to be chosen.\n",
        "\n",
        "The direction and magnitude of all estimates are reasonable. Consumers dislike ads, show brand preferences, and are price-sensitive—patterns expected in digital subscription markets.\n",
        "\n",
        "To simulate or estimate a multi-level MNL model, we must account for individual differences in preferences. Unlike the standard MNL model, which assumes a single set of coefficients shared by all respondents, a hierarchical model allows each respondent to have their own set of parameters.\n",
        "\n",
        "- **Key changes:**\n",
        "Instead of using one fixed β, assign each respondent a personal βₖ drawn from a population distribution:  \\[ \\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma) \\]\n",
        "\n",
        "- **Estimation:**  \n",
        "  Estimate both the respondent-level βᵢs and the population-level parameters (μ, Σ). This requires hierarchical modeling techniques, often implemented via Bayesian MCMC or mixed logit estimation.\n",
        "\n",
        "This approach better reflects real-world data by capturing preference heterogeneity across individuals.\n"
      ],
      "id": "cbc4abc5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "mgta495",
      "language": "python",
      "display_name": "Python (mgta495)",
      "path": "/Users/qqtweety/Library/Jupyter/kernels/mgta495"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}