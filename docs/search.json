[
  {
    "objectID": "Maximum_Likelihood_Estimation.html",
    "href": "Maximum_Likelihood_Estimation.html",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.\nInstead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#introduction",
    "href": "Maximum_Likelihood_Estimation.html#introduction",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.\nInstead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#objective",
    "href": "Maximum_Likelihood_Estimation.html#objective",
    "title": "Blueprinty Case Study",
    "section": "Objective",
    "text": "Objective\nThe objective of this project is to assess whether engineering firms that use Blueprinty‚Äôs software produce more patents than firms that do not, after accounting for firm age and regional location. The analysis aims to test whether the available data support Blueprinty‚Äôs claim that its software is associated with stronger patent performance."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#data-description",
    "href": "Maximum_Likelihood_Estimation.html#data-description",
    "title": "Blueprinty Case Study",
    "section": "Data Description",
    "text": "Data Description\n\n\n\n\n\n\nData dictionary\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded in the last 5 years (count outcome)\n\n\nregion\nFirm geographic region (categorical: Midwest, Southwest, Northwest, Northeast, etc.)\n\n\nage\nYears since incorporation (continuous)\n\n\niscustomer\nBlueprinty software user indicator (1 = customer, 0 = non-customer)\n\n\n\n\n\n\n\nDescriptive comparison: patents by customer status\n\n\n\nCustomer status\nGroup\nMean patents (5 years)\n\n\n\n\n0\nNon-customer\n3.473\n\n\n1\nCustomer\n4.133\n\n\n\nBlueprinty customers have a higher average number of patents awarded over the past five years (4.133) than non-customers (3.473). This difference is descriptive and does not account for other factors such as firm age or region, which are addressed in the regression analysis.\nFigure 1. Distribution of Patent Counts by Blueprinty Usage\n\nThe histogram shows a right-skewed distribution of patent counts, which is typical for count outcomes. Firms using Blueprinty‚Äôs software show a modest rightward shift, suggesting higher patent counts are somewhat more common among customers.\nDescriptively, Blueprinty customers average 4.133 patents over five years versus 3.473 for non-customers. This comparison is correlational: customers are not randomly selected, and differences in firm age or region may explain part of the gap. The regression analysis tests whether the association remains after controlling for these factors.\nFirm age by Blueprinty customer status\n\n\n\niscustomer\nN\nMean age\nMedian age\nSD\n\n\n\n\n0\n1019\n26.10\n25.50\n6.95\n\n\n1\n481\n26.90\n26.50\n7.81\n\n\n\nCustomer and non-customer firms have similar age distributions: customers are slightly older on average (26.90 vs 26.10 years) and have a slightly higher median age (26.5 vs 25.5). Age is therefore included as a control in the regression.\nRegion by Blueprinty customer status (counts)\n\n\n\nRegion\nNon-customer (0)\nCustomer (1)\nTotal\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nTotal\n1019\n481\n1500\n\n\n\nCustomer status varies by region, with a large share of customers located in the Northeast. Because region is related to adoption, it is controlled for in the regression to reduce confounding."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#poisson-model-specification-and-estimation",
    "href": "Maximum_Likelihood_Estimation.html#poisson-model-specification-and-estimation",
    "title": "Blueprinty Case Study",
    "section": "Poisson Model Specification and Estimation",
    "text": "Poisson Model Specification and Estimation\nWe model the number of patents awarded to each firm over a fixed five-year period. Because patents is a nonnegative count outcome, a Poisson model provides a natural starting point.\nLet (Y_i) denote the number of patents awarded to firm (i) in the last five years. We assume:\n[ Y_i () ]\nwhere () is the expected number of patents per firm over the five-year window.\nFor a sample of (n) independent firms with observed counts (y_1, y_2, , y_n), the log-likelihood function is:\n[ (y_1,,y_n) = -n+ ({i=1}^n y_i)() - {i=1}^n (y_i!) ]\nWe estimate () using maximum likelihood (MLE). This baseline model provides a benchmark before adding firm characteristics (e.g., Blueprinty usage, firm age, and region) in a regression framework.\n\nPoisson log-likelihood and the MLE\nFigure 2. Poisson Log-Likelihood vs.¬†Lambda\n\nThis figure shows the Poisson log-likelihood as a function of \\(\\lambda\\) for the observed patent counts. The log-likelihood is maximized at the sample mean, so the Poisson MLE is:\n\\[\n\\hat{\\lambda} = \\bar{Y}.\n\\]\nFor independent observations \\(y_1,\\ldots,y_n\\), the Poisson log-likelihood is:\n\\[\n\\ell(\\lambda)\n= -n\\lambda\n+ \\left(\\sum_{i=1}^n y_i\\right)\\log(\\lambda)\n- \\sum_{i=1}^n \\log(y_i!).\n\\]\nDifferentiating and setting the derivative to zero gives:\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n= -n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^n y_i = \\bar{y}.\n\\]\nIn this dataset, \\(\\hat{\\lambda}=3.6847\\), matching \\(\\bar{y}=3.6847\\) up to rounding."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#poisson-regression-model",
    "href": "Maximum_Likelihood_Estimation.html#poisson-regression-model",
    "title": "Blueprinty Case Study",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nNext, we model five-year patent counts using a Poisson regression. Let \\(Y_i\\) be the number of patents awarded to firm \\(i\\) over the last five years:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\qquad \\log(\\lambda_i) = X_i^\\top \\beta,\n\\]\nequivalently \\(\\lambda_i = \\exp(X_i^\\top \\beta)\\). The covariate vector \\(X_i\\) includes firm age, age squared, region indicators, and Blueprinty usage (iscustomer).\nPoisson regression (GLM) results\n\n\n\nVariable\nCoefficient\nStd. Error\nz\np-value\n\n\n\n\nIntercept\n1.3447\n0.038\n35.059\n&lt;0.001\n\n\nNortheast (vs Midwest)\n0.0292\n0.044\n0.669\n0.504\n\n\nNorthwest (vs Midwest)\n-0.0176\n0.054\n-0.327\n0.744\n\n\nSouth (vs Midwest)\n0.0566\n0.053\n1.074\n0.283\n\n\nSouthwest (vs Midwest)\n0.0506\n0.047\n1.072\n0.284\n\n\nBlueprinty customer (iscustomer)\n0.2076\n0.031\n6.719\n&lt;0.001\n\n\nAge (centered) age_c\n-0.0080\n0.002\n-3.843\n&lt;0.001\n\n\nAge(^2) (centered) age_c2\n-0.0030\n0.000\n-11.513\n&lt;0.001\n\n\n\nNotes: Reference region is Midwest. Age is centered at the sample mean; age_c2 = age_c^2.\nHolding firm age and region constant, Blueprinty customers have higher expected patent counts. The coefficient on iscustomer is 0.2076 (p &lt; 0.001), implying an expected increase of about (e^{0.2076}-1 %) in patents for customers relative to non-customers.\nAge shows a concave pattern: the negative linear and squared centered terms indicate that expected patent output declines with age and falls faster at higher ages (relative to the sample mean). Regional indicators are not statistically significant, suggesting no meaningful regional differences once firm characteristics are controlled for.\nFigure 3. Poisson Regression Coefficients (95% CI)\n\nThe figure shows Poisson regression coefficient estimates with 95% confidence intervals (dashed line at 0 indicates ‚Äúno effect‚Äù on the log scale). The iscustomer estimate is positive and its confidence interval does not cross zero, indicating a statistically significant association between Blueprinty usage and higher expected patent counts. Regional coefficients cluster near zero and their intervals overlap zero, suggesting no meaningful regional differences after controlling for other variables. The negative squared-age term indicates a concave relationship between age and patent output: patenting increases with age but at a diminishing rate.\n\nPoisson Regression Results\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nz value\np-value\n2.5%\n97.5%\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\nage\n0.1486\n0.0139\n10.7162\n&lt;0.001\n0.1214\n0.1758\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n&lt;0.001\n-0.0035\n-0.0025\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n&lt;0.001\n0.1470\n0.2681\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\nNote: Coefficients are on the log scale. Exponentiating a coefficient gives the incidence rate ratio (IRR).\nFigure X plots each coefficient estimate with its 95% confidence interval. The dashed vertical line at 0 represents no association on the log scale. Coefficients whose intervals do not cross 0 are statistically distinguishable from 0 at the 5% level.\nThe iscustomer effect is clearly positive and statistically significant, indicating higher expected patent counts for Blueprinty customers after controlling for age and region. Regional effects are small and not statistically significant. The positive age term and negative age_squared term indicate a concave relationship: expected patenting increases with age but the marginal gain declines at higher ages."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#model-implications",
    "href": "Maximum_Likelihood_Estimation.html#model-implications",
    "title": "Blueprinty Case Study",
    "section": "Model Implications",
    "text": "Model Implications\nModel fit and assumptions The Pearson chi-square statistic relative to degrees of freedom is approximately (2070/1492 ), suggesting mild overdispersion. As a robustness check, we report results using robust (sandwich) standard errors; the main conclusion for iscustomer is unchanged.\nFigure 4. Predicted Patents by Blueprinty Usage\n\nThis figure translates the Poisson regression into predicted five-year patent counts at representative firm ages. Across all ages shown, Blueprinty customers are predicted to produce more patents than comparable non-customers (holding other covariates fixed). The gap is largest in the mid-age range, consistent with the nonlinear age pattern captured by the positive age and negative age_squared terms."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#conclusion",
    "href": "Maximum_Likelihood_Estimation.html#conclusion",
    "title": "Blueprinty Case Study",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, these findings reinforce the value of Blueprinty‚Äôs software as a meaningful contributor to patent productivity. At the same time, they highlight the role of firm maturity in shaping innovation outcomes. While regional variation appears limited, the analysis underscores the importance of targeting firms at the right stage of development and aligning product value with their innovation capacity. These insights can guide Blueprinty‚Äôs strategic messaging and outreach efforts, particularly when engaging established firms seeking to strengthen their patent portfolios."
  },
  {
    "objectID": "AB_Testing.html",
    "href": "AB_Testing.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "AB_Testing.html#introduction",
    "href": "AB_Testing.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "AB_Testing.html#objective",
    "href": "AB_Testing.html#objective",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Objective",
    "text": "Objective\nThis project replicates and interprets the findings from Karlan and List‚Äôs (2007) field experiment on charitable giving. Using the original dataset, I examine how the presence and structure of a matching donation offer influence both the likelihood of giving and the amount contributed.\nThis project involves:\n\nAnalyzing treatment effects using t-tests, regression models, and probit analysis\nComparing donation behavior across different match ratios and suggested ask levels\nConducting simulations to illustrate the Law of Large Numbers and the Central Limit Theorem\nPresenting results in a clear, reproducible format using Quarto and Python\n\nThe overall goal is to better understand the behavioral response to charitable incentives and how small changes in message framing can impact donor behavior."
  },
  {
    "objectID": "AB_Testing.html#description-of-the-experiment",
    "href": "AB_Testing.html#description-of-the-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description of the Experiment",
    "text": "Description of the Experiment\nIn their 2007 study published in the American Economic Review, Dean Karlan and John List conducted a large-scale natural field experiment to test the effectiveness of matching donations in a real-world fundraising campaign. Over 50,000 previous donors to a U.S. nonprofit were mailed fundraising letters and randomly assigned to one of two groups:\n\nControl group: received a standard fundraising letter with no special offer\nTreatment group: received a similar letter but was offered a matching donation, meaning their contribution would be matched by another donor\n\nWithin the treatment group, individuals were further randomized into subgroups based on:\n\nMatch ratio: \\(1\\!:\\!1\\), \\(2\\!:\\!1\\), or \\(3\\!:\\!1\\)\nMaximum match amount: \\(25{,}000\\), \\(50{,}000\\), \\(100{,}000\\), or unstated\nSuggested donation amount: equal to 1.25√ó or 1.5√ó the donor‚Äôs previous contribution\n\nThe randomized design allows for causal analysis of how these variations influence both the decision to donate and the amount given. This experiment provides a powerful example of how field experiments can be used to study economic behavior in natural settings."
  },
  {
    "objectID": "AB_Testing.html#data-description",
    "href": "AB_Testing.html#data-description",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data Description",
    "text": "Data Description\nWe begin by loading the dataset provided by Karlan and List (2007), which contains detailed records from their fundraising field experiment.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nSummary Statistics by Treatment Status\nFigure 1. Donation Rate by Group \nThe treatment group has a higher donation rate (2.20%) than the control group (1.79%). Sample sizes are shown above each bar.\nFigure 2. Average Donation Amount by Group \nAverage donations, including non-donors, are higher in the treatment group ($0.97) than in the control group ($0.81), suggesting that the matching offer increased total giving on average. Sample sizes are shown above each bar."
  },
  {
    "objectID": "AB_Testing.html#balance-test",
    "href": "AB_Testing.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\nTo assess whether the randomization produced comparable treatment and control groups, we tested balance on several pre-treatment characteristics: months since last donation (mrm2), years since first donation, number of prior donations (freq), and a gender indicator (female).\n\nT-Test and Linear Regression for Each Variable\n\n\n\nVariable\nTreatment Coefficient\np-value\n\n\n\n\nmrm2\n0.0137\n0.905\n\n\nyears\n‚àí0.0575\n0.270\n\n\nfreq\n‚àí0.0120\n0.912\n\n\nfemale\n‚àí0.0075\n0.079\n\n\n\nNone of the estimated treatment effects are statistically significant at the 5% level, indicating balance across observable characteristics. These results are consistent with the balance checks reported in Table 1 of Karlan and List (2007).\n\n\nBalance Check for mrm2\n\n\n\nStatistic\nValue\n\n\n\n\nTreatment coefficient\n0.0137\n\n\np-value (t-test)\n0.9049\n\n\np-value (regression)\n0.9050\n\n\n95% confidence interval\n[‚àí0.211, 0.238]\n\n\n\nThe estimated difference in months since last donation between treatment and control groups is small and statistically insignificant, indicating comparable donation recency prior to treatment.\n\n\nAdditional Balance Tests\n\n\nBalance Test ‚Äî Key Estimates\n\n\n\nVariable\nTreatment coef\nt-stat\np-value\n95% CI\n\n\n\n\nmrm2\n0.0137\n0.119\n0.905\n[-0.211, 0.238]\n\n\nyears\n‚àí0.0575\n‚àí1.103\n0.270\n[-0.160, 0.045]\n\n\nfreq\n‚àí0.0120\n‚àí0.111\n0.912\n[-0.224, 0.200]\n\n\nfemale\n‚àí0.0075\n‚àí1.758\n0.079\n[-0.016, 0.001]\n\n\n\nNone of the estimated treatment effects are statistically significant at the 5% level; the gender indicator is borderline (p = 0.079) but the estimated difference is small."
  },
  {
    "objectID": "AB_Testing.html#experimental-results",
    "href": "AB_Testing.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nFigure 3. Donation Rate by Treatment Status \nThe treatment group exhibits a higher donation rate than the control group (2.20% vs.¬†1.79%). The difference is statistically significant at the 1% level.\n\nTreatment Effect on Donation Rates\nWe estimate the effect of the matching donation treatment on the probability of giving using both a two-sample t-test and a bivariate linear regression. Both approaches indicate a statistically significant difference in donation rates between the treatment and control groups.\nThe regression results imply that assignment to the treatment increases the probability of donating by approximately 0.42 percentage points (p = 0.002). Relative to the control group‚Äôs donation rate of about 1.8%, this corresponds to a roughly 22% increase in the likelihood of giving.\nAlthough the absolute effect is small, the estimate is precise and consistent across methods, indicating that offering a matching donation increases participation in charitable giving.\n\n\n\n\n\n\n\n\n\nOutcome: Donated (gave)\nEstimate\np-value\n95% Confidence Interval\n\n\n\n\nTreatment effect\n0.0042\n0.002\n[0.002, 0.007]\n\n\n\n\n\nProbit Regression\nAs a robustness check for the binary outcome, we estimate a Probit model and report the average marginal effect. The treatment increases the probability of donating by 0.43 percentage points (p = 0.002), which closely matches the linear probability model estimate.\n\n\n\nModel\nEffect on Pr(Donate)\np-value\n95% CI\n\n\n\n\nProbit (AME)\n0.0043\n0.002\n[0.002, 0.007]"
  },
  {
    "objectID": "AB_Testing.html#differences-between-match-rates",
    "href": "AB_Testing.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\nWe examine whether larger match ratios within the treatment group lead to higher donation rates by comparing 1:1, 2:1, and 3:1 matching offers. Pairwise t-tests indicate no statistically significant differences in donation rates across match ratios.\nRelative to a 1:1 match, increasing the ratio to 2:1 raises the donation rate by approximately 0.19 percentage points (p = 0.335), while moving from 2:1 to 3:1 increases the rate by only 0.01 percentage points (p = 0.310). Neither difference is statistically significant.\nThese results suggest that higher match ratios do not meaningfully increase participation beyond the presence of a match itself, consistent with the findings in Karlan and List (2007).\n\n\n\nComparison\nChange in Donation Rate (pp)\np-value\n\n\n\n\n2:1 vs 1:1\n+0.19\n0.335\n\n\n3:1 vs 2:1\n+0.01\n0.310\n\n\n\n\n\nEffect of Match Ratio on Donation Rates\n\n\n\nMatch Ratio\nChange in Donation Rate (pp)\np-value\n\n\n\n\n2:1 vs 1:1\n+0.19\n0.338\n\n\n3:1 vs 1:1\n+0.20\n0.313\n\n\n\n\n\n\nDonation Rates by Match Ratio\nAmong treated individuals, donation rates are similar across match ratios. The response rate is 2.07% under a 1:1 match, 2.26% under a 2:1 match, and 2.27% under a 3:1 match.\nThe increase in donation rates beyond a 1:1 match is small‚Äî0.19 percentage points for a 2:1 match and 0.01 percentage points for a 3:1 match‚Äîand not statistically significant, consistent with the formal tests reported earlier. This pattern aligns with the findings of Karlan and List (2007), which show that increasing the match ratio does not materially increase participation once a match is offered.\n\n\n\nMatch Ratio\nDonation Rate\n\n\n\n\n1:1\n0.0207 (2.07%)\n\n\n2:1\n0.0226 (2.26%)\n\n\n3:1\n0.0227 (2.27%)\n\n\n\n\n\n\nDonation Amount\nWe next examine whether the matching donation treatment affects the amount donated among individuals who chose to give, focusing on the intensive margin of charitable giving.\nA two-sample t-test and a linear regression both estimate a small difference in donation amounts between treatment and control donors; however, the effects are not statistically significant at conventional levels (p ‚âà 0.06). The estimated magnitudes are modest.\nOverall, these results suggest that while offering a matching donation increases participation, it does not meaningfully affect the amount donated conditional on giving.\n\n\nDistribution of Donation Amounts Among Donors\nFigure 4. Distribution of Donation Amounts by Treatment Status  Donation amounts in both the control and treatment groups are right-skewed, with most contributions concentrated at lower values and a small number of large donations. Mean donation amounts are similar across groups ($45.54 for the control group and $43.87 for the treatment group).\n\nFigure 5. Sampling Distribution of Treatment‚ÄìControl Differences (Simulated)  The figure shows the distribution of treatment‚Äìcontrol differences in donation rates across 10,000 simulated experiments with 500 observations per group. The distribution is approximately normal and centered near 0.004, the difference implied by the assumed donation probabilities. The dashed line indicates the mean simulated difference, and the dotted line marks the null of no treatment effect.\n\n\n\nLaw of Large Numbers\nFigure 6. Cumulative estimate of the treatment‚Äìcontrol difference in mean donation amounts\n\nFigure 6 plots the cumulative difference in mean donation amounts between the treatment and control groups as observations are added. When the sample size is small, the estimated difference fluctuates substantially. As the number of observations increases, the cumulative estimate stabilizes and converges toward the full-sample difference, illustrated by the horizontal dashed line.\n\n\n\nSampling Variability and Sample Size\nFigure 7. Sampling distributions of treatment‚Äìcontrol differences by sample size\n\nFigure 7 shows simulated sampling distributions of treatment‚Äìcontrol differences in donation rates for sample sizes of 50, 200, 500, and 1,000. With small samples, the distribution is wide. As sample size increases, the distributions concentrate around their mean, indicating greater precision."
  },
  {
    "objectID": "Multinomial_Logit_Model.html",
    "href": "Multinomial_Logit_Model.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "Multinomial_Logit_Model.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "Multinomial_Logit_Model.html#simulate-conjoint-data",
    "href": "Multinomial_Logit_Model.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(123)\n\n# Attribute Levels \nbrand_levels = [\"N\", \"P\", \"H\"]        \nad_levels = [\"Yes\", \"No\"]             \nprice_levels = np.arange(4, 33, 4)    \n\n# All Possible Profiles \nprofiles = pd.DataFrame(\n    list(product(brand_levels, ad_levels, price_levels)),\n    columns=[\"brand\", \"ad\", \"price\"]\n)\n\n# Part-Worth Utilities \nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}        \na_util = {\"Yes\": -0.8, \"No\": 0.0}           \nprice_util = lambda p: -0.1 * p               \n\n# Simulation Parameters \nn_peeps = 100     \nn_tasks = 10      \nn_alts = 3      \n\n# Simulate One Respondent‚Äôs Data \ndef simulate_one(respondent_id: int) -&gt; pd.DataFrame:\n    tasks = []\n\n    for task_no in range(1, n_tasks + 1):\n        dat = profiles.sample(n=n_alts).copy()\n        dat.insert(0, \"task\", task_no)\n        dat.insert(0, \"resp\", respondent_id)\n\n        # Deterministic utility\n        dat[\"v\"] = (\n            dat[\"brand\"].map(b_util)\n            + dat[\"ad\"].map(a_util)\n            + price_util(dat[\"price\"])\n        )\n\n        # Gumbel-distributed noise\n        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        dat[\"u\"] = dat[\"v\"] + e\n\n        # Determine choice\n        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n\n        tasks.append(dat)\n\n    return pd.concat(tasks, ignore_index=True)\n\n# Simulate All Respondents\nconjoint_data = pd.concat(\n    [simulate_one(i) for i in range(1, n_peeps + 1)],\n    ignore_index=True\n)\n\n# Keep Only Observable Data \nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Display Result Summary \nrows, cols = conjoint_data.shape\nprint(f\"The simulated dataset contains {rows} rows and {cols} columns \"\n      f\"({n_peeps} respondents √ó {n_tasks} tasks √ó {n_alts} alternatives).\")\n\n# Peek at Data \nprint(conjoint_data.head())\n\nThe simulated dataset contains 3000 rows and 6 columns (100 respondents √ó 10 tasks √ó 3 alternatives).\n   resp  task brand   ad  price  choice\n0     1     1     P  Yes     12       0\n1     1     1     N   No     24       0\n2     1     1     P   No     12       1\n3     1     2     H   No     12       0\n4     1     2     P  Yes     24       0"
  },
  {
    "objectID": "Multinomial_Logit_Model.html#preparing-the-data-for-estimation",
    "href": "Multinomial_Logit_Model.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n   resp  task  choice brand   ad  price  brand_N  brand_P  ad_yes  task_id\n0     1     1       1     N  Yes     28        1        0       1        0\n1     1     1       0     H  Yes     16        0        0       1        0\n2     1     1       0     P  Yes     16        0        1       1        0\n3     1     2       0     N  Yes     32        1        0       1        1\n4     1     2       1     P  Yes     16        0        1       1        1\n\n\nEach task_id represents a choice task with three alternatives. For example, task_id = 0 (resp = 1, task = 1) includes three options from brands N, H, and P, all with ads and different prices. The first row (brand = N, price = 28) has choice = 1, indicating it was selected. Dummy variables for brand and ad presence are created, with Hulu and ad-free as baselines. The data is now formatted for model estimation."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#estimation-via-maximum-likelihood",
    "href": "Multinomial_Logit_Model.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLE Coefficient\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.941195\n0.109961\n0.725672\n1.156718\n\n\nbrand_P\n0.501616\n0.120088\n0.266242\n0.736989\n\n\nad_yes\n-0.731994\n0.088279\n-0.905021\n-0.558968\n\n\nprice\n-0.099480\n0.006363\n-0.111951\n-0.087010\n\n\n\n\n\n\n\nThe MLE estimates are:\n\nbrand_N: 0.94\n\nbrand_P: 0.50\n\nad_yes: -0.73\n\nprice: -0.099\n\nStandard errors range from 0.006 to 0.12, and all 95% confidence intervals exclude zero. The model converged successfully and parameters were estimated with high precision."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#estimation-via-bayesian-methods",
    "href": "Multinomial_Logit_Model.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior Mean\nPosterior Std\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.954482\n0.111946\n0.737799\n1.172539\n\n\nbrand_P\n0.506766\n0.112751\n0.285853\n0.728708\n\n\nad_yes\n-0.732992\n0.088405\n-0.910828\n-0.566985\n\n\nprice\n-0.099719\n0.006344\n-0.112362\n-0.087510\n\n\n\n\n\n\n\nThe posterior means are:\n\nbrand_N: 0.95\n\nbrand_P: 0.50\n\nad_yes: -0.74\n\nprice: -0.100\n\nPosterior standard deviations closely match the MLE standard errors (within ¬±0.001). The 95% credible intervals are nearly identical to the MLE confidence intervals. These results reflect strong agreement between the two methods given the large sample size and weakly informative priors.\n\n\n\n\n\nPosterior Histogram for brand_N\n\n\n\n\nThe posterior histogram is approximately normal and centered near 0.95. The distribution is symmetric and unimodal, with most density between 0.75 and 1.15. This indicates a well-identified parameter with low uncertainty.\n\n\n\n\n\nTrace Plot for brand_N\n\n\n\n\nThe trace plot shows consistent sampling around a stable mean with no visible drift or trends. The values fluctuate tightly across the 10,000 post-burn-in iterations, suggesting good mixing and convergence of the Metropolis-Hastings sampler.\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Mean\nBayes Std. Dev.\nBayes CI Lower\nBayes CI Upper\nMLE Mean\nMLE Std. Error\nMLE CI Lower\nMLE CI Upper\n\n\n\n\nbrand_N\n0.954482\n0.111946\n0.737799\n1.172539\n0.941195\n0.109961\n0.725672\n1.156718\n\n\nbrand_P\n0.506766\n0.112751\n0.285853\n0.728708\n0.501616\n0.120088\n0.266242\n0.736989\n\n\nad_yes\n-0.732992\n0.088405\n-0.910828\n-0.566985\n-0.731994\n0.088279\n-0.905021\n-0.558968\n\n\nprice\n-0.099719\n0.006344\n-0.112362\n-0.087510\n-0.099480\n0.006363\n-0.111951\n-0.087010\n\n\n\n\n\n\n\nThe Bayesian posterior means closely align with the MLE point estimates across all four parameters. Differences are minimal:\n\nbrand_N: Posterior mean is 0.95; MLE is 0.94. Both intervals fully overlap.\nbrand_P: Posterior mean is 0.51 vs.¬†MLE of 0.50.\nad_yes: Both methods estimate the effect around -0.73, with nearly identical uncertainty.\nprice: Posterior mean is -0.0997; MLE is -0.0995, with near-identical intervals.\n\nPosterior standard deviations match MLE standard errors within 0.002 across all parameters. Given the large sample and weak priors, the Bayesian and MLE results are functionally equivalent and reinforce the same conclusions: consumers prefer Netflix and Prime, dislike ads, and are price sensitive."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#discussion",
    "href": "Multinomial_Logit_Model.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf this were real data, the parameter estimates would reveal consumers‚Äô underlying preferences.\n\nŒ≤_Netflix &gt; Œ≤_Prime implies that, all else equal, consumers prefer Netflix to Prime Video. This reflects a higher perceived utility from Netflix‚Äôs offering.\nŒ≤_price &lt; 0 is consistent with economic theory: as price increases, utility decreases, making the product less likely to be chosen.\n\nThe direction and magnitude of all estimates are reasonable. Consumers dislike ads, show brand preferences, and are price-sensitive‚Äîpatterns expected in digital subscription markets.\nTo simulate or estimate a multi-level MNL model, we must account for individual differences in preferences. Unlike the standard MNL model, which assumes a single set of coefficients shared by all respondents, a hierarchical model allows each respondent to have their own set of parameters.\n\nKey changes: Instead of using one fixed Œ≤, assign each respondent a personal Œ≤‚Çñ drawn from a population distribution: [ _i (, ) ]\nEstimation:\nEstimate both the respondent-level Œ≤·µ¢s and the population-level parameters (Œº, Œ£). This requires hierarchical modeling techniques, often implemented via Bayesian MCMC or mixed logit estimation.\n\nThis approach better reflects real-world data by capturing preference heterogeneity across individuals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tweety Chang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Finance & Business Analytics\nüìß ctweety15@gmail.com üìû 858-888-1353 üìÑ Resume"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nWith a background in business analytics and accounting, I bring hands-on experience analyzing data, identifying process gaps, and supporting data-driven improvements through clear communication and cross-functional collaboration. My work has focused on turning complex, multi-source data into practical insights that support reporting accuracy and informed decision-making.\nIn recent analytics projects, I have analyzed operational and business data to uncover inconsistencies, trends, and improvement opportunities affecting downstream reporting and analysis. I have supported projects end-to-end by clarifying requirements, validating outputs through testing, documenting findings, and presenting results in a clear, actionable way to both technical and non-technical stakeholders. Across roles, I have worked closely with business, analytics, and technical teams to ensure analysis aligned with real user needs and project goals.\nI am comfortable working in ambiguous environments, learning new domains quickly, and balancing multiple priorities. I work regularly with Excel, SQL, Python, Tableau, and Power BI, which allows me to move fluidly between data analysis and business context. My background in client-facing and team-based roles has also strengthened my ability to communicate clearly, collaborate effectively, and contribute thoughtfully to shared outcomes.\nI am currently pursuing an M.S. in Business Analytics at UC San Diego and am seeking internship or full-time opportunities where analytical rigor and practical problem-solving can support real-world business decisions."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego ‚Äî Rady School of Management\nMaster of Science, Business Analytics\nJul 2024 ‚Äì Mar 2026\nNorthern Illinois University\nBachelor of Science, Accountancy\nAug 2021 ‚Äì Aug 2023\nAsia University\nBachelor of Science, Accounting and Information Systems\nSep 2019 ‚Äì Aug 2023"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience"
  },
  {
    "objectID": "about.html#work-experience-1",
    "href": "about.html#work-experience-1",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nHighspring\nContent Analyst ¬∑ Jul 2025 ‚Äì Dec 2025\nBuilt a lightweight tracking workflow and reporting to monitor labeling accuracy and productivity for ad-content review.\nHP\nData Analyst ¬∑ Mar 2025 ‚Äì Jun 2025\nAutomated data quality checks and anomaly scans for campaign datasets; surfaced mismatches and outliers for stakeholder review.\nHBC Tax Inc.\nTax Associate ¬∑ Jan 2023 ‚Äì May 2023\nPrepared individual and small-business tax filings and supported document intake, reconciliation, and client follow-ups.\nNorthern Illinois University\nResearch Assistant ¬∑ May 2022 ‚Äì Aug 2022\nCleaned and organized research datasets and produced summary outputs for faculty analysis."
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "Projects",
    "text": "Projects\n\nPredictive Analysis for Intuit QuickBooks Upgrade\nDatabase Management\nA/B Testing\nMaximum Likelihood Estimation (MLE)\nMultinomial Logit (MNL) & Conjoint Analysis\nKey Drivers Analysis"
  },
  {
    "objectID": "about.html#focus-areas",
    "href": "about.html#focus-areas",
    "title": "About",
    "section": "Focus Areas",
    "text": "Focus Areas\n\nFinancial, business, and operational data analysis\n\nData validation, reconciliation, and reporting accuracy\n\nReporting automation and dashboard development\n\nExperimentation and A/B testing\n\nCommunicating insights to support operational and strategic decisions"
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html",
    "href": "Intuit_Quickbooks_Upgrade.html",
    "title": "Intuit Quickbooks Upgrade",
    "section": "",
    "text": "This project builds a predictive response model for Intuit‚Äôs QuickBooks upgrade upsell campaign. The dataset (intuit75k.parquet) contains 75,000 small businesses randomly sampled from the 801,821 businesses that received the wave-1 mailing. Each business record includes customer attributes and purchase history that can be used to estimate the likelihood of upgrading.\nThe response variable res1 indicates whether a business purchased QuickBooks version 3.0 through Intuit Direct after receiving the wave-1 offer. Because the case materials have been modified (variables added, removed, and recoded), the analysis relies on the updated variable definitions provided in this report rather than Exhibit 3 in the course reader."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#introduction",
    "href": "Intuit_Quickbooks_Upgrade.html#introduction",
    "title": "Intuit Quickbooks Upgrade",
    "section": "",
    "text": "This project builds a predictive response model for Intuit‚Äôs QuickBooks upgrade upsell campaign. The dataset (intuit75k.parquet) contains 75,000 small businesses randomly sampled from the 801,821 businesses that received the wave-1 mailing. Each business record includes customer attributes and purchase history that can be used to estimate the likelihood of upgrading.\nThe response variable res1 indicates whether a business purchased QuickBooks version 3.0 through Intuit Direct after receiving the wave-1 offer. Because the case materials have been modified (variables added, removed, and recoded), the analysis relies on the updated variable definitions provided in this report rather than Exhibit 3 in the course reader."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#objective",
    "href": "Intuit_Quickbooks_Upgrade.html#objective",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Objective",
    "text": "Objective\nDeliver a wave-2 targeting approach that increases incremental profit versus untargeted mailing. Specifically, we (1) estimate each business‚Äôs likelihood of responding to wave-2, (2) translate predicted responses into expected profit using campaign economics, and (3) recommend a mailing cutoff rule that sends offers only when expected return exceeds break-even."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#data-and-problem-setup",
    "href": "Intuit_Quickbooks_Upgrade.html#data-and-problem-setup",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Data and Problem Setup",
    "text": "Data and Problem Setup\n\nData source and unit of analysis\nEach row represents a small-business customer in Intuit Direct‚Äôs mailing file. The analysis uses the sample provided in intuit75k.parquet, drawn from wave-1 recipients.\n\n\nOutcome definition\n\nres1 is the response indicator for the wave-1 mailing (‚ÄúYes‚Äù = purchased QuickBooks 3.0 through Intuit Direct; ‚ÄúNo‚Äù otherwise).\n\nThe modeling task is a binary classification problem used for ranking customers for wave-2 targeting.\n\n\n\nPredictor variables (summary)\nThe predictors describe customer location, business indicators, and purchase behavior over the prior 36 months, plus product/version history. A full data dictionary is included below.\n\n\nTrain/validation split\n\ntraining = 1 indicates the training sample (70%).\n\ntraining = 0 indicates the validation sample (30%).\nModel selection and thresholding decisions are based on validation performance.\n\n\n\n\n\n\n\nData dictionary\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nSmall business customer ID\n\n\nzip5\n5-digit ZIP code (00000 = unknown, 99999 = international ZIPs)\n\n\nzip_bins\nZIP-code bins (~20 roughly equal-sized bins from lowest to highest ZIP number)\n\n\nsex\nGender: Female, Male, or Unknown\n\n\nbizflag\nBusiness flag: address contains a business name (1 = yes, 0 = no/unknown)\n\n\nnumords\nNumber of Intuit Direct orders in the previous 36 months\n\n\ndollars\nTotal dollars ordered from Intuit Direct in the previous 36 months\n\n\nlast\nMonths since last Intuit Direct order (within the previous 36 months)\n\n\nsincepurch\nMonths since original (non-upgrade) QuickBooks purchase\n\n\nversion1\n1 if current QuickBooks is version 1; 0 if version 2\n\n\nowntaxprod\n1 if customer purchased tax software; 0 otherwise\n\n\nupgraded\n1 if customer upgraded previously (vs.¬†purchasing a base version); 0 otherwise\n\n\nres1\nResponse to wave-1 mailing (‚ÄúYes‚Äù / ‚ÄúNo‚Äù)\n\n\ntraining\nSplit indicator (1 = training, 0 = validation)"
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#exploratory-analysis",
    "href": "Intuit_Quickbooks_Upgrade.html#exploratory-analysis",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nResponse rate by ZIP bin\nWave-1 response varies materially across zip_bins. One segment is a clear outlier: zip_bins = 1 has a response rate of 0.2185, while all other bins fall between 0.0331 and 0.0519. Bin sizes are comparable across segments (~3,700‚Äì3,800 customers), so the difference is not driven by uneven volume.\nWave-1 response rate by ZIP bin (sorted by response rate)\n\n\n\nzip_bins\nres1_yes response rate\nn\n\n\n\n\n1\n0.219\n3,757\n\n\n18\n0.052\n3,774\n\n\n12\n0.049\n3,754\n\n\n19\n0.044\n3,726\n\n\n2\n0.043\n3,745\n\n\n4\n0.042\n3,751\n\n\n17\n0.039\n3,747\n\n\n20\n0.039\n3,748\n\n\n10\n0.038\n3,742\n\n\n11\n0.038\n3,749\n\n\n7\n0.037\n3,751\n\n\n16\n0.037\n3,749\n\n\n6\n0.037\n3,748\n\n\n8\n0.037\n3,748\n\n\n9\n0.036\n3,757\n\n\n14\n0.036\n3,718\n\n\n15\n0.035\n3,752\n\n\n3\n0.034\n3,751\n\n\n13\n0.034\n3,782\n\n\n5\n0.033\n3,751\n\n\n\nAction taken. zip_bins = 1 is a clear outlier, so I drill down to ZIP5 within this bin to identify the specific codes driving the spike.\n\n\nZIP5 drill-down within zip_bins = 1\nTwo ZIP codes within zip_bins = 1 account for the unusually high response rates at meaningful volumes:\nZIP5 segments within zip_bins = 1 with response_rate &gt; 0.30 and n &gt; 50\n\n\n\nzip5\nresponse_rate (res1_yes)\nn\n\n\n\n\n00801\n0.412470\n1668\n\n\n00804\n0.344086\n186\n\n\n\nThese ZIP codes are captured explicitly in modeling using an indicator variable (isweird) to prevent the effect from being diluted across broader ZIP binning.\n\n\nConfirming the anomaly ZIPs: 00801 and 00804\nTo determine whether the elevated response observed in zip_bins = 1 reflects a broader ZIP-level pattern, ZIP5 response rates were reviewed across the full dataset with minimum-volume filters (response rate &gt; 0.20 and n &gt; 40). Only 00801 and 00804 met these criteria. No other ZIP5 codes exhibited similarly high response rates at meaningful volume.\nHigh-response ZIP5 segments (response_rate &gt; 0.20, n &gt; 40)\n\n\n\nzip5\nresponse_rate (res1_yes)\nn\n\n\n\n\n00801\n0.412470\n1668\n\n\n00804\n0.344086\n186\n\n\n\nThese two ZIP codes are treated as a distinct segment in modeling through an indicator feature."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#modeling-approach",
    "href": "Intuit_Quickbooks_Upgrade.html#modeling-approach",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Modeling Approach",
    "text": "Modeling Approach\n\nBaselines: logistic regression (LR1‚ÄìLR2)\n\nFeature engineering: anomaly indicator (isweird)\nA binary feature, isweird, flags whether a customer is in the anomalous ZIP codes identified above:\n\nisweird = 1 if zip5 ‚àà {00801, 00804}, otherwise 0.\n\nThis design isolates a localized geographic effect without relying solely on broader ZIP binning.\n\n\nBaseline comparison: impact of isweird\nTo quantify the incremental value of the anomaly indicator, two logistic regression models were evaluated on the validation sample:\n\nLR1: baseline model using the original predictors\n\nLR2: LR1 plus isweird\n\nBoth models use the same preprocessing (categorical encoding for discrete fields).\n\nValidation confusion matrices\nLR1 (baseline)\n- TP = 749, FP = 6,188, TN = 15,209, FN = 354\nLR2 (+ isweird)\n- TP = 743, FP = 5,736, TN = 15,661, FN = 360\n\n\nValidation profit metrics\nProfit comparison on validation sample\n\n\n\nModel\nProfit (validation)\nProjected profit\nROME\n\n\n\n\nLR1\n35,158.83\n1,192,795.11\n3.59\n\n\nLR2\n35,444.61\n1,202,490.81\n3.88\n\n\n\nInterpretation. Adding isweird improves profit and ROME relative to the baseline. The gain is driven primarily by a reduction in false positives (FP decreases from 6,188 to 5,736), which improves mailing efficiency without materially changing the count of false negatives.\n\n\n\nLR1: baseline logistic regression\nAdding the anomaly indicator (isweird) improved both validation profit and ROME relative to the baseline logistic regression, indicating better campaign efficiency under the profit framework. To interpret what the baseline model is learning, Table X summarizes the key signals from the LR1 coefficient output.\n\nModel fit (LR1)\n\nAUC: 0.755\n\nMcFadden pseudo R¬≤: 0.114\n\nOverall model test: œá¬≤(29) = 2289.8, p &lt; 0.001\n\nSample size: 52,500 (training)\n\nThese statistics indicate the model has meaningful discriminative power for ranking customers, although the pseudo R¬≤ suggests substantial unexplained variation remains‚Äîtypical for response modeling with sparse outcomes.\n\n\nKey drivers (LR1)\nOdds ratios (OR) below are interpreted holding other variables constant:\n\nPrior upgrade behavior (upgraded=1): OR = 2.616 (p &lt; 0.001)\nCustomers with prior upgrade history are much more likely to respond to the upgrade offer.\nCurrently on version 1 (version1=1): OR = 2.113 (p &lt; 0.001)\nBeing on an older version is strongly associated with upgrade likelihood.\nOrdering frequency (numords): OR = 1.259 per additional order (p &lt; 0.001)\nMore prior orders correlate with higher probability of responding.\nRecency (last): OR = 0.957 per additional month since last order (p &lt; 0.001)\nLonger time since last purchase reduces response likelihood, consistent with customer inactivity.\nTax product ownership (owntaxprod=1): OR = 1.356 (p = 0.003)\nOwnership of related products is associated with higher response propensity.\nSpend (dollars): OR ‚âà 1.001 per dollar (p &lt; 0.001)\nThe per-dollar effect is small by construction; the practical impact depends on the spend range across customers.\n\n\n\nLimited effects (LR1)\n\nGender (sex) and business flag (bizflag) are not statistically significant in the baseline specification.\nTenure since original purchase (sincepurch) is not significant once recency and purchase behavior are included.\n\n\n\n\nGeographic effects (LR1)\nzip_bins shows large differences relative to the reference bin, with most bins having substantially lower odds than the reference category. This aligns with the earlier EDA result that a localized geographic segment behaves differently and supports isolating that segment explicitly (rather than relying on binning alone).\nFigure 1. LR1 prediction profiles across key predictors\n\nResponse probability is dominated by geography: zip_bins = 1 is a clear outlier, while other bins remain near a low baseline. Behavioral variables show predictable structure‚Äîprobability increases with numords and is higher for upgraded = 1 and version1 = 1. Recency is strongly negative: response probability declines as last increases. sex, bizflag, and sincepurch are nearly flat over the plotted ranges, suggesting limited incremental value in this specification.\nFigure 2. LR1 permutation importance (AUC decrease)\n\nPermutation importance ranks zip_bins as the strongest driver of model discrimination, followed by upgraded and last. numords and version1 provide moderate contribution; remaining variables add minimal lift. The concentration of signal in the geography feature supports isolating the outlier ZIP behavior explicitly (via isweird) rather than relying on broad ZIP binning alone.\n\n\nLR2: enhanced logistic regression (anomalous ZIP isolation)\nLR2 extends the baseline model by adding isweird, a binary indicator for ZIP codes 00801 and 00804. This feature captures a localized geographic segment with unusually high wave-1 response.\n\nModel fit (LR2)\n\nAUC: 0.768 (vs 0.755 in LR1)\n\nMcFadden pseudo R¬≤: 0.148 (vs 0.114 in LR1)\n\nOverall model test: œá¬≤(30) = 2965.6, p &lt; 0.001\n\nThese changes indicate improved ranking performance after accounting for the anomalous ZIP segment.\n\n\nKey coefficients (LR2)\nisweird is the dominant effect in the model:\n\nisweird = 1: OR = 24.6, p &lt; 0.001\nCustomers in ZIP codes 00801/00804 have substantially higher odds of responding, even after controlling for purchase behavior and product history.\n\nCore purchase-history signals remain stable and directionally consistent: - upgraded = 1: OR = 2.74, p &lt; 0.001 - version1 = 1: OR = 2.18, p &lt; 0.001 - numords: OR = 1.28 per additional order, p &lt; 0.001 - last: OR = 0.956 per additional month since last order, p &lt; 0.001 - owntaxprod = 1: OR = 1.38, p = 0.002\nAfter introducing isweird, most zip_bins coefficients shrink toward neutrality and many become statistically insignificant, indicating that the earlier geographic pattern was primarily driven by the two outlier ZIP codes rather than broad differences across bins.\n\n\nVariable importance\nFigure confirms the same conclusion from a predictive standpoint. Permuting isweird produces the largest AUC degradation, followed by upgraded and last. zip_bins contributes relatively little once the outlier ZIP behavior is modeled explicitly.\n\n\n\n\nFeature refinement: version path construction\nFigure 3. LR2 permutation importance (AUC decrease)\n\nisweird is the dominant driver of LR2 discrimination: permuting it produces the largest AUC drop, far exceeding any other feature. After isolating the 00801/00804 segment, zip_bins contributes little incremental lift, while upgraded and purchase recency (last) remain the strongest broad-based predictors outside the anomaly. This supports modeling the outlier ZIP behavior explicitly with isweird rather than relying on ZIP-bin effects that would blur a concentrated segment signal.\nFigure 4. LR2 prediction profiles across key predictors\n\nAfter adding isweird, the geographic spike shifts from zip_bins to the anomaly indicator. Predicted probability increases sharply when isweird = 1, while the zip_bins profile becomes largely flat, indicating limited incremental segmentation value once the outlier ZIPs are modeled explicitly. Purchase-history signals remain stable: probability increases with numords, and is higher for upgraded = 1 and version1 = 1, while last remains strongly negative.\nOn the validation sample, LR2 (baseline predictors + isweird, with zip_bins retained) achieves profit = 35,444.61 and ROME = 3.88, with projected profit of 1,202,490.81. The confusion matrix is TP = 743, FP = 5,736, TN = 15,661, FN = 360. Relative to LR1, the primary change is a reduction in false positives (fewer low-probability customers mailed), which improves campaign efficiency while keeping overall capture of responders comparable.\n\nConsolidating version1 and upgraded\nFigure 5. LR2 prediction profiles for key predictors\n\nThe prediction profiles show positive shifts for both version1 = 1 and upgraded = 1, but these fields describe overlapping aspects of product history and are not mutually exclusive. To make version behavior interpretable and usable for targeting, the two indicators are recoded into a single categorical feature, version_upgrade, representing mutually exclusive version paths.\nDefinition of version_upgrade (derived from version1 and upgraded)\n\nversion1 = 1, upgraded = 0 ‚Üí category 0: currently on version 1, no prior upgrade (n = 16,050)\n\nversion1 = 0, upgraded = 1 ‚Üí category 1: upgraded to version 2 (n = 15,629)\n\nversion1 = 0, upgraded = 0 ‚Üí category 2: directly purchased version 2 (n = 43,321)\n\nversion1 = 1, upgraded = 1 ‚Üí not observed in the dataset\n\nThis recode removes ambiguity in interpretation and prevents the model from splitting a single business concept across two correlated indicators.\n\n\nDefining version_upgrade\nversion1 and upgraded describe overlapping aspects of product history and are difficult to interpret separately. They are recoded into a single categorical feature, version_upgrade, representing mutually exclusive version paths:\n\nversion1 = 1, upgraded = 0 ‚Üí version_upgrade = 0: currently on version 1, no prior upgrade (n = 16,050)\n\nversion1 = 0, upgraded = 1 ‚Üí version_upgrade = 1: upgraded to version 2 (n = 15,629)\n\nversion1 = 0, upgraded = 0 ‚Üí version_upgrade = 2: directly purchased version 2 (n = 43,321)\n\nversion1 = 1, upgraded = 1 ‚Üí not observed in the dataset\n\nThis change is made for interpretability and reporting consistency. Model scoring is unchanged when version1 and upgraded are replaced by version_upgrade.\nFigure 6. Predicted response by version path (version_upgrade)\n\nCustomers who previously upgraded to version 2 have the highest predicted response. Customers still on version 1 score next. Customers who directly purchased version 2 have the lowest predicted response, indicating lower incremental upgrade propensity under the wave-1 offer.\n\n\n\nBenchmark: mail-to-all strategy\nA mail-to-all (‚Äúspam‚Äù) strategy was evaluated as a benchmark to ensure that targeting adds incremental value versus sending wave-2 to every eligible business.\nOn the validation sample, mailing to all customers yields the following confusion matrix:\n\nTP = 1,103\n\nFP = 21,397\n\nTN = 0\n\nFN = 0\n\nFinancial results for the mail-to-all benchmark are:\n\nProfit (validation): 34,455.00\n\nProjected profit: 1,168,918.80\n\nROME: 1.09\n\nThis benchmark underperforms the targeted logistic regression approaches (e.g., LR2), indicating that selective mailing improves profitability by avoiding low-probability customers. As a result, subsequent work focuses on models that improve ranking quality beyond logistic regression, including neural network classifiers and interaction specifications.\n\n\nNeural network exploration\nA simple multilayer perceptron (MLP) classifier with a single hidden unit was trained on the same inputs used in the logistic model (including isweird and version_upgrade). The objective of this step was to test whether a nonlinear learner identifies materially different structure or improves ranking beyond the logistic specification.\nFigure 7. MLP (1-unit) prediction profiles across key predictors\n\nPredicted response increases sharply for isweird = 1. Outside the anomaly segment, effects are modest and mostly monotone: higher numords raises response, and larger last (less recent) lowers response. Differences across version_upgrade remain visible, while sex, bizflag, sincepurch, and most zip_bins variation are small in the plotted range.\nFigure 8. MLP (1-unit) permutation importance (AUC decrease)\n\nPermutation importance is concentrated in isweird, followed by last and version_upgrade, with numords providing a smaller lift. Most zip_bins indicators contribute little once the anomaly ZIP segment is captured directly.\nFigure 9. MLP permutation importance with a larger hidden layer\n\nAcross both configurations, isweird remains the largest contributor to discrimination, confirming that the anomaly ZIP segment is the strongest signal in the data. Increasing model complexity shifts the relative importance among several features‚Äîmost noticeably version_upgrade, last, sincepurch, and sex‚Äîindicating that nonlinear interactions may be present even when the top driver is unchanged. These shifts are treated as hypothesis-generating: they motivate testing explicit interaction terms in a constrained model (e.g., logistic regression with selected interactions) rather than relying on depth alone.\n\nMLP check: single hidden unit\nUnder the larger MLP configuration, the relative importance of recency (last) increases and several effects appear to vary across segments rather than remaining parallel shifts. Interaction plots highlight three patterns that warrant explicit testing in a constrained model: (1) response differences by zip_bins vary by sex, (2) the uplift from isweird differs across version paths (version_upgrade), and (3) the uplift from isweird varies with ordering behavior (numords).\nAs a screening step, the MLP with a larger hidden layer was evaluated on the validation sample to confirm it remains a viable ranking model. Validation results are TP = 666, FP = 5,398, TN = 15,999, FN = 437 with projected profit = 1,065,600.97 and ROME = 3.67. This underperforms LR2 on projected profit, so the MLP is used here primarily to motivate interaction candidates rather than as the final scoring model.\n\n\n\nLR3: logistic regression with interactions\nDeeper MLP configurations did not deliver a profit lift over the tuned logistic baseline, so interaction effects were tested directly within logistic regression. LR3 extends LR2 by adding three interaction specifications: sex √ó zip_bins, isweird √ó version_upgrade, and isweird √ó numords.\nOn the validation sample, LR3 improves results versus LR2: - Profit (validation): 35,940.90\n- Projected profit: 1,219,324.02\n- ROME: 3.92\n\nWhat changed in LR3\n\nThe anomaly ZIP segment remains the dominant effect: isweird OR = 22.22 (p &lt; 0.001).\nVersion path separation remains strong: version_upgrade[1] OR = 1.27 (p &lt; 0.001) and version_upgrade[2] OR = 0.42 (p &lt; 0.001) relative to the reference category.\nAmong the interaction terms, the clearest incremental effect is isweird √ó version_upgrade[2] OR = 1.62 (p = 0.002), indicating that within the anomaly ZIP segment, customers who directly purchased version 2 exhibit higher incremental upgrade propensity than the baseline version path.\nisweird √ó numords is not supported (p = 0.296), suggesting that the anomaly ZIP uplift is largely additive with respect to ordering frequency.\n\nSeveral sex √ó zip_bins coefficients are statistically significant, implying that geographic differences are not uniform across gender categories in some bins. Because this interaction family introduces many parameters, its value is assessed primarily by validation profit rather than by individual coefficient significance.\n\n\nModel specification and validation performance\nMLP1 is selected as the final scoring model because it delivers the highest projected profit on the validation sample among the evaluated approaches. Hyperparameters were chosen through cross-validated search over network depth, regularization, and learning rate. Recall was used during tuning to avoid missing responders, and the final selection criterion was validation profit.\nThe selected configuration uses a three-layer architecture (3, 3, 3) with L2 regularization alpha = 0.0095 (solver: Adam). Validation performance:\n\nConfusion matrix: TP = 768, FP = 6,292, TN = 15,105, FN = 335\n\nProfit (validation): 36,125.40\n\nProjected profit: 1,225,587.45\n\nROME: 3.63\n\n\n\nInterpretation: permutation importance (MLP1)\nFigure 10. MLP1 permutation importance (AUC decrease)\n\nThe importance ranking is concentrated in a small set of features. isweird produces the largest AUC drop when permuted, indicating that the anomaly ZIP segment is the strongest discriminator."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#recommendation-and-expected-impact",
    "href": "Intuit_Quickbooks_Upgrade.html#recommendation-and-expected-impact",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Recommendation and Expected Impact",
    "text": "Recommendation and Expected Impact\n\nRecommended wave-2 targeting rule\nUse the final scoring model (MLP1) to rank all wave-1 non-responders by predicted response probability. To account for lower expected responsiveness in wave-2, scale predicted probabilities by 50% of the wave-1 rate. Mail to customers whose adjusted probability exceeds the campaign break-even response rate:\n\nMail if: 0.5 √ó pÃÇ(response) ‚â• 0.0235\n\nThis rule targets customers expected to generate positive incremental profit after mailing costs.\n\n\nExpected financial impact\nApplying the recommended targeting policy yields an expected projected profit of 1,225,587.45 from the wave-2 mailing under the stated wave-2 response adjustment and campaign economics. This materially outperforms a mail-to-all strategy and the logistic regression baselines evaluated during model selection.\n\n\nOperational notes\n\nThe model should be used for ranking and cutoff selection rather than as a literal probability forecast.\nPerformance should be monitored after wave-2 results arrive, with particular attention to the stability of the anomaly ZIP segment captured by isweird."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#conclusion",
    "href": "Intuit_Quickbooks_Upgrade.html#conclusion",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Conclusion",
    "text": "Conclusion\nWave-1 response behavior is concentrated in a small set of signals: a localized geographic segment (ZIPs 00801 and 00804) and purchase-history indicators such as version path, recency, and ordering frequency. These patterns were consistent across models and remained the primary drivers of response separation.\nThe selected modeling approach provides a practical scoring framework that can be reused for future waves. After wave-2, results should be reviewed to confirm that the anomaly ZIP segment and version-path effects remain stable, and the targeting cutoff should be recalibrated using observed wave-2 economics and response rates."
  },
  {
    "objectID": "Key_Drivers_Analysis.html",
    "href": "Key_Drivers_Analysis.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Clustering helps reveal patterns when there is no target variable to predict. Here, K-Means is used to explore the Palmer Penguins dataset through two measurements‚Äîbill length and flipper length‚Äîto see whether the data naturally separates into distinct groups without using species labels. After forming clusters, we compare them back to the species categories to check how well morphology alone reflects biological differences.\nK-Means is an unsupervised method that assigns each observation to one of k clusters by minimizing within-cluster variation. Because the result depends on the choice of k and on initialization, it is useful to inspect how the algorithm evolves and to evaluate multiple cluster counts rather than relying on a single run.\nThis page includes a from-scratch implementation alongside the scikit-learn version, visualizes centroid updates across iterations, and uses WCSS and silhouette score to guide the choice of k."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#introduction",
    "href": "Key_Drivers_Analysis.html#introduction",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Clustering helps reveal patterns when there is no target variable to predict. Here, K-Means is used to explore the Palmer Penguins dataset through two measurements‚Äîbill length and flipper length‚Äîto see whether the data naturally separates into distinct groups without using species labels. After forming clusters, we compare them back to the species categories to check how well morphology alone reflects biological differences.\nK-Means is an unsupervised method that assigns each observation to one of k clusters by minimizing within-cluster variation. Because the result depends on the choice of k and on initialization, it is useful to inspect how the algorithm evolves and to evaluate multiple cluster counts rather than relying on a single run.\nThis page includes a from-scratch implementation alongside the scikit-learn version, visualizes centroid updates across iterations, and uses WCSS and silhouette score to guide the choice of k."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#objective",
    "href": "Key_Drivers_Analysis.html#objective",
    "title": "Key Drivers Analysis",
    "section": "Objective",
    "text": "Objective\nThe objective of this analysis is to apply K-Means clustering to the Palmer Penguins dataset using bill length and flipper length, and to examine whether these two measurements alone reveal meaningful structure without using species labels. To make the method transparent, K-Means is implemented both from scratch and with scikit-learn, and the results are compared in terms of centroid locations and cluster assignments. We also visualize how centroids and memberships change across iterations to understand convergence behavior. Finally, we evaluate multiple values of k using within-cluster sum of squares (WCSS) and silhouette score, then interpret the selected clustering by comparing groups back to species categories after the clustering step."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#data-description",
    "href": "Key_Drivers_Analysis.html#data-description",
    "title": "Key Drivers Analysis",
    "section": "Data Description",
    "text": "Data Description\nThis project uses the Palmer Penguins dataset. Each row represents one observed penguin and includes metadata (species, island, sex, year) plus physical measurements recorded in standard units.\n\nVariables\n\nspecies: Ad√©lie, Chinstrap, or Gentoo\n\nisland: island of observation\n\nbill_length_mm: bill length (mm)\n\nbill_depth_mm: bill depth (mm)\n\nflipper_length_mm: flipper length (mm)\n\nbody_mass_g: body mass (g)\n\nsex: recorded biological sex (when available)\n\nyear: observation year\n\n\n\nFeatures used for clustering\nK-Means is fit using two continuous features:\n\nBill length (bill_length_mm)\nFlipper length (flipper_length_mm)\n\nSpecies is not used during clustering; it is used afterward to interpret how clusters align with known categories.\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\n\n\n\n\n\n\nData preview (first 5 rows)\n\n\n\n\n\nspecies island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year Adelie Torgersen 39.1 18.7 181 3750 male 2007 Adelie Torgersen 39.5 17.4 186 3800 female 2007 Adelie Torgersen 40.3 18.0 195 3250 female 2007 Adelie Torgersen 36.7 19.3 193 3450 female 2007 Adelie Torgersen 39.3 20.6 190 3650 male 2007"
  },
  {
    "objectID": "Key_Drivers_Analysis.html#k-means-clustering",
    "href": "Key_Drivers_Analysis.html#k-means-clustering",
    "title": "Key Drivers Analysis",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nSelecting the Number of Clusters\nFigure 1. Custom K-Means ‚Äî Iteration 1 \nWith randomly initialized centroids, clusters are still loosely defined and several regions overlap, especially in the middle of the plot. The centroid markers show starting positions that do not yet match the densest parts of the data.\nFigure 2. Custom K-Means ‚Äî Iteration 2 \nAfter one update, centroids shift sharply toward high-density areas and the cluster boundaries become more coherent. Most of the separation emerges here, with a clearer split between the high‚Äìflipper-length group and the two lower bands.\nFigure 3. Custom K-Means ‚Äî Final clustering (Iteration 10)\n\nIteration 10 represents the converged solution. Centroid movement is negligible relative to earlier iterations, and cluster assignments are stable. The final clustering forms three distinct groups, primarily separated by flipper length with bill length refining boundaries within clusters.\n\n\nComparison with scikit-learn implementation\nTo validate the custom K-Means implementation, the final centroids are compared with those obtained from scikit-learn. Each centroid is shown as:\n(Bill Length, Flipper Length)\n\n\n\nCluster\nCustom Implementation\nscikit-learn\n\n\n\n\n1\n(38.45, 187.05)\n(39.56, 188.14)\n\n\n2\n(47.63, 216.92)\n(48.14, 219.29)\n\n\n3\n(45.95, 196.73)\n(46.50, 201.77)\n\n\n\nThe centroid locations are close across implementations. Small differences are expected due to initialization and convergence tolerance, but both methods identify the same overall cluster structure.\n\nWithin-Cluster Sum of Squares (WCSS)\nSelecting the Number of Clusters (WCSS) {fig4_wcss}\nWCSS decreases as the number of clusters increases, which is expected because additional clusters reduce within-group variance. The largest drop occurs between k = 2 and k = 3, after which the curve begins to flatten. This ‚Äúelbow‚Äù suggests that three clusters capture most of the structure in the data without adding unnecessary complexity.\n\n\nSilhouette Score\nCluster Quality (Silhouette Score) {fig5_silhouette}\nThe silhouette score is highest at k = 2, but remains reasonably strong at k = 3. Because the WCSS elbow occurs at three clusters and the dataset contains three biological species, k = 3 provides a balanced and interpretable solution.\n\n\n\nEvaluation of Cluster Quantity\nTo select the number of clusters, WCSS and silhouette scores were computed for k = 2 to 7.\n\n\n\nK\nWCSS\nSilhouette Score\n\n\n\n\n2\n20949.79\n0.612\n\n\n3\n14269.56\n0.456\n\n\n4\n9587.14\n0.445\n\n\n5\n7597.61\n0.410\n\n\n6\n6326.31\n0.414\n\n\n7\n6030.08\n0.370\n\n\n\nAlthough the silhouette score is highest at k = 2, this solution collapses two biologically distinct groups into a single cluster. The WCSS curve shows a clear elbow at k = 3, indicating substantial variance reduction when moving from 2 to 3 clusters.\nBecause three clusters preserve meaningful morphological distinctions while still maintaining reasonable separation, k = 3 provides a more interpretable segmentation.\n\n\nFinal Clustering Result (k = 3)\n{fig6_kmeans_k3}\nWith k = 3, the algorithm separates the data into three distinct groups. The upper cluster captures penguins with the longest flippers and bills, the lower-left cluster represents shorter measurements, and the middle cluster occupies the intermediate range. Centroids (black markers) lie near the center of each group, indicating stable cluster formation."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#interpretation",
    "href": "Key_Drivers_Analysis.html#interpretation",
    "title": "Key Drivers Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nThe three clusters differ primarily along flipper length. One group shows substantially shorter flippers and bills, another shows the longest measurements, and the third occupies the intermediate range.\nCentroid values indicate that flipper length drives most of the separation, with differences exceeding 20‚Äì30 mm between groups. Cluster sizes are reasonably balanced, suggesting the segmentation is not dominated by outliers."
  }
]