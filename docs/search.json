[
  {
    "objectID": "Maximum_Likelihood_Estimation.html",
    "href": "Maximum_Likelihood_Estimation.html",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.\nInstead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#introduction",
    "href": "Maximum_Likelihood_Estimation.html#introduction",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.\nInstead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#objective",
    "href": "Maximum_Likelihood_Estimation.html#objective",
    "title": "Blueprinty Case Study",
    "section": "Objective",
    "text": "Objective\nThe objective of this project is to assess whether engineering firms that use Blueprinty’s software produce more patents than firms that do not, after accounting for firm age and regional location. The analysis aims to test whether the available data support Blueprinty’s claim that its software is associated with stronger patent performance."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#data-description",
    "href": "Maximum_Likelihood_Estimation.html#data-description",
    "title": "Blueprinty Case Study",
    "section": "Data Description",
    "text": "Data Description\n\n\n\n\n\n\nData dictionary\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded in the last 5 years (count outcome)\n\n\nregion\nFirm geographic region (categorical: Midwest, Southwest, Northwest, Northeast, etc.)\n\n\nage\nYears since incorporation (continuous)\n\n\niscustomer\nBlueprinty software user indicator (1 = customer, 0 = non-customer)\n\n\n\n\n\n\n\nDescriptive comparison: patents by customer status\n\n\n\nCustomer status\nGroup\nMean patents (5 years)\n\n\n\n\n0\nNon-customer\n3.473\n\n\n1\nCustomer\n4.133\n\n\n\nBlueprinty customers have a higher average number of patents awarded over the past five years (4.133) than non-customers (3.473). This difference is descriptive and does not account for other factors such as firm age or region, which are addressed in the regression analysis.\nFigure 1. Distribution of Patent Counts by Blueprinty Usage\n\nThe histogram shows a right-skewed distribution of patent counts, which is typical for count outcomes. Firms using Blueprinty’s software show a modest rightward shift, suggesting higher patent counts are somewhat more common among customers.\nDescriptively, Blueprinty customers average 4.133 patents over five years versus 3.473 for non-customers. This comparison is correlational: customers are not randomly selected, and differences in firm age or region may explain part of the gap. The regression analysis tests whether the association remains after controlling for these factors.\nFirm age by Blueprinty customer status\n\n\n\niscustomer\nN\nMean age\nMedian age\nSD\n\n\n\n\n0\n1019\n26.10\n25.50\n6.95\n\n\n1\n481\n26.90\n26.50\n7.81\n\n\n\nCustomer and non-customer firms have similar age distributions: customers are slightly older on average (26.90 vs 26.10 years) and have a slightly higher median age (26.5 vs 25.5). Age is therefore included as a control in the regression.\nRegion by Blueprinty customer status (counts)\n\n\n\nRegion\nNon-customer (0)\nCustomer (1)\nTotal\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nTotal\n1019\n481\n1500\n\n\n\nCustomer status varies by region, with a large share of customers located in the Northeast. Because region is related to adoption, it is controlled for in the regression to reduce confounding."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#poisson-model-specification-and-estimation",
    "href": "Maximum_Likelihood_Estimation.html#poisson-model-specification-and-estimation",
    "title": "Blueprinty Case Study",
    "section": "Poisson Model Specification and Estimation",
    "text": "Poisson Model Specification and Estimation\nWe model the number of patents awarded to each firm over a fixed five-year period. Because patents is a nonnegative count outcome, a Poisson model provides a natural starting point.\nLet (Y_i) denote the number of patents awarded to firm (i) in the last five years. We assume:\n[ Y_i () ]\nwhere () is the expected number of patents per firm over the five-year window.\nFor a sample of (n) independent firms with observed counts (y_1, y_2, , y_n), the log-likelihood function is:\n[ (y_1,,y_n) = -n+ ({i=1}^n y_i)() - {i=1}^n (y_i!) ]\nWe estimate () using maximum likelihood (MLE). This baseline model provides a benchmark before adding firm characteristics (e.g., Blueprinty usage, firm age, and region) in a regression framework.\n\nPoisson log-likelihood and the MLE\nFigure 2. Poisson Log-Likelihood vs. Lambda\n\nThis figure shows the Poisson log-likelihood as a function of \\(\\lambda\\) for the observed patent counts. The log-likelihood is maximized at the sample mean, so the Poisson MLE is:\n\\[\n\\hat{\\lambda} = \\bar{Y}.\n\\]\nFor independent observations \\(y_1,\\ldots,y_n\\), the Poisson log-likelihood is:\n\\[\n\\ell(\\lambda)\n= -n\\lambda\n+ \\left(\\sum_{i=1}^n y_i\\right)\\log(\\lambda)\n- \\sum_{i=1}^n \\log(y_i!).\n\\]\nDifferentiating and setting the derivative to zero gives:\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n= -n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^n y_i = \\bar{y}.\n\\]\nIn this dataset, \\(\\hat{\\lambda}=3.6847\\), matching \\(\\bar{y}=3.6847\\) up to rounding."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#poisson-regression-model",
    "href": "Maximum_Likelihood_Estimation.html#poisson-regression-model",
    "title": "Blueprinty Case Study",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nNext, we model five-year patent counts using a Poisson regression. Let \\(Y_i\\) be the number of patents awarded to firm \\(i\\) over the last five years:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\qquad \\log(\\lambda_i) = X_i^\\top \\beta,\n\\]\nequivalently \\(\\lambda_i = \\exp(X_i^\\top \\beta)\\). The covariate vector \\(X_i\\) includes firm age, age squared, region indicators, and Blueprinty usage (iscustomer).\nPoisson regression (GLM) results\n\n\n\nVariable\nCoefficient\nStd. Error\nz\np-value\n\n\n\n\nIntercept\n1.3447\n0.038\n35.059\n&lt;0.001\n\n\nNortheast (vs Midwest)\n0.0292\n0.044\n0.669\n0.504\n\n\nNorthwest (vs Midwest)\n-0.0176\n0.054\n-0.327\n0.744\n\n\nSouth (vs Midwest)\n0.0566\n0.053\n1.074\n0.283\n\n\nSouthwest (vs Midwest)\n0.0506\n0.047\n1.072\n0.284\n\n\nBlueprinty customer (iscustomer)\n0.2076\n0.031\n6.719\n&lt;0.001\n\n\nAge (centered) age_c\n-0.0080\n0.002\n-3.843\n&lt;0.001\n\n\nAge(^2) (centered) age_c2\n-0.0030\n0.000\n-11.513\n&lt;0.001\n\n\n\nNotes: Reference region is Midwest. Age is centered at the sample mean; age_c2 = age_c^2.\nHolding firm age and region constant, Blueprinty customers have higher expected patent counts. The coefficient on iscustomer is 0.2076 (p &lt; 0.001), implying an expected increase of about (e^{0.2076}-1 %) in patents for customers relative to non-customers.\nAge shows a concave pattern: the negative linear and squared centered terms indicate that expected patent output declines with age and falls faster at higher ages (relative to the sample mean). Regional indicators are not statistically significant, suggesting no meaningful regional differences once firm characteristics are controlled for.\nFigure 3. Poisson Regression Coefficients (95% CI)\n\nThe figure shows Poisson regression coefficient estimates with 95% confidence intervals (dashed line at 0 indicates “no effect” on the log scale). The iscustomer estimate is positive and its confidence interval does not cross zero, indicating a statistically significant association between Blueprinty usage and higher expected patent counts. Regional coefficients cluster near zero and their intervals overlap zero, suggesting no meaningful regional differences after controlling for other variables. The negative squared-age term indicates a concave relationship between age and patent output: patenting increases with age but at a diminishing rate.\n\nPoisson Regression Results\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nz value\np-value\n2.5%\n97.5%\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\nage\n0.1486\n0.0139\n10.7162\n&lt;0.001\n0.1214\n0.1758\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n&lt;0.001\n-0.0035\n-0.0025\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n&lt;0.001\n0.1470\n0.2681\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\nNote: Coefficients are on the log scale. Exponentiating a coefficient gives the incidence rate ratio (IRR).\nFigure X plots each coefficient estimate with its 95% confidence interval. The dashed vertical line at 0 represents no association on the log scale. Coefficients whose intervals do not cross 0 are statistically distinguishable from 0 at the 5% level.\nThe iscustomer effect is clearly positive and statistically significant, indicating higher expected patent counts for Blueprinty customers after controlling for age and region. Regional effects are small and not statistically significant. The positive age term and negative age_squared term indicate a concave relationship: expected patenting increases with age but the marginal gain declines at higher ages."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#model-implications",
    "href": "Maximum_Likelihood_Estimation.html#model-implications",
    "title": "Blueprinty Case Study",
    "section": "Model Implications",
    "text": "Model Implications\nModel fit and assumptions The Pearson chi-square statistic relative to degrees of freedom is approximately (2070/1492 ), suggesting mild overdispersion. As a robustness check, we report results using robust (sandwich) standard errors; the main conclusion for iscustomer is unchanged.\nFigure 4. Predicted Patents by Blueprinty Usage\n\nThis figure translates the Poisson regression into predicted five-year patent counts at representative firm ages. Across all ages shown, Blueprinty customers are predicted to produce more patents than comparable non-customers (holding other covariates fixed). The gap is largest in the mid-age range, consistent with the nonlinear age pattern captured by the positive age and negative age_squared terms."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#conclusion",
    "href": "Maximum_Likelihood_Estimation.html#conclusion",
    "title": "Blueprinty Case Study",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, these findings reinforce the value of Blueprinty’s software as a meaningful contributor to patent productivity. At the same time, they highlight the role of firm maturity in shaping innovation outcomes. While regional variation appears limited, the analysis underscores the importance of targeting firms at the right stage of development and aligning product value with their innovation capacity. These insights can guide Blueprinty’s strategic messaging and outreach efforts, particularly when engaging established firms seeking to strengthen their patent portfolios."
  },
  {
    "objectID": "Key_Drivers_Analysis.html",
    "href": "Key_Drivers_Analysis.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Clustering helps reveal patterns when there is no target variable to predict. Here, K-Means is used to explore the Palmer Penguins dataset through two measurements—bill length and flipper length—to see whether the data naturally separates into distinct groups without using species labels. After forming clusters, we compare them back to the species categories to check how well morphology alone reflects biological differences.\nK-Means is an unsupervised method that assigns each observation to one of k clusters by minimizing within-cluster variation. Because the result depends on the choice of k and on initialization, it is useful to inspect how the algorithm evolves and to evaluate multiple cluster counts rather than relying on a single run.\nThis page includes a from-scratch implementation alongside the scikit-learn version, visualizes centroid updates across iterations, and uses WCSS and silhouette score to guide the choice of k."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#introduction",
    "href": "Key_Drivers_Analysis.html#introduction",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Clustering helps reveal patterns when there is no target variable to predict. Here, K-Means is used to explore the Palmer Penguins dataset through two measurements—bill length and flipper length—to see whether the data naturally separates into distinct groups without using species labels. After forming clusters, we compare them back to the species categories to check how well morphology alone reflects biological differences.\nK-Means is an unsupervised method that assigns each observation to one of k clusters by minimizing within-cluster variation. Because the result depends on the choice of k and on initialization, it is useful to inspect how the algorithm evolves and to evaluate multiple cluster counts rather than relying on a single run.\nThis page includes a from-scratch implementation alongside the scikit-learn version, visualizes centroid updates across iterations, and uses WCSS and silhouette score to guide the choice of k."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#objective",
    "href": "Key_Drivers_Analysis.html#objective",
    "title": "Key Drivers Analysis",
    "section": "Objective",
    "text": "Objective\nThe objective of this analysis is to apply K-Means clustering to the Palmer Penguins dataset using bill length and flipper length, and to examine whether these two measurements alone reveal meaningful structure without using species labels. To make the method transparent, K-Means is implemented both from scratch and with scikit-learn, and the results are compared in terms of centroid locations and cluster assignments. We also visualize how centroids and memberships change across iterations to understand convergence behavior. Finally, we evaluate multiple values of k using within-cluster sum of squares (WCSS) and silhouette score, then interpret the selected clustering by comparing groups back to species categories after the clustering step."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#data-description",
    "href": "Key_Drivers_Analysis.html#data-description",
    "title": "Key Drivers Analysis",
    "section": "Data Description",
    "text": "Data Description\nThis project uses the Palmer Penguins dataset. Each row represents one observed penguin and includes metadata (species, island, sex, year) plus physical measurements recorded in standard units.\n\nVariables\n\nspecies: Adélie, Chinstrap, or Gentoo\n\nisland: island of observation\n\nbill_length_mm: bill length (mm)\n\nbill_depth_mm: bill depth (mm)\n\nflipper_length_mm: flipper length (mm)\n\nbody_mass_g: body mass (g)\n\nsex: recorded biological sex (when available)\n\nyear: observation year\n\n\n\nFeatures used for clustering\nK-Means is fit using two continuous features:\n\nBill length (bill_length_mm)\nFlipper length (flipper_length_mm)\n\nSpecies is not used during clustering; it is used afterward to interpret how clusters align with known categories.\n\n\n\n\n\n\nData Preview (first 5 rows)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007"
  },
  {
    "objectID": "Key_Drivers_Analysis.html#k-means-clustering",
    "href": "Key_Drivers_Analysis.html#k-means-clustering",
    "title": "Key Drivers Analysis",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\n\nSelecting the Number of Clusters\nFigure 1. Custom K-Means — Iteration 1 \nWith randomly initialized centroids, clusters are still loosely defined and several regions overlap, especially in the middle of the plot. The centroid markers show starting positions that do not yet match the densest parts of the data.\nFigure 2. Custom K-Means — Iteration 2 \nAfter one update, centroids shift sharply toward high-density areas and the cluster boundaries become more coherent. Most of the separation emerges here, with a clearer split between the high–flipper-length group and the two lower bands.\nFigure 3. Custom K-Means — Final clustering (Iteration 10)\n\nIteration 10 represents the converged solution. Centroid movement is negligible relative to earlier iterations, and cluster assignments are stable. The final clustering forms three distinct groups, primarily separated by flipper length with bill length refining boundaries within clusters.\n\n\nComparison with scikit-learn implementation\nTo validate the custom K-Means implementation, the final centroids are compared with those obtained from scikit-learn. Each centroid is shown as:\n(Bill Length, Flipper Length)\n\n\n\nCluster\nCustom Implementation\nscikit-learn\n\n\n\n\n1\n(38.45, 187.05)\n(39.56, 188.14)\n\n\n2\n(47.63, 216.92)\n(48.14, 219.29)\n\n\n3\n(45.95, 196.73)\n(46.50, 201.77)\n\n\n\nThe centroid locations are close across implementations. Small differences are expected due to initialization and convergence tolerance, but both methods identify the same overall cluster structure.\n\nWithin-Cluster Sum of Squares (WCSS)\nSelecting the Number of Clusters (WCSS) \nWCSS decreases as the number of clusters increases, which is expected because additional clusters reduce within-group variance. The largest drop occurs between k = 2 and k = 3, after which the curve begins to flatten. This “elbow” suggests that three clusters capture most of the structure in the data without adding unnecessary complexity.\n\n\nSilhouette Score\nCluster Quality (Silhouette Score) \nThe silhouette score is highest at k = 2, but remains reasonably strong at k = 3. Because the WCSS elbow occurs at three clusters and the dataset contains three biological species, k = 3 provides a balanced and interpretable solution.\n\n\n\nEvaluation of Cluster Quantity\nTo select the number of clusters, WCSS and silhouette scores were computed for k = 2 to 7.\n\n\n\nK\nWCSS\nSilhouette Score\n\n\n\n\n2\n20949.79\n0.612\n\n\n3\n14269.56\n0.456\n\n\n4\n9587.14\n0.445\n\n\n5\n7597.61\n0.410\n\n\n6\n6326.31\n0.414\n\n\n7\n6030.08\n0.370\n\n\n\nAlthough the silhouette score is highest at k = 2, this solution collapses two biologically distinct groups into a single cluster. The WCSS curve shows a clear elbow at k = 3, indicating substantial variance reduction when moving from 2 to 3 clusters.\nBecause three clusters preserve meaningful morphological distinctions while still maintaining reasonable separation, k = 3 provides a more interpretable segmentation.\n\n\nFinal Clustering Result (k = 3)\n\n\n\nK-Means clustering result (k = 3)\n\n\nWith k = 3, the algorithm separates the data into three distinct groups. The upper cluster captures penguins with the longest flippers and bills, the lower-left cluster represents shorter measurements, and the middle cluster occupies the intermediate range. Centroids (black markers) lie near the center of each group, indicating stable cluster formation."
  },
  {
    "objectID": "Key_Drivers_Analysis.html#interpretation",
    "href": "Key_Drivers_Analysis.html#interpretation",
    "title": "Key Drivers Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nThe three clusters differ primarily along flipper length. One group shows substantially shorter flippers and bills, another shows the longest measurements, and the third occupies the intermediate range.\nCentroid values indicate that flipper length drives most of the separation, with differences exceeding 20–30 mm between groups. Cluster sizes are reasonably balanced, suggesting the segmentation is not dominated by outliers."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html",
    "href": "Intuit_Quickbooks_Upgrade.html",
    "title": "Intuit Quickbooks Upgrade",
    "section": "",
    "text": "This project builds a predictive response model for Intuit’s QuickBooks upgrade upsell campaign. The dataset (intuit75k.parquet) contains 75,000 small businesses randomly sampled from the 801,821 businesses that received the wave-1 mailing. Each business record includes customer attributes and purchase history that can be used to estimate the likelihood of upgrading.\nThe response variable res1 indicates whether a business purchased QuickBooks version 3.0 through Intuit Direct after receiving the wave-1 offer. Because the case materials have been modified (variables added, removed, and recoded), the analysis relies on the updated variable definitions provided in this report rather than Exhibit 3 in the course reader."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#introduction",
    "href": "Intuit_Quickbooks_Upgrade.html#introduction",
    "title": "Intuit Quickbooks Upgrade",
    "section": "",
    "text": "This project builds a predictive response model for Intuit’s QuickBooks upgrade upsell campaign. The dataset (intuit75k.parquet) contains 75,000 small businesses randomly sampled from the 801,821 businesses that received the wave-1 mailing. Each business record includes customer attributes and purchase history that can be used to estimate the likelihood of upgrading.\nThe response variable res1 indicates whether a business purchased QuickBooks version 3.0 through Intuit Direct after receiving the wave-1 offer. Because the case materials have been modified (variables added, removed, and recoded), the analysis relies on the updated variable definitions provided in this report rather than Exhibit 3 in the course reader."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#objective",
    "href": "Intuit_Quickbooks_Upgrade.html#objective",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Objective",
    "text": "Objective\nDeliver a wave-2 targeting approach that increases incremental profit versus untargeted mailing. Specifically, we (1) estimate each business’s likelihood of responding to wave-2, (2) translate predicted responses into expected profit using campaign economics, and (3) recommend a mailing cutoff rule that sends offers only when expected return exceeds break-even."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#data-and-problem-setup",
    "href": "Intuit_Quickbooks_Upgrade.html#data-and-problem-setup",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Data and Problem Setup",
    "text": "Data and Problem Setup\n\nData source and unit of analysis\nEach row represents a small-business customer in Intuit Direct’s mailing file. The analysis uses the sample provided in intuit75k.parquet, drawn from wave-1 recipients.\n\n\nOutcome definition\n\nres1 is the response indicator for the wave-1 mailing (“Yes” = purchased QuickBooks 3.0 through Intuit Direct; “No” otherwise).\n\nThe modeling task is a binary classification problem used for ranking customers for wave-2 targeting.\n\n\n\nPredictor variables (summary)\nThe predictors describe customer location, business indicators, and purchase behavior over the prior 36 months, plus product/version history. A full data dictionary is included below.\n\n\nTrain/validation split\n\ntraining = 1 indicates the training sample (70%).\n\ntraining = 0 indicates the validation sample (30%).\nModel selection and thresholding decisions are based on validation performance.\n\n\n\n\n\n\n\nData dictionary\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nid\nSmall business customer ID\n\n\nzip5\n5-digit ZIP code (00000 = unknown, 99999 = international ZIPs)\n\n\nzip_bins\nZIP-code bins (~20 roughly equal-sized bins from lowest to highest ZIP number)\n\n\nsex\nGender: Female, Male, or Unknown\n\n\nbizflag\nBusiness flag: address contains a business name (1 = yes, 0 = no/unknown)\n\n\nnumords\nNumber of Intuit Direct orders in the previous 36 months\n\n\ndollars\nTotal dollars ordered from Intuit Direct in the previous 36 months\n\n\nlast\nMonths since last Intuit Direct order (within the previous 36 months)\n\n\nsincepurch\nMonths since original (non-upgrade) QuickBooks purchase\n\n\nversion1\n1 if current QuickBooks is version 1; 0 if version 2\n\n\nowntaxprod\n1 if customer purchased tax software; 0 otherwise\n\n\nupgraded\n1 if customer upgraded previously (vs. purchasing a base version); 0 otherwise\n\n\nres1\nResponse to wave-1 mailing (“Yes” / “No”)\n\n\ntraining\nSplit indicator (1 = training, 0 = validation)"
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#exploratory-analysis",
    "href": "Intuit_Quickbooks_Upgrade.html#exploratory-analysis",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\n\nResponse rate by ZIP bin\nWave-1 response varies materially across zip_bins. One segment is a clear outlier: zip_bins = 1 has a response rate of 0.2185, while all other bins fall between 0.0331 and 0.0519. Bin sizes are comparable across segments (~3,700–3,800 customers), so the difference is not driven by uneven volume.\nWave-1 response rate by ZIP bin (sorted by response rate)\n\n\n\nzip_bins\nres1_yes response rate\nn\n\n\n\n\n1\n0.219\n3,757\n\n\n18\n0.052\n3,774\n\n\n12\n0.049\n3,754\n\n\n19\n0.044\n3,726\n\n\n2\n0.043\n3,745\n\n\n4\n0.042\n3,751\n\n\n17\n0.039\n3,747\n\n\n20\n0.039\n3,748\n\n\n10\n0.038\n3,742\n\n\n11\n0.038\n3,749\n\n\n7\n0.037\n3,751\n\n\n16\n0.037\n3,749\n\n\n6\n0.037\n3,748\n\n\n8\n0.037\n3,748\n\n\n9\n0.036\n3,757\n\n\n14\n0.036\n3,718\n\n\n15\n0.035\n3,752\n\n\n3\n0.034\n3,751\n\n\n13\n0.034\n3,782\n\n\n5\n0.033\n3,751\n\n\n\nAction taken. zip_bins = 1 is a clear outlier, so I drill down to ZIP5 within this bin to identify the specific codes driving the spike.\n\n\nZIP5 drill-down within zip_bins = 1\nTwo ZIP codes within zip_bins = 1 account for the unusually high response rates at meaningful volumes:\nZIP5 segments within zip_bins = 1 with response_rate &gt; 0.30 and n &gt; 50\n\n\n\nzip5\nresponse_rate (res1_yes)\nn\n\n\n\n\n00801\n0.412470\n1668\n\n\n00804\n0.344086\n186\n\n\n\nThese ZIP codes are captured explicitly in modeling using an indicator variable (isweird) to prevent the effect from being diluted across broader ZIP binning.\n\n\nConfirming the anomaly ZIPs: 00801 and 00804\nTo determine whether the elevated response observed in zip_bins = 1 reflects a broader ZIP-level pattern, ZIP5 response rates were reviewed across the full dataset with minimum-volume filters (response rate &gt; 0.20 and n &gt; 40). Only 00801 and 00804 met these criteria. No other ZIP5 codes exhibited similarly high response rates at meaningful volume.\nHigh-response ZIP5 segments (response_rate &gt; 0.20, n &gt; 40)\n\n\n\nzip5\nresponse_rate (res1_yes)\nn\n\n\n\n\n00801\n0.412470\n1668\n\n\n00804\n0.344086\n186\n\n\n\nThese two ZIP codes are treated as a distinct segment in modeling through an indicator feature."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#modeling-approach",
    "href": "Intuit_Quickbooks_Upgrade.html#modeling-approach",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Modeling Approach",
    "text": "Modeling Approach\n\nBaselines: logistic regression (LR1–LR2)\n\nFeature engineering: anomaly indicator (isweird)\nA binary feature, isweird, flags whether a customer is in the anomalous ZIP codes identified above:\n\nisweird = 1 if zip5 ∈ {00801, 00804}, otherwise 0.\n\nThis design isolates a localized geographic effect without relying solely on broader ZIP binning.\n\n\nBaseline comparison: impact of isweird\nTo quantify the incremental value of the anomaly indicator, two logistic regression models were evaluated on the validation sample:\n\nLR1: baseline model using the original predictors\n\nLR2: LR1 plus isweird\n\nBoth models use the same preprocessing (categorical encoding for discrete fields).\n\nValidation confusion matrices\nLR1 (baseline)\n- TP = 749, FP = 6,188, TN = 15,209, FN = 354\nLR2 (+ isweird)\n- TP = 743, FP = 5,736, TN = 15,661, FN = 360\n\n\nValidation profit metrics\nProfit comparison on validation sample\n\n\n\nModel\nProfit (validation)\nProjected profit\nROME\n\n\n\n\nLR1\n35,158.83\n1,192,795.11\n3.59\n\n\nLR2\n35,444.61\n1,202,490.81\n3.88\n\n\n\nInterpretation. Adding isweird improves profit and ROME relative to the baseline. The gain is driven primarily by a reduction in false positives (FP decreases from 6,188 to 5,736), which improves mailing efficiency without materially changing the count of false negatives.\n\n\n\nLR1: baseline logistic regression\nAdding the anomaly indicator (isweird) improved both validation profit and ROME relative to the baseline logistic regression, indicating better campaign efficiency under the profit framework. To interpret what the baseline model is learning, Table X summarizes the key signals from the LR1 coefficient output.\n\nModel fit (LR1)\n\nAUC: 0.755\n\nMcFadden pseudo R²: 0.114\n\nOverall model test: χ²(29) = 2289.8, p &lt; 0.001\n\nSample size: 52,500 (training)\n\nThese statistics indicate the model has meaningful discriminative power for ranking customers, although the pseudo R² suggests substantial unexplained variation remains—typical for response modeling with sparse outcomes.\n\n\nKey drivers (LR1)\nOdds ratios (OR) below are interpreted holding other variables constant:\n\nPrior upgrade behavior (upgraded=1): OR = 2.616 (p &lt; 0.001)\nCustomers with prior upgrade history are much more likely to respond to the upgrade offer.\nCurrently on version 1 (version1=1): OR = 2.113 (p &lt; 0.001)\nBeing on an older version is strongly associated with upgrade likelihood.\nOrdering frequency (numords): OR = 1.259 per additional order (p &lt; 0.001)\nMore prior orders correlate with higher probability of responding.\nRecency (last): OR = 0.957 per additional month since last order (p &lt; 0.001)\nLonger time since last purchase reduces response likelihood, consistent with customer inactivity.\nTax product ownership (owntaxprod=1): OR = 1.356 (p = 0.003)\nOwnership of related products is associated with higher response propensity.\nSpend (dollars): OR ≈ 1.001 per dollar (p &lt; 0.001)\nThe per-dollar effect is small by construction; the practical impact depends on the spend range across customers.\n\n\n\nLimited effects (LR1)\n\nGender (sex) and business flag (bizflag) are not statistically significant in the baseline specification.\nTenure since original purchase (sincepurch) is not significant once recency and purchase behavior are included.\n\n\n\n\nGeographic effects (LR1)\nzip_bins shows large differences relative to the reference bin, with most bins having substantially lower odds than the reference category. This aligns with the earlier EDA result that a localized geographic segment behaves differently and supports isolating that segment explicitly (rather than relying on binning alone).\nFigure 1. LR1 prediction profiles across key predictors\n\nResponse probability is dominated by geography: zip_bins = 1 is a clear outlier, while other bins remain near a low baseline. Behavioral variables show predictable structure—probability increases with numords and is higher for upgraded = 1 and version1 = 1. Recency is strongly negative: response probability declines as last increases. sex, bizflag, and sincepurch are nearly flat over the plotted ranges, suggesting limited incremental value in this specification.\nFigure 2. LR1 permutation importance (AUC decrease)\n\nPermutation importance ranks zip_bins as the strongest driver of model discrimination, followed by upgraded and last. numords and version1 provide moderate contribution; remaining variables add minimal lift. The concentration of signal in the geography feature supports isolating the outlier ZIP behavior explicitly (via isweird) rather than relying on broad ZIP binning alone.\n\n\nLR2: enhanced logistic regression (anomalous ZIP isolation)\nLR2 extends the baseline model by adding isweird, a binary indicator for ZIP codes 00801 and 00804. This feature captures a localized geographic segment with unusually high wave-1 response.\n\nModel fit (LR2)\n\nAUC: 0.768 (vs 0.755 in LR1)\n\nMcFadden pseudo R²: 0.148 (vs 0.114 in LR1)\n\nOverall model test: χ²(30) = 2965.6, p &lt; 0.001\n\nThese changes indicate improved ranking performance after accounting for the anomalous ZIP segment.\n\n\nKey coefficients (LR2)\nisweird is the dominant effect in the model:\n\nisweird = 1: OR = 24.6, p &lt; 0.001\nCustomers in ZIP codes 00801/00804 have substantially higher odds of responding, even after controlling for purchase behavior and product history.\n\nCore purchase-history signals remain stable and directionally consistent: - upgraded = 1: OR = 2.74, p &lt; 0.001 - version1 = 1: OR = 2.18, p &lt; 0.001 - numords: OR = 1.28 per additional order, p &lt; 0.001 - last: OR = 0.956 per additional month since last order, p &lt; 0.001 - owntaxprod = 1: OR = 1.38, p = 0.002\nAfter introducing isweird, most zip_bins coefficients shrink toward neutrality and many become statistically insignificant, indicating that the earlier geographic pattern was primarily driven by the two outlier ZIP codes rather than broad differences across bins.\n\n\nVariable importance\nFigure confirms the same conclusion from a predictive standpoint. Permuting isweird produces the largest AUC degradation, followed by upgraded and last. zip_bins contributes relatively little once the outlier ZIP behavior is modeled explicitly.\n\n\n\n\nFeature refinement: version path construction\nFigure 3. LR2 permutation importance (AUC decrease)\n\nisweird is the dominant driver of LR2 discrimination: permuting it produces the largest AUC drop, far exceeding any other feature. After isolating the 00801/00804 segment, zip_bins contributes little incremental lift, while upgraded and purchase recency (last) remain the strongest broad-based predictors outside the anomaly. This supports modeling the outlier ZIP behavior explicitly with isweird rather than relying on ZIP-bin effects that would blur a concentrated segment signal.\nFigure 4. LR2 prediction profiles across key predictors\n\nAfter adding isweird, the geographic spike shifts from zip_bins to the anomaly indicator. Predicted probability increases sharply when isweird = 1, while the zip_bins profile becomes largely flat, indicating limited incremental segmentation value once the outlier ZIPs are modeled explicitly. Purchase-history signals remain stable: probability increases with numords, and is higher for upgraded = 1 and version1 = 1, while last remains strongly negative.\nOn the validation sample, LR2 (baseline predictors + isweird, with zip_bins retained) achieves profit = 35,444.61 and ROME = 3.88, with projected profit of 1,202,490.81. The confusion matrix is TP = 743, FP = 5,736, TN = 15,661, FN = 360. Relative to LR1, the primary change is a reduction in false positives (fewer low-probability customers mailed), which improves campaign efficiency while keeping overall capture of responders comparable.\n\nConsolidating version1 and upgraded\nFigure 5. LR2 prediction profiles for key predictors\n\nThe prediction profiles show positive shifts for both version1 = 1 and upgraded = 1, but these fields describe overlapping aspects of product history and are not mutually exclusive. To make version behavior interpretable and usable for targeting, the two indicators are recoded into a single categorical feature, version_upgrade, representing mutually exclusive version paths.\nDefinition of version_upgrade (derived from version1 and upgraded)\n\nversion1 = 1, upgraded = 0 → category 0: currently on version 1, no prior upgrade (n = 16,050)\n\nversion1 = 0, upgraded = 1 → category 1: upgraded to version 2 (n = 15,629)\n\nversion1 = 0, upgraded = 0 → category 2: directly purchased version 2 (n = 43,321)\n\nversion1 = 1, upgraded = 1 → not observed in the dataset\n\nThis recode removes ambiguity in interpretation and prevents the model from splitting a single business concept across two correlated indicators.\n\n\nDefining version_upgrade\nversion1 and upgraded describe overlapping aspects of product history and are difficult to interpret separately. They are recoded into a single categorical feature, version_upgrade, representing mutually exclusive version paths:\n\nversion1 = 1, upgraded = 0 → version_upgrade = 0: currently on version 1, no prior upgrade (n = 16,050)\n\nversion1 = 0, upgraded = 1 → version_upgrade = 1: upgraded to version 2 (n = 15,629)\n\nversion1 = 0, upgraded = 0 → version_upgrade = 2: directly purchased version 2 (n = 43,321)\n\nversion1 = 1, upgraded = 1 → not observed in the dataset\n\nThis change is made for interpretability and reporting consistency. Model scoring is unchanged when version1 and upgraded are replaced by version_upgrade.\nFigure 6. Predicted response by version path (version_upgrade)\n\nCustomers who previously upgraded to version 2 have the highest predicted response. Customers still on version 1 score next. Customers who directly purchased version 2 have the lowest predicted response, indicating lower incremental upgrade propensity under the wave-1 offer.\n\n\n\nBenchmark: mail-to-all strategy\nA mail-to-all (“spam”) strategy was evaluated as a benchmark to ensure that targeting adds incremental value versus sending wave-2 to every eligible business.\nOn the validation sample, mailing to all customers yields the following confusion matrix:\n\nTP = 1,103\n\nFP = 21,397\n\nTN = 0\n\nFN = 0\n\nFinancial results for the mail-to-all benchmark are:\n\nProfit (validation): 34,455.00\n\nProjected profit: 1,168,918.80\n\nROME: 1.09\n\nThis benchmark underperforms the targeted logistic regression approaches (e.g., LR2), indicating that selective mailing improves profitability by avoiding low-probability customers. As a result, subsequent work focuses on models that improve ranking quality beyond logistic regression, including neural network classifiers and interaction specifications.\n\n\nNeural network exploration\nA simple multilayer perceptron (MLP) classifier with a single hidden unit was trained on the same inputs used in the logistic model (including isweird and version_upgrade). The objective of this step was to test whether a nonlinear learner identifies materially different structure or improves ranking beyond the logistic specification.\nFigure 7. MLP (1-unit) prediction profiles across key predictors\n\nPredicted response increases sharply for isweird = 1. Outside the anomaly segment, effects are modest and mostly monotone: higher numords raises response, and larger last (less recent) lowers response. Differences across version_upgrade remain visible, while sex, bizflag, sincepurch, and most zip_bins variation are small in the plotted range.\nFigure 8. MLP (1-unit) permutation importance (AUC decrease)\n\nPermutation importance is concentrated in isweird, followed by last and version_upgrade, with numords providing a smaller lift. Most zip_bins indicators contribute little once the anomaly ZIP segment is captured directly.\nFigure 9. MLP permutation importance with a larger hidden layer\n\nAcross both configurations, isweird remains the largest contributor to discrimination, confirming that the anomaly ZIP segment is the strongest signal in the data. Increasing model complexity shifts the relative importance among several features—most noticeably version_upgrade, last, sincepurch, and sex—indicating that nonlinear interactions may be present even when the top driver is unchanged. These shifts are treated as hypothesis-generating: they motivate testing explicit interaction terms in a constrained model (e.g., logistic regression with selected interactions) rather than relying on depth alone.\n\nMLP check: single hidden unit\nUnder the larger MLP configuration, the relative importance of recency (last) increases and several effects appear to vary across segments rather than remaining parallel shifts. Interaction plots highlight three patterns that warrant explicit testing in a constrained model: (1) response differences by zip_bins vary by sex, (2) the uplift from isweird differs across version paths (version_upgrade), and (3) the uplift from isweird varies with ordering behavior (numords).\nAs a screening step, the MLP with a larger hidden layer was evaluated on the validation sample to confirm it remains a viable ranking model. Validation results are TP = 666, FP = 5,398, TN = 15,999, FN = 437 with projected profit = 1,065,600.97 and ROME = 3.67. This underperforms LR2 on projected profit, so the MLP is used here primarily to motivate interaction candidates rather than as the final scoring model.\n\n\n\nLR3: logistic regression with interactions\nDeeper MLP configurations did not deliver a profit lift over the tuned logistic baseline, so interaction effects were tested directly within logistic regression. LR3 extends LR2 by adding three interaction specifications: sex × zip_bins, isweird × version_upgrade, and isweird × numords.\nOn the validation sample, LR3 improves results versus LR2: - Profit (validation): 35,940.90\n- Projected profit: 1,219,324.02\n- ROME: 3.92\n\nWhat changed in LR3\n\nThe anomaly ZIP segment remains the dominant effect: isweird OR = 22.22 (p &lt; 0.001).\nVersion path separation remains strong: version_upgrade[1] OR = 1.27 (p &lt; 0.001) and version_upgrade[2] OR = 0.42 (p &lt; 0.001) relative to the reference category.\nAmong the interaction terms, the clearest incremental effect is isweird × version_upgrade[2] OR = 1.62 (p = 0.002), indicating that within the anomaly ZIP segment, customers who directly purchased version 2 exhibit higher incremental upgrade propensity than the baseline version path.\nisweird × numords is not supported (p = 0.296), suggesting that the anomaly ZIP uplift is largely additive with respect to ordering frequency.\n\nSeveral sex × zip_bins coefficients are statistically significant, implying that geographic differences are not uniform across gender categories in some bins. Because this interaction family introduces many parameters, its value is assessed primarily by validation profit rather than by individual coefficient significance.\n\n\nModel specification and validation performance\nMLP1 is selected as the final scoring model because it delivers the highest projected profit on the validation sample among the evaluated approaches. Hyperparameters were chosen through cross-validated search over network depth, regularization, and learning rate. Recall was used during tuning to avoid missing responders, and the final selection criterion was validation profit.\nThe selected configuration uses a three-layer architecture (3, 3, 3) with L2 regularization alpha = 0.0095 (solver: Adam). Validation performance:\n\nConfusion matrix: TP = 768, FP = 6,292, TN = 15,105, FN = 335\n\nProfit (validation): 36,125.40\n\nProjected profit: 1,225,587.45\n\nROME: 3.63\n\n\n\nInterpretation: permutation importance (MLP1)\nFigure 10. MLP1 permutation importance (AUC decrease)\n\nThe importance ranking is concentrated in a small set of features. isweird produces the largest AUC drop when permuted, indicating that the anomaly ZIP segment is the strongest discriminator."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#recommendation-and-expected-impact",
    "href": "Intuit_Quickbooks_Upgrade.html#recommendation-and-expected-impact",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Recommendation and Expected Impact",
    "text": "Recommendation and Expected Impact\n\nRecommended wave-2 targeting rule\nUse the final scoring model (MLP1) to rank all wave-1 non-responders by predicted response probability. To account for lower expected responsiveness in wave-2, scale predicted probabilities by 50% of the wave-1 rate. Mail to customers whose adjusted probability exceeds the campaign break-even response rate:\n\nMail if: 0.5 × p̂(response) ≥ 0.0235\n\nThis rule targets customers expected to generate positive incremental profit after mailing costs.\n\n\nExpected financial impact\nApplying the recommended targeting policy yields an expected projected profit of 1,225,587.45 from the wave-2 mailing under the stated wave-2 response adjustment and campaign economics. This materially outperforms a mail-to-all strategy and the logistic regression baselines evaluated during model selection.\n\n\nOperational notes\n\nThe model should be used for ranking and cutoff selection rather than as a literal probability forecast.\nPerformance should be monitored after wave-2 results arrive, with particular attention to the stability of the anomaly ZIP segment captured by isweird."
  },
  {
    "objectID": "Intuit_Quickbooks_Upgrade.html#conclusion",
    "href": "Intuit_Quickbooks_Upgrade.html#conclusion",
    "title": "Intuit Quickbooks Upgrade",
    "section": "Conclusion",
    "text": "Conclusion\nWave-1 response behavior is concentrated in a small set of signals: a localized geographic segment (ZIPs 00801 and 00804) and purchase-history indicators such as version path, recency, and ordering frequency. These patterns were consistent across models and remained the primary drivers of response separation.\nThe selected modeling approach provides a practical scoring framework that can be reused for future waves. After wave-2, results should be reviewed to confirm that the anomaly ZIP segment and version-path effects remain stable, and the targeting cutoff should be recalibrated using observed wave-2 economics and response rates."
  },
  {
    "objectID": "Multinomial_Logit_Model.html",
    "href": "Multinomial_Logit_Model.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "Multinomial_Logit_Model.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "Multinomial_Logit_Model.html#simulate-conjoint-data",
    "href": "Multinomial_Logit_Model.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(123)\n\n# Attribute Levels \nbrand_levels = [\"N\", \"P\", \"H\"]        \nad_levels = [\"Yes\", \"No\"]             \nprice_levels = np.arange(4, 33, 4)    \n\n# All Possible Profiles \nprofiles = pd.DataFrame(\n    list(product(brand_levels, ad_levels, price_levels)),\n    columns=[\"brand\", \"ad\", \"price\"]\n)\n\n# Part-Worth Utilities \nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}        \na_util = {\"Yes\": -0.8, \"No\": 0.0}           \nprice_util = lambda p: -0.1 * p               \n\n# Simulation Parameters \nn_peeps = 100     \nn_tasks = 10      \nn_alts = 3      \n\n# Simulate One Respondent’s Data \ndef simulate_one(respondent_id: int) -&gt; pd.DataFrame:\n    tasks = []\n\n    for task_no in range(1, n_tasks + 1):\n        dat = profiles.sample(n=n_alts).copy()\n        dat.insert(0, \"task\", task_no)\n        dat.insert(0, \"resp\", respondent_id)\n\n        # Deterministic utility\n        dat[\"v\"] = (\n            dat[\"brand\"].map(b_util)\n            + dat[\"ad\"].map(a_util)\n            + price_util(dat[\"price\"])\n        )\n\n        # Gumbel-distributed noise\n        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        dat[\"u\"] = dat[\"v\"] + e\n\n        # Determine choice\n        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n\n        tasks.append(dat)\n\n    return pd.concat(tasks, ignore_index=True)\n\n# Simulate All Respondents\nconjoint_data = pd.concat(\n    [simulate_one(i) for i in range(1, n_peeps + 1)],\n    ignore_index=True\n)\n\n# Keep Only Observable Data \nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Display Result Summary \nrows, cols = conjoint_data.shape\nprint(f\"The simulated dataset contains {rows} rows and {cols} columns \"\n      f\"({n_peeps} respondents × {n_tasks} tasks × {n_alts} alternatives).\")\n\n# Peek at Data \nprint(conjoint_data.head())\n\nThe simulated dataset contains 3000 rows and 6 columns (100 respondents × 10 tasks × 3 alternatives).\n   resp  task brand   ad  price  choice\n0     1     1     P  Yes     12       0\n1     1     1     N   No     24       0\n2     1     1     P   No     12       1\n3     1     2     H   No     12       0\n4     1     2     P  Yes     24       0"
  },
  {
    "objectID": "Multinomial_Logit_Model.html#preparing-the-data-for-estimation",
    "href": "Multinomial_Logit_Model.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n   resp  task  choice brand   ad  price  brand_N  brand_P  ad_yes  task_id\n0     1     1       1     N  Yes     28        1        0       1        0\n1     1     1       0     H  Yes     16        0        0       1        0\n2     1     1       0     P  Yes     16        0        1       1        0\n3     1     2       0     N  Yes     32        1        0       1        1\n4     1     2       1     P  Yes     16        0        1       1        1\n\n\nEach task_id represents a choice task with three alternatives. For example, task_id = 0 (resp = 1, task = 1) includes three options from brands N, H, and P, all with ads and different prices. The first row (brand = N, price = 28) has choice = 1, indicating it was selected. Dummy variables for brand and ad presence are created, with Hulu and ad-free as baselines. The data is now formatted for model estimation."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#estimation-via-maximum-likelihood",
    "href": "Multinomial_Logit_Model.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLE Coefficient\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.941195\n0.109961\n0.725672\n1.156718\n\n\nbrand_P\n0.501616\n0.120088\n0.266242\n0.736989\n\n\nad_yes\n-0.731994\n0.088279\n-0.905021\n-0.558968\n\n\nprice\n-0.099480\n0.006363\n-0.111951\n-0.087010\n\n\n\n\n\n\n\nThe MLE estimates are:\n\nbrand_N: 0.94\n\nbrand_P: 0.50\n\nad_yes: -0.73\n\nprice: -0.099\n\nStandard errors range from 0.006 to 0.12, and all 95% confidence intervals exclude zero. The model converged successfully and parameters were estimated with high precision."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#estimation-via-bayesian-methods",
    "href": "Multinomial_Logit_Model.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior Mean\nPosterior Std\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.954482\n0.111946\n0.737799\n1.172539\n\n\nbrand_P\n0.506766\n0.112751\n0.285853\n0.728708\n\n\nad_yes\n-0.732992\n0.088405\n-0.910828\n-0.566985\n\n\nprice\n-0.099719\n0.006344\n-0.112362\n-0.087510\n\n\n\n\n\n\n\nThe posterior means are:\n\nbrand_N: 0.95\n\nbrand_P: 0.50\n\nad_yes: -0.74\n\nprice: -0.100\n\nPosterior standard deviations closely match the MLE standard errors (within ±0.001). The 95% credible intervals are nearly identical to the MLE confidence intervals. These results reflect strong agreement between the two methods given the large sample size and weakly informative priors.\n\n\n\n\n\nPosterior Histogram for brand_N\n\n\n\n\nThe posterior histogram is approximately normal and centered near 0.95. The distribution is symmetric and unimodal, with most density between 0.75 and 1.15. This indicates a well-identified parameter with low uncertainty.\n\n\n\n\n\nTrace Plot for brand_N\n\n\n\n\nThe trace plot shows consistent sampling around a stable mean with no visible drift or trends. The values fluctuate tightly across the 10,000 post-burn-in iterations, suggesting good mixing and convergence of the Metropolis-Hastings sampler.\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Mean\nBayes Std. Dev.\nBayes CI Lower\nBayes CI Upper\nMLE Mean\nMLE Std. Error\nMLE CI Lower\nMLE CI Upper\n\n\n\n\nbrand_N\n0.954482\n0.111946\n0.737799\n1.172539\n0.941195\n0.109961\n0.725672\n1.156718\n\n\nbrand_P\n0.506766\n0.112751\n0.285853\n0.728708\n0.501616\n0.120088\n0.266242\n0.736989\n\n\nad_yes\n-0.732992\n0.088405\n-0.910828\n-0.566985\n-0.731994\n0.088279\n-0.905021\n-0.558968\n\n\nprice\n-0.099719\n0.006344\n-0.112362\n-0.087510\n-0.099480\n0.006363\n-0.111951\n-0.087010\n\n\n\n\n\n\n\nThe Bayesian posterior means closely align with the MLE point estimates across all four parameters. Differences are minimal:\n\nbrand_N: Posterior mean is 0.95; MLE is 0.94. Both intervals fully overlap.\nbrand_P: Posterior mean is 0.51 vs. MLE of 0.50.\nad_yes: Both methods estimate the effect around -0.73, with nearly identical uncertainty.\nprice: Posterior mean is -0.0997; MLE is -0.0995, with near-identical intervals.\n\nPosterior standard deviations match MLE standard errors within 0.002 across all parameters. Given the large sample and weak priors, the Bayesian and MLE results are functionally equivalent and reinforce the same conclusions: consumers prefer Netflix and Prime, dislike ads, and are price sensitive."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#discussion",
    "href": "Multinomial_Logit_Model.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf this were real data, the parameter estimates would reveal consumers’ underlying preferences.\n\nβ_Netflix &gt; β_Prime implies that, all else equal, consumers prefer Netflix to Prime Video. This reflects a higher perceived utility from Netflix’s offering.\nβ_price &lt; 0 is consistent with economic theory: as price increases, utility decreases, making the product less likely to be chosen.\n\nThe direction and magnitude of all estimates are reasonable. Consumers dislike ads, show brand preferences, and are price-sensitive—patterns expected in digital subscription markets.\nTo simulate or estimate a multi-level MNL model, we must account for individual differences in preferences. Unlike the standard MNL model, which assumes a single set of coefficients shared by all respondents, a hierarchical model allows each respondent to have their own set of parameters.\n\nKey changes: Instead of using one fixed β, assign each respondent a personal βₖ drawn from a population distribution: [ _i (, ) ]\nEstimation:\nEstimate both the respondent-level βᵢs and the population-level parameters (μ, Σ). This requires hierarchical modeling techniques, often implemented via Bayesian MCMC or mixed logit estimation.\n\nThis approach better reflects real-world data by capturing preference heterogeneity across individuals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tweety Chang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Finance & Business Analytics\n📧 ctweety15@gmail.com 📞 858-888-1353 📄 Resume"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nWith a background in business analytics and accounting, I bring hands-on experience analyzing data, identifying process gaps, and supporting data-driven improvements through clear communication and cross-functional collaboration. My work has focused on turning complex, multi-source data into practical insights that support reporting accuracy and informed decision-making.\nIn recent analytics projects, I have analyzed operational and business data to uncover inconsistencies, trends, and improvement opportunities affecting downstream reporting and analysis. I have supported projects end-to-end by clarifying requirements, validating outputs through testing, documenting findings, and presenting results in a clear, actionable way to both technical and non-technical stakeholders. Across roles, I have worked closely with business, analytics, and technical teams to ensure analysis aligned with real user needs and project goals.\nI am comfortable working in ambiguous environments, learning new domains quickly, and balancing multiple priorities. I work regularly with Excel, SQL, Python, Tableau, and Power BI, which allows me to move fluidly between data analysis and business context. My background in client-facing and team-based roles has also strengthened my ability to communicate clearly, collaborate effectively, and contribute thoughtfully to shared outcomes.\nI am currently pursuing an M.S. in Business Analytics at UC San Diego and am seeking internship or full-time opportunities where analytical rigor and practical problem-solving can support real-world business decisions."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego — Rady School of Management\nMaster of Science, Business Analytics\nJul 2024 – Mar 2026\nNorthern Illinois University\nBachelor of Science, Accountancy\nAug 2021 – Aug 2023\nAsia University\nBachelor of Science, Accounting and Information Systems\nSep 2019 – Aug 2023"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nHighspring\nContent Analyst · Jul 2025 – Dec 2025\nBuilt a tracking workflow and reporting to monitor labeling accuracy and productivity for ad-content review.\nHP\nData Analyst · Mar 2025 – Jun 2025\nAutomated data quality checks and anomaly scans for campaign datasets; surfaced mismatches and outliers for stakeholder review. 3 HBC Tax Inc.\nTax Associate · Jan 2023 – May 2023\nPrepared individual and small-business tax filings and supported document intake, reconciliation, and client follow-ups.\nNorthern Illinois University\nResearch Assistant · May 2022 – Aug 2022\nCleaned and organized research datasets and produced summary outputs for faculty analysis."
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "Projects",
    "text": "Projects\n\nFraud Detection\nUplift Modeling\nDatabase Management\nA/B Testing\nPatent Count Modeling (MLE)\nK-Means Clustering"
  },
  {
    "objectID": "about.html#focus-areas",
    "href": "about.html#focus-areas",
    "title": "About",
    "section": "Focus Areas",
    "text": "Focus Areas\n\nFinancial, business, and operational data analysis\n\nData validation, reconciliation, and reporting accuracy\n\nReporting automation and dashboard development\n\nExperimentation and A/B testing\n\nCommunicating insights to support operational and strategic decisions"
  },
  {
    "objectID": "database_management.html",
    "href": "database_management.html",
    "title": "Database Management",
    "section": "",
    "text": "This project focused on database management and data engineering using PostgreSQL, Snowflake, SQL, and Python. The work centered on building reliable ETL workflows and consolidating multi-source operational and financial data into an analysis-ready format for reporting and correlation analysis."
  },
  {
    "objectID": "database_management.html#introduction",
    "href": "database_management.html#introduction",
    "title": "Database Management",
    "section": "",
    "text": "This project focused on database management and data engineering using PostgreSQL, Snowflake, SQL, and Python. The work centered on building reliable ETL workflows and consolidating multi-source operational and financial data into an analysis-ready format for reporting and correlation analysis."
  },
  {
    "objectID": "database_management.html#objective",
    "href": "database_management.html#objective",
    "title": "Database Management",
    "section": "Objective",
    "text": "Objective\n\nBuild ETL workflows to consolidate 400,000+ records into a clean, analysis-ready dataset.\n\nIntegrate data across PostgreSQL, Snowflake, and Python pipelines to unify operational and financial information.\n\nSupport structured financial reporting by reviewing trends and relationships in the data.\n\nExplore potential correlations between external drivers (e.g., weather fluctuations) and purchase orders using multi-source data.\n\nLead a five-person team using clear task ownership and coordinated delivery."
  },
  {
    "objectID": "database_management.html#data-description",
    "href": "database_management.html#data-description",
    "title": "Database Management",
    "section": "Data Description",
    "text": "Data Description\nThis case integrates operational, financial, and environmental data from multiple sources to support structured analysis of purchase orders and invoices.\nThe datasets used include:\n\n1. Purchase Order Data (CSV – 41 Monthly Files)\n\nLine-level purchase order records\n\nIncludes quantities received, expected prices, supplier identifiers, and order dates\n\nUsed to calculate total purchase order amounts\n\n\n\n2. Supplier Invoice Data (XML)\n\nInvoice-level financial records\n\nIncludes invoice amounts, tax information, and transaction dates\n\nUsed to compare invoiced amounts against quoted purchase orders\n\n\n\n3. Supplier Data (PostgreSQL)\n\nSupplier-level reference data\n\nIncludes supplier location (ZIP code), identifiers, and related attributes\n\nUsed to connect operational data to geographic information\n\n\n\n4. Weather Data (Snowflake Marketplace – NOAA via Cybersyn)\n\nDaily temperature data\n\nUsed to explore potential relationships between weather fluctuations and purchasing behavior\n\nTogether, these datasets allow us to:\n\nReconcile purchase orders and invoices\n\nDetect discrepancies between quoted and invoiced amounts\n\nExamine whether external factors (e.g., temperature) are associated with purchasing patterns\n\n\n\nSnowflake Marketplace Setup\nThe NOAA Weather & Environment dataset was accessed through Snowflake Marketplace (Cybersyn provider).\nThe dataset was installed in Snowflake as the WEATHER__ENVIRONMENT database.\nFigure 1. Snowflake Marketplace – Weather & Environment product\n\n**Figure 2. Installed WEATHER__ENVIRONMENT database in Snowflake**"
  },
  {
    "objectID": "database_management.html#case-overview",
    "href": "database_management.html#case-overview",
    "title": "Database Management",
    "section": "Case Overview",
    "text": "Case Overview\nThe objective of this case was to design and implement an end-to-end ETL workflow using Snowflake and Python. Rather than transforming data in local tools, most processing was executed directly within Snowflake to ensure scalability and performance.\nThe workflow consisted of:\n\nExtracting data from CSV, XML, PostgreSQL, and Marketplace sources\n\nLoading the data into Snowflake\n\nTransforming the data into structured, analysis-ready tables\n\nJoining operational, financial, and environmental datasets\n\nCreating summarized outputs for reconciliation and correlation analysis\n\nThe final dataset enables comparison of:\n\nPurchase Order Amount\n\nInvoice Amount\n\nWeather Conditions (by supplier ZIP code and date)\n\nThis structure supports automated discrepancy detection and cross-source analytical exploration."
  },
  {
    "objectID": "database_management.html#purchase-order-aggregation",
    "href": "database_management.html#purchase-order-aggregation",
    "title": "Database Management",
    "section": "Purchase Order Aggregation",
    "text": "Purchase Order Aggregation\nPurchase order data was provided as 41 separate monthly CSV files at the line-item level. These files were consolidated into a single dataset and loaded into Snowflake.\nBecause each purchase order contains multiple line items, totals were calculated for each PurchaseOrderID by summing:\n[ ]\nThis produced one summarized record per purchase order, reducing approximately 8,000+ line items to about 2,100 purchase orders. The result is a clean table of total quoted amounts by supplier and order date."
  },
  {
    "objectID": "database_management.html#invoice-amount-for-each-purchase-order-id",
    "href": "database_management.html#invoice-amount-for-each-purchase-order-id",
    "title": "Database Management",
    "section": "Invoice Amount for Each Purchase Order ID",
    "text": "Invoice Amount for Each Purchase Order ID\nSupplier invoices were provided in XML format and converted into a structured dataset before being loaded into Snowflake.\nFrom the supplier transactions data, invoice totals were extracted using the Amount Excluding Tax field. The data was organized by:\n\nPurchaseOrderID\n\nSupplierID\n\nInvoiceAmount\n\nTransactionDate\n\nThis created a comparable summary of invoiced amounts for each purchase order."
  },
  {
    "objectID": "database_management.html#joining-purchase-orders-and-invoices",
    "href": "database_management.html#joining-purchase-orders-and-invoices",
    "title": "Database Management",
    "section": "Joining Purchase Orders and Invoices",
    "text": "Joining Purchase Orders and Invoices\nThe summarized purchase order table and invoice table were joined using PurchaseOrderID and SupplierID.\nA new field, invoiced_vs_quoted, was calculated as:\n[ - ]\nThis value captures the difference between the quoted purchase amount and the invoiced amount.\nThe final reconciliation table contains approximately 2,100 matched records and provides a clear view of pricing differences across suppliers and transactions."
  },
  {
    "objectID": "database_management.html#supplier-postal-codes",
    "href": "database_management.html#supplier-postal-codes",
    "title": "Database Management",
    "section": "Supplier Postal Codes",
    "text": "Supplier Postal Codes\nSupplier reference data was extracted from PostgreSQL and loaded into Snowflake. From this dataset, each supplier’s postal code was identified and standardized.\nA simple reference table was created containing:\n\nSupplierID\n\nPostalPostalCode\n\nThis table provides the geographic link needed to connect supplier activity with external data sources."
  },
  {
    "objectID": "database_management.html#weather-data",
    "href": "database_management.html#weather-data",
    "title": "Database Management",
    "section": "Weather Data",
    "text": "Weather Data\nTo examine whether weather conditions were associated with purchasing activity, daily temperature data was added to the analysis.\nA national ZIP code reference file was used to obtain latitude and longitude for each postal code. Each supplier ZIP code was then matched to the nearest NOAA weather station available in Snowflake Marketplace.\nFrom the NOAA dataset, daily maximum temperature values were extracted. The resulting table, supplier_zip_code_weather, contains:\n\nZIP code\n\nDate\n\nDaily high temperature\n\nApproximately 1,300 ZIP-date temperature records were generated.\nThis structure allows supplier transactions to be analyzed alongside daily weather conditions using shared ZIP codes and dates."
  },
  {
    "objectID": "database_management.html#joining-everything-together",
    "href": "database_management.html#joining-everything-together",
    "title": "Database Management",
    "section": "Joining Everything Together",
    "text": "Joining Everything Together\nThe final step combined all prepared datasets:\n\nPurchase order and invoice comparison table\n\nSupplier reference data\n\nDaily weather data by ZIP code\n\nThe tables were joined using:\n\nSupplier ZIP code\n\nTransaction date\n\nOnly records with available temperature data were included.\n\nResult\nThe final dataset contains:\n\nZIP code\n\nDate\n\nDaily high temperature\n\nPurchase order and invoice comparison information\n\nApproximately 1,300 matched records were produced.\n\n\nObservation\nAfter joining the datasets, no measurable relationship was observed between daily temperature and the difference between quoted and invoiced amounts. The invoiced_vs_quoted values did not vary in a way that suggested weather influence.\nBased on this analysis, weather conditions do not appear to explain invoice discrepancies in this dataset."
  },
  {
    "objectID": "AB_Testing.html",
    "href": "AB_Testing.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "AB_Testing.html#introduction",
    "href": "AB_Testing.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse."
  },
  {
    "objectID": "AB_Testing.html#objective",
    "href": "AB_Testing.html#objective",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Objective",
    "text": "Objective\nThis project replicates and interprets the findings from Karlan and List’s (2007) field experiment on charitable giving. Using the original dataset, I examine how the presence and structure of a matching donation offer influence both the likelihood of giving and the amount contributed.\nThis project involves:\n\nAnalyzing treatment effects using t-tests, regression models, and probit analysis\nComparing donation behavior across different match ratios and suggested ask levels\nConducting simulations to illustrate the Law of Large Numbers and the Central Limit Theorem\nPresenting results in a clear, reproducible format using Quarto and Python\n\nThe overall goal is to better understand the behavioral response to charitable incentives and how small changes in message framing can impact donor behavior."
  },
  {
    "objectID": "AB_Testing.html#description-of-the-experiment",
    "href": "AB_Testing.html#description-of-the-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description of the Experiment",
    "text": "Description of the Experiment\nIn their 2007 study published in the American Economic Review, Dean Karlan and John List conducted a large-scale natural field experiment to test the effectiveness of matching donations in a real-world fundraising campaign. Over 50,000 previous donors to a U.S. nonprofit were mailed fundraising letters and randomly assigned to one of two groups:\n\nControl group: received a standard fundraising letter with no special offer\nTreatment group: received a similar letter but was offered a matching donation, meaning their contribution would be matched by another donor\n\nWithin the treatment group, individuals were further randomized into subgroups based on:\n\nMatch ratio: \\(1\\!:\\!1\\), \\(2\\!:\\!1\\), or \\(3\\!:\\!1\\)\nMaximum match amount: \\(25{,}000\\), \\(50{,}000\\), \\(100{,}000\\), or unstated\nSuggested donation amount: equal to 1.25× or 1.5× the donor’s previous contribution\n\nThe randomized design allows for causal analysis of how these variations influence both the decision to donate and the amount given. This experiment provides a powerful example of how field experiments can be used to study economic behavior in natural settings."
  },
  {
    "objectID": "AB_Testing.html#data-description",
    "href": "AB_Testing.html#data-description",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data Description",
    "text": "Data Description\nWe begin by loading the dataset provided by Karlan and List (2007), which contains detailed records from their fundraising field experiment.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nSummary Statistics by Treatment Status\nFigure 1. Donation Rate by Group \nThe treatment group has a higher donation rate (2.20%) than the control group (1.79%). Sample sizes are shown above each bar.\nFigure 2. Average Donation Amount by Group \nAverage donations, including non-donors, are higher in the treatment group ($0.97) than in the control group ($0.81), suggesting that the matching offer increased total giving on average. Sample sizes are shown above each bar."
  },
  {
    "objectID": "AB_Testing.html#balance-test",
    "href": "AB_Testing.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\nTo assess whether the randomization produced comparable treatment and control groups, we tested balance on several pre-treatment characteristics: months since last donation (mrm2), years since first donation, number of prior donations (freq), and a gender indicator (female).\n\nT-Test and Linear Regression for Each Variable\n\n\n\nVariable\nTreatment Coefficient\np-value\n\n\n\n\nmrm2\n0.0137\n0.905\n\n\nyears\n−0.0575\n0.270\n\n\nfreq\n−0.0120\n0.912\n\n\nfemale\n−0.0075\n0.079\n\n\n\nNone of the estimated treatment effects are statistically significant at the 5% level, indicating balance across observable characteristics. These results are consistent with the balance checks reported in Table 1 of Karlan and List (2007).\n\n\nBalance Check for mrm2\n\n\n\nStatistic\nValue\n\n\n\n\nTreatment coefficient\n0.0137\n\n\np-value (t-test)\n0.9049\n\n\np-value (regression)\n0.9050\n\n\n95% confidence interval\n[−0.211, 0.238]\n\n\n\nThe estimated difference in months since last donation between treatment and control groups is small and statistically insignificant, indicating comparable donation recency prior to treatment.\n\n\nAdditional Balance Tests\n\n\nBalance Test — Key Estimates\n\n\n\nVariable\nTreatment coef\nt-stat\np-value\n95% CI\n\n\n\n\nmrm2\n0.0137\n0.119\n0.905\n[-0.211, 0.238]\n\n\nyears\n−0.0575\n−1.103\n0.270\n[-0.160, 0.045]\n\n\nfreq\n−0.0120\n−0.111\n0.912\n[-0.224, 0.200]\n\n\nfemale\n−0.0075\n−1.758\n0.079\n[-0.016, 0.001]\n\n\n\nNone of the estimated treatment effects are statistically significant at the 5% level; the gender indicator is borderline (p = 0.079) but the estimated difference is small."
  },
  {
    "objectID": "AB_Testing.html#experimental-results",
    "href": "AB_Testing.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nFigure 3. Donation Rate by Treatment Status \nThe treatment group exhibits a higher donation rate than the control group (2.20% vs. 1.79%). The difference is statistically significant at the 1% level.\n\nTreatment Effect on Donation Rates\nWe estimate the effect of the matching donation treatment on the probability of giving using both a two-sample t-test and a bivariate linear regression. Both approaches indicate a statistically significant difference in donation rates between the treatment and control groups.\nThe regression results imply that assignment to the treatment increases the probability of donating by approximately 0.42 percentage points (p = 0.002). Relative to the control group’s donation rate of about 1.8%, this corresponds to a roughly 22% increase in the likelihood of giving.\nAlthough the absolute effect is small, the estimate is precise and consistent across methods, indicating that offering a matching donation increases participation in charitable giving.\n\n\n\n\n\n\n\n\n\nOutcome: Donated (gave)\nEstimate\np-value\n95% Confidence Interval\n\n\n\n\nTreatment effect\n0.0042\n0.002\n[0.002, 0.007]\n\n\n\n\n\nProbit Regression\nAs a robustness check for the binary outcome, we estimate a Probit model and report the average marginal effect. The treatment increases the probability of donating by 0.43 percentage points (p = 0.002), which closely matches the linear probability model estimate.\n\n\n\nModel\nEffect on Pr(Donate)\np-value\n95% CI\n\n\n\n\nProbit (AME)\n0.0043\n0.002\n[0.002, 0.007]"
  },
  {
    "objectID": "AB_Testing.html#differences-between-match-rates",
    "href": "AB_Testing.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\nWe examine whether larger match ratios within the treatment group lead to higher donation rates by comparing 1:1, 2:1, and 3:1 matching offers. Pairwise t-tests indicate no statistically significant differences in donation rates across match ratios.\nRelative to a 1:1 match, increasing the ratio to 2:1 raises the donation rate by approximately 0.19 percentage points (p = 0.335), while moving from 2:1 to 3:1 increases the rate by only 0.01 percentage points (p = 0.310). Neither difference is statistically significant.\nThese results suggest that higher match ratios do not meaningfully increase participation beyond the presence of a match itself, consistent with the findings in Karlan and List (2007).\n\n\n\nComparison\nChange in Donation Rate (pp)\np-value\n\n\n\n\n2:1 vs 1:1\n+0.19\n0.335\n\n\n3:1 vs 2:1\n+0.01\n0.310\n\n\n\n\n\nEffect of Match Ratio on Donation Rates\n\n\n\nMatch Ratio\nChange in Donation Rate (pp)\np-value\n\n\n\n\n2:1 vs 1:1\n+0.19\n0.338\n\n\n3:1 vs 1:1\n+0.20\n0.313\n\n\n\n\n\n\nDonation Rates by Match Ratio\nAmong treated individuals, donation rates are similar across match ratios. The response rate is 2.07% under a 1:1 match, 2.26% under a 2:1 match, and 2.27% under a 3:1 match.\nThe increase in donation rates beyond a 1:1 match is small—0.19 percentage points for a 2:1 match and 0.01 percentage points for a 3:1 match—and not statistically significant, consistent with the formal tests reported earlier. This pattern aligns with the findings of Karlan and List (2007), which show that increasing the match ratio does not materially increase participation once a match is offered.\n\n\n\nMatch Ratio\nDonation Rate\n\n\n\n\n1:1\n0.0207 (2.07%)\n\n\n2:1\n0.0226 (2.26%)\n\n\n3:1\n0.0227 (2.27%)\n\n\n\n\n\n\nDonation Amount\nWe next examine whether the matching donation treatment affects the amount donated among individuals who chose to give, focusing on the intensive margin of charitable giving.\nA two-sample t-test and a linear regression both estimate a small difference in donation amounts between treatment and control donors; however, the effects are not statistically significant at conventional levels (p ≈ 0.06). The estimated magnitudes are modest.\nOverall, these results suggest that while offering a matching donation increases participation, it does not meaningfully affect the amount donated conditional on giving.\n\n\nDistribution of Donation Amounts Among Donors\nFigure 4. Distribution of Donation Amounts by Treatment Status  Donation amounts in both the control and treatment groups are right-skewed, with most contributions concentrated at lower values and a small number of large donations. Mean donation amounts are similar across groups ($45.54 for the control group and $43.87 for the treatment group).\n\nFigure 5. Sampling Distribution of Treatment–Control Differences (Simulated)  The figure shows the distribution of treatment–control differences in donation rates across 10,000 simulated experiments with 500 observations per group. The distribution is approximately normal and centered near 0.004, the difference implied by the assumed donation probabilities. The dashed line indicates the mean simulated difference, and the dotted line marks the null of no treatment effect.\n\n\n\nLaw of Large Numbers\nFigure 6. Cumulative estimate of the treatment–control difference in mean donation amounts\n\nFigure 6 plots the cumulative difference in mean donation amounts between the treatment and control groups as observations are added. When the sample size is small, the estimated difference fluctuates substantially. As the number of observations increases, the cumulative estimate stabilizes and converges toward the full-sample difference, illustrated by the horizontal dashed line.\n\n\n\nSampling Variability and Sample Size\nFigure 7. Sampling distributions of treatment–control differences by sample size\n\nFigure 7 shows simulated sampling distributions of treatment–control differences in donation rates for sample sizes of 50, 200, 500, and 1,000. With small samples, the distribution is wide. As sample size increases, the distributions concentrate around their mean, indicating greater precision."
  },
  {
    "objectID": "credit_card_fraud_detection.html",
    "href": "credit_card_fraud_detection.html",
    "title": "Credit Card Fraud Detection",
    "section": "",
    "text": "Credit card fraud poses a persistent risk to financial institutions, resulting in substantial financial losses and increased operational burden. As digital transactions continue to grow, detecting fraudulent behavior accurately and efficiently has become increasingly important.\nThis project develops a structured analytical framework for identifying fraudulent credit card transactions using historical transaction data. The dataset contains 97,852 transactions, of which 2,047 are labeled as fraudulent, reflecting a highly imbalanced classification problem.\nThe report outlines the full modeling workflow, from data preparation to model evaluation, with emphasis on building a practical and deployable fraud detection system."
  },
  {
    "objectID": "credit_card_fraud_detection.html#introduction",
    "href": "credit_card_fraud_detection.html#introduction",
    "title": "Credit Card Fraud Detection",
    "section": "",
    "text": "Credit card fraud poses a persistent risk to financial institutions, resulting in substantial financial losses and increased operational burden. As digital transactions continue to grow, detecting fraudulent behavior accurately and efficiently has become increasingly important.\nThis project develops a structured analytical framework for identifying fraudulent credit card transactions using historical transaction data. The dataset contains 97,852 transactions, of which 2,047 are labeled as fraudulent, reflecting a highly imbalanced classification problem.\nThe report outlines the full modeling workflow, from data preparation to model evaluation, with emphasis on building a practical and deployable fraud detection system."
  },
  {
    "objectID": "credit_card_fraud_detection.html#objective",
    "href": "credit_card_fraud_detection.html#objective",
    "title": "Credit Card Fraud Detection",
    "section": "Objective",
    "text": "Objective\nThe primary objective of this project is to build a machine learning model capable of identifying fraudulent transactions while controlling false positives.\nSpecifically, the goals are:\n\nDevelop a robust fraud detection pipeline including cleaning, feature engineering, and model selection\n\nEvaluate model performance using out-of-time validation\n\nDetermine an operational cutoff that balances fraud capture and review cost\n\nQuantify the financial impact of the selected decision threshold\n\nThe final model is assessed not only on predictive performance but also on its practical business value."
  },
  {
    "objectID": "credit_card_fraud_detection.html#data-description",
    "href": "credit_card_fraud_detection.html#data-description",
    "title": "Credit Card Fraud Detection",
    "section": "Data Description",
    "text": "Data Description\nThe dataset contains 97,852 credit card transactions recorded in 2010. Each transaction includes numeric, categorical, and temporal variables used to model fraudulent behavior.\n\nNumeric Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nField Name\n# Records\n% Populated\n# Zeros\nMin\nMax\nMean\nStd Dev\nMost Common\n\n\n\n\nAmount\n97,852\n100%\n0\n0.01\n3,102,045.53\n425.47\n9,949.80\n3.62\n\n\nFraud\n97,852\n100%\n95,805\n0\n1\n0.0209\n0.14\n0\n\n\n\n\nAmount shows a highly right-skewed distribution, with extreme high-value transactions.\nFraud is the target variable. Fraudulent transactions account for approximately 2.09% of the dataset, indicating strong class imbalance.\n\n\n\nCategorical Variables\n\n\n\nField Name\n# Records\n% Populated\n# Unique\nMost Common\n\n\n\n\nRecnum\n97,852\n100%\n97,852\n1\n\n\nCardnum\n97,852\n100%\n1,645\n5142148452\n\n\nMerchnum\n94,455\n96.5%\n13,091\n930090121224\n\n\nMerch description\n97,852\n100%\n13,126\nGSA-FSS-ADV\n\n\nMerch state\n96,649\n98.8%\n227\nTN\n\n\nMerch zip\n93,149\n95.2%\n4,567\n38118\n\n\nTranstype\n97,852\n100%\n4\nP\n\n\n\n\nTransaction identifiers such as Recnum are unique per record.\nMerchant-related fields (Merchnum, Merch description, Merch state, Merch zip) provide geographic and behavioral context.\nMissing values are primarily present in merchant-related fields.\nTranstype contains four transaction categories.\n\n\n\nTemporal Variable\n\n\n\n\n\n\n\n\n\n\n\nField Name\n# Records\n% Populated\nEarliest Date\nLatest Date\nMost Common\n\n\n\n\nDate\n97,852\n100%\n2010-01-01\n2010-12-31\n2010-02-28\n\n\n\n\nThe data spans the full calendar year of 2010.\nThe availability of transaction dates enables temporal feature engineering such as velocity and recency variables.\n\n\n\nAmount\nTransaction amounts are strongly right-skewed. Most transactions are small, while a small number reach very high values.\n\nOnly 3 transactions exceed $30,275.\nThe majority fall below $3,000.\nA long right tail is clearly visible.\n\nBecause extreme values can dominate tree splits and distort scale-sensitive features, this variable will be transformed or capped during preprocessing.\n\nAmount Distribution (Full Scale)\nFigure 1. Transaction amount distribution (full scale)\n\nTo better visualize the heavy tail, the density axis is log-transformed.\n\n\nAmount Distribution (Log Scale)\nFigure 2. Transaction amount distribution (log scale)\n\nThe log-scale view highlights the spread of high-value transactions without being dominated by the tail.\n\n\n\nMerch State\nFigure 3. Top 20 merchant states by transaction count\n\nThe top 20 merchant states account for a large share of transactions.\nTN has the highest volume (12,169 transactions), followed by several other high-activity states. Transaction activity is clearly concentrated geographically rather than evenly distributed across states.\n\n\nMerch Zip\nFigure 4. Top 20 merchant ZIP codes by transaction count\n\n\nZIP code 38118 alone accounts for 11,998 transactions.\nA small number of ZIP codes represent a disproportionate share of activity.\n\nThis level of concentration indicates strong location clustering in the data.\n\n\nFraud (Target Variable)\nFigure 5. Fraud vs non-fraud distribution\n\n\n95,805 non-fraud transactions\n\n2,047 fraud transactions\n\nFraud rate ≈ 2.09%\n\nGiven this imbalance, overall accuracy is not an informative metric. Model evaluation will focus on fraud detection rate and decision thresholds aligned with business impact."
  },
  {
    "objectID": "credit_card_fraud_detection.html#data-cleaning",
    "href": "credit_card_fraud_detection.html#data-cleaning",
    "title": "Credit Card Fraud Detection",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nOutlier Removal\nThe transaction amount variable contains one extreme observation:\n\nRecnum #53179\n\nAmount: $3,102,045\n\nThreshold: $30,275 (3 standard deviations above the mean)\n\nAfter review with the business manager, this transaction was considered anomalous and removed from modeling.\n\nRecords reduced from 97,852 to 97,851.\n\n\n\nData Exclusions – Transaction Type\nThe variable Transtype contains four categories:\n\nP: 97,497\n\nA: 181\n\nD: 173\n\nY: 1\n\nBecause the business team could not clearly define categories A, D, and Y, these transactions were excluded to avoid introducing ambiguity into the model.\n\nRecords reduced from 97,851 to 97,496.\n\n\n\nHandling Missing and Invalid Values\nSeveral merchant-related fields contained missing or invalid values.\n\nMerchnum\n\n3,279 missing values (including 59 records coded as 0, treated as null).\n\nImputation strategy:\n\nRecovered 1,164 values using matching Merch description.\nAssigned “unknown” to records with descriptions such as\nRETAIL CREDIT ADJUSTMENT and RETAIL DEBIT ADJUSTMENT (694 records).\nFor the remaining records, new merchant IDs were created based on unique merchant descriptions.\n\nAfter imputation, missing values in Merchnum were reduced to zero.\n\n\nMerch State\n\n1,028 missing values.\n\nImputation strategy:\n\nRecovered state values using corresponding ZIP codes.\nUsed merchant number and description matching where possible.\nAssigned “unknown” to adjustment-type transactions.\nRemaining missing values were set to “unknown”.\nNon-U.S. states were recoded as “foreign”.\n\nThis approach preserves geographic signal while avoiding artificial distortion.\n\n\nMerch Zip\n\n4,347 missing values.\n\nImputation strategy:\n\nRecovered ZIP codes using merchant number and description mapping.\nFor records with known state but missing ZIP, assigned the most common ZIP within that state.\nRemaining missing values were set to “unknown”.\n\n\n\n\nData Status After Cleaning\nAll key merchant fields (Merchnum, Merch state, Merch zip) were fully resolved, and the dataset was prepared for feature engineering.\nFinal dataset size: 97,496 transactions"
  },
  {
    "objectID": "credit_card_fraud_detection.html#variable-creation",
    "href": "credit_card_fraud_detection.html#variable-creation",
    "title": "Credit Card Fraud Detection",
    "section": "Variable Creation",
    "text": "Variable Creation\nThis project focuses on Card-Not-Present (CNP) fraud, where stolen card information is used across different merchants or locations without physical verification. Suspicious patterns often include:\n\nThe same card appearing in multiple states or ZIP codes within short periods\n\nSudden increases in transaction frequency\n\nRapid changes in spending behavior\n\nTransactions at unfamiliar merchants\n\nTo capture these patterns, features were engineered around transaction timing, spending intensity, and entity relationships. Variables were created under four principles:\n\nAmount Features – Rolling statistics of transaction amounts (mean, max, median, sum, and ratios) across multiple time windows.\nFrequency Features – Transaction counts within rolling windows (0, 1, 3, 7, 14, 30, 60 days).\nVelocity Features – Short-term activity compared to longer-term historical behavior.\nRecency Features – Days since previous transaction.\n\nFeatures were computed across key entities such as Cardnum, Merchnum, Merch state, Merch zip, and their linkages.\n\n\n\n\n\n\nSummary of Variables Created\n\n\n\n\n\n\n\n\nCategory\n# Fields Created\nCumulative\n\n\n\n\nOriginal fields\n10\n10\n\n\nDay-of-week + encoding\n2\n12\n\n\nEntity linkings\n21\n33\n\n\nTarget encoding\n3\n36\n\n\nDays-since features\n23\n59\n\n\nFrequency features\n161\n220\n\n\nAmount features\n1,288\n1,508\n\n\nVelocity features\n552\n2,060\n\n\nAmount difference features\n414\n2,474\n\n\nCross-entity frequency\n696\n3,170\n\n\nNormalized velocity\n184\n3,354\n\n\nAmount binning\n1\n3,355\n\n\nForeign indicator\n1\n3,356\n\n\nNew entity flags\n22\n3,378\n\n\nHoliday indicator\n\n\n\n\n\n\n\n\nTotal engineered variables: 3,379"
  },
  {
    "objectID": "credit_card_fraud_detection.html#feature-selection",
    "href": "credit_card_fraud_detection.html#feature-selection",
    "title": "Credit Card Fraud Detection",
    "section": "Feature Selection",
    "text": "Feature Selection\nFeature selection was conducted in two stages: filtering and wrapping.\n\n1. Filtering\nEach variable was ranked independently using the Kolmogorov–Smirnov (KS) statistic.\nThe top candidate features were selected based on:\n\n130 variables (≈10% of total features)\n260 variables (≈20% of total features)\n\nThese filtered sets were passed to the wrapper stage.\n\n\n2. Wrapping\nForward selection was applied using two models:\n\nLightGBM (LGBM) – deterministic\nRandom Forest (RF) – stochastic\n\nFor RF, training was repeated five times and the most frequently selected feature combination was retained.\nWrapper sizes tested: - 10 features - 20 features\nThe wrapper objective was Fraud Detection Rate at 3% (FDR@3%).\n\n\nResults Summary\n\n\n\n\n\n\n\n\n\n\n\nDirection\n# Filters\n# Wrappers\nModel\nStochastic\nAvg Performance\n\n\n\n\nForward\n130\n10\nLGBM\nNo\n0.71\n\n\nForward\n130\n10\nRF\nYes\n0.66\n\n\nForward\n130\n20\nLGBM\nNo\n0.71\n\n\nForward\n130\n20\nRF\nYes\n0.66\n\n\nForward\n260\n10\nLGBM\nNo\n0.72\n\n\nForward\n260\n10\nRF\nYes\n0.66\n\n\nForward\n260\n20\nLGBM\nNo\n0.72\n\n\nForward\n260\n20\nRF\nYes\n0.68\n\n\n\n\n\nObservations\n\nLightGBM consistently outperformed Random Forest.\nIncreasing the filter pool from 130 to 260 improved performance.\nExpanding wrappers from 10 to 20 features did not materially improve results.\n\nBased on these findings, LightGBM with 260 filtered candidates was selected for subsequent modeling.\n\n\nWrapper Results (Forward Selection)\nForward selection was applied using different filter sizes and wrapper counts.\nPerformance is measured using FDR@3%.\n\n\n\nFilters\nWrappers\nModel\nStochastic\nFDR@3%\n\n\n\n\n260\n10\nLGBM\nNo\n0.72\n\n\n260\n10\nRF\nYes\n0.66\n\n\n260\n20\nLGBM\nNo\n0.72\n\n\n260\n20\nRF\nYes\n0.68\n\n\n520\n10\nLGBM\nNo\n0.72\n\n\n520\n10\nRF\nYes\n0.71\n\n\n520\n20\nLGBM\nNo\n0.73\n\n\n520\n20\nRF\nYes\n0.73\n\n\n\n\nStepwise Selection Performance\nFigure 6. Forward stepwise selection performance curve\n\nThe performance curve increases rapidly in the first few steps and stabilizes after approximately 12 features, indicating diminishing returns from adding additional variables.\n\n\n\nFeature Selection Conclusion\nIncreasing the number of filtered candidates improved wrapper performance.\nWith 520 filters and 20 wrappers, both LightGBM and Random Forest achieved an average performance of 0.73.\nLightGBM was selected as the final wrapper model because it is deterministic and produces stable feature rankings across runs, whereas Random Forest introduces variability due to its stochastic nature.\nThe forward selection curve shows performance stabilizing after approximately 12 features. However, to ensure sufficient coverage of relevant signals, we retained 20 features (approximately twice the saturation size) for the final model.\n\n\n\n\n\n\nSelected Variables (Top 20)\n\n\n\n\n\n\n\n\nField Name\nCumulative Score\n\n\n\n\nCardnum_unique_count_for_card_state_1\n0.492025\n\n\nCard_Merchdesc_State_total_7\n0.669325\n\n\nCardnum_count_1_by_30\n0.688957\n\n\nCardnum_max_14\n0.698160\n\n\nCard_dow_vdratio_0by60\n0.704908\n\n\nCard_dow_vdratio_0by14\n0.708589\n\n\nMerchnum_desc_State_total_3\n0.714724\n\n\nCard_Merchdesc_total_7\n0.716564\n\n\nCard_dow_unique_count_for_merch_zip_7\n0.720859\n\n\nCardnum_actual/toal_0\n0.726380\n\n\nCard_dow_vdratio_0by7\n0.728834\n\n\nCardnum_vdratio_1by7\n0.730061\n\n\nCardnum_unique_count_for_card_state_3\n0.730061\n\n\nCardnum_unique_count_for_card_zip_3\n0.730061\n\n\nMerchnum_desc_Zip_total_3\n0.730061\n\n\nCardnum_unique_count_for_Merchnum_3\n0.730061\n\n\nCardnum_actual/toal_1\n0.730061\n\n\nCardnum_unique_count_for_card_state_7\n0.730061\n\n\nCardnum_actual/max_0\n0.730061\n\n\nCard_dow_unique_count_for_merch_state_1\n0.730061"
  },
  {
    "objectID": "credit_card_fraud_detection.html#preliminary-model-exploration",
    "href": "credit_card_fraud_detection.html#preliminary-model-exploration",
    "title": "Credit Card Fraud Detection",
    "section": "Preliminary Model Exploration",
    "text": "Preliminary Model Exploration\nWe compared a baseline linear model against several non-linear models. All models were evaluated using FDR@3% on training, testing, and out-of-time (OOT) data.\n\nBaseline: Logistic Regression\n\nNon-linear models: Decision Tree, Random Forest, LightGBM, Neural Network\n\n\nModels Compared\n\nLogistic Regression: baseline linear classifier for interpretability and a stable benchmark.\nDecision Tree: single-tree model that captures non-linear splits but can overfit.\nRandom Forest: bagged trees that reduce variance and improve stability over a single tree.\nLightGBM: gradient-boosted trees that typically perform well on structured/tabular data.\nNeural Network: flexible non-linear function approximator, included as a higher-capacity alternative.\n\n\n\nHyperparameters\n\nLogistic Regression: penalty='l1', C=0.01, solver='liblinear'\n\nDecision Tree: criterion='gini', max_depth=8, min_samples_split=120, min_samples_leaf=60\n\nRandom Forest: criterion='gini', n_estimators=30, max_depth=8, min_samples_split=120, min_samples_leaf=60\n\nLightGBM: n_estimators=30, num_leaves=4, learning_rate=0.1\n\nNeural Net: activation='relu', hidden_layer_sizes=(20, 20), solver='adam', alpha=0.005, learning_rate_init=0.01\n\n\n\nModel Performance Comparison\nFigure 7. Model performance comparison (FDR@3%)\n\nLightGBM shows the strongest overall performance and the best OOT results among the models tested, which is the main factor for model selection in this project."
  },
  {
    "objectID": "credit_card_fraud_detection.html#final-model-performance",
    "href": "credit_card_fraud_detection.html#final-model-performance",
    "title": "Credit Card Fraud Detection",
    "section": "Final Model Performance",
    "text": "Final Model Performance\n\nModel Selection\nLightGBM was selected as the final model based on its balance between in-sample performance and out-of-time (OOT) stability.\n\n\n\nModel\nTrain\nTest\nOOT\n\n\n\n\nLGBM\n0.7438\n0.7490\n0.5296\n\n\n\nTraining and testing scores are closely aligned, indicating controlled model complexity. Although performance declines on OOT data, LightGBM maintains stronger generalization compared to the other models evaluated.\n\n\nHyperparameters\n\nn_estimators = 30\n\nnum_leaves = 4\n\nlearning_rate = 0.1\n\nmax_depth = default\n\nThe configuration favors a shallow boosting structure with limited leaf growth. This reduces variance and improves stability under time drift, which is critical for fraud detection.\nThe moderate learning rate ensures gradual updates across boosting rounds, avoiding aggressive fitting to short-term noise.\n\n\nPerformance Summary\n\nTrain and test scores are stable.\nOOT performance remains consistent with wrapper-stage expectations.\nNo large divergence between datasets, suggesting controlled overfitting.\n\nThe final model is therefore suitable for operational deployment under a defined fraud detection threshold."
  },
  {
    "objectID": "credit_card_fraud_detection.html#financial-impact-and-recommended-cutoff",
    "href": "credit_card_fraud_detection.html#financial-impact-and-recommended-cutoff",
    "title": "Credit Card Fraud Detection",
    "section": "Financial Impact and Recommended Cutoff",
    "text": "Financial Impact and Recommended Cutoff\n\nFinancial Assumptions\nTo translate model performance into business impact, the following assumptions were applied:\n\n$400 gain per fraud transaction correctly detected\n\n$20 loss per false positive\n\nThe sample (~100,000 records) represents a portfolio of 10 million transactions per year\n\nThe dataset covers 2 months of activity; annualized by multiplying by 6\n\n\n\nFinancial Curves\nFigure 8. Financial impact curve across cutoff percentages\n\n\nGreen: Fraud dollars captured\n\nRed: Revenue loss from false positives\n\nBlue: Net overall savings\n\nFraud capture increases steadily as the cutoff expands. However, false positive cost also rises, reducing net gains beyond a certain point.\n\n\nCutoff Comparison\n\n\n\nCutoff %\nOverall Savings\nFDR\nFPR\n\n\n\n\n1%\n$23,484,000\n33.3%\n0.232%\n\n\n2%\n$28,308,000\n41.8%\n0.976%\n\n\n3%\n$35,664,000\n53.5%\n1.308%\n\n\n4%\n$40,500,000\n62.0%\n1.658%\n\n\n5%\n$43,056,000\n67.3%\n2.060%\n\n\n6%\n$45,120,000\n72.1%\n2.430%\n\n\n\n\n\nRecommended Cutoff: 4%\nAt a 4% cutoff:\n\n62% of fraudulent transactions are detected\n\nFalse positive rate remains controlled at 1.658%\n\nEstimated annual savings: $40.5 million\n\nAlthough higher cutoffs increase fraud detection, they also introduce rising false positive costs and operational friction. The 4% threshold balances fraud capture and customer impact while delivering strong financial return."
  },
  {
    "objectID": "credit_card_fraud_detection.html#summary",
    "href": "credit_card_fraud_detection.html#summary",
    "title": "Credit Card Fraud Detection",
    "section": "Summary",
    "text": "Summary\nThis project developed a data-driven credit card fraud detection model using 97,852 transactions, of which approximately 2% were fraudulent.\nA structured pipeline was implemented, including data cleaning, feature engineering, feature selection, and model comparison. From 3,379 engineered variables, the top 20 were selected using a filter-and-wrapper approach.\nAmong the models evaluated, LightGBM demonstrated the strongest balance between in-sample performance and out-of-time stability. The final model achieved:\n\nTrain FDR: 0.744\n\nTest FDR: 0.749\n\nOOT FDR: 0.530\n\nA 4% decision cutoff was selected based on financial impact analysis. At this threshold:\n\n62% of fraud is detected\n\nFalse positive rate remains controlled at 1.658%\n\nEstimated annual savings: $40.5 million\n\nThe results indicate that behavioral feature engineering combined with boosted tree models provides strong performance under time drift. Future improvements may focus on additional data sources and enhanced behavioral signals to further improve OOT stability."
  }
]