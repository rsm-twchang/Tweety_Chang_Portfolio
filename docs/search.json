[
  {
    "objectID": "Maximum_Likelihood_Estimation.html",
    "href": "Maximum_Likelihood_Estimation.html",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.\nInstead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#introduction",
    "href": "Maximum_Likelihood_Estimation.html#introduction",
    "title": "Blueprinty Case Study",
    "section": "",
    "text": "Blueprinty is a small firm that develops blueprint software for patent applications submitted to the U.S. Patent Office. Its marketing team wants to show that firms using the software are more successful in obtaining patent approvals. Ideally, this would be evaluated using approval rates before and after adoption, but such data are unavailable.\nInstead, Blueprinty has data on 1,500 mature engineering firms, including the number of patents awarded over the past five years, region, firm age, and software usage. The goal is to assess whether software users receive more patents than non-users."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#objective",
    "href": "Maximum_Likelihood_Estimation.html#objective",
    "title": "Blueprinty Case Study",
    "section": "Objective",
    "text": "Objective\nThe objective of this project is to assess whether engineering firms that use Blueprinty‚Äôs software produce more patents than firms that do not, after accounting for firm age and regional location. The analysis aims to test whether the available data support Blueprinty‚Äôs claim that its software is associated with stronger patent performance."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#data-description",
    "href": "Maximum_Likelihood_Estimation.html#data-description",
    "title": "Blueprinty Case Study",
    "section": "Data Description",
    "text": "Data Description\n\nVariables\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded in the last 5 years (count outcome)\n\n\nregion\nFirm geographic region (categorical: Midwest, Southwest, Northwest, Northeast, etc.)\n\n\nage\nYears since incorporation (continuous)\n\n\niscustomer\nBlueprinty software user indicator (1 = customer, 0 = non-customer)\n\n\n\n\n\nDescriptive comparison: patents by customer status\n\n\n\nCustomer status\nGroup\nMean patents (5 years)\n\n\n\n\n0\nNon-customer\n3.473\n\n\n1\nCustomer\n4.133\n\n\n\nBlueprinty customers have a higher average number of patents awarded over the past five years (4.133) than non-customers (3.473). This difference is descriptive and does not account for other factors such as firm age or region, which are addressed in the regression analysis.\nFigure 1. Distribution of Patent Counts by Blueprinty Usage\n\nThe histogram shows a right-skewed distribution of patent counts, which is typical for count outcomes. Firms using Blueprinty‚Äôs software show a modest rightward shift, suggesting higher patent counts are somewhat more common among customers.\nDescriptively, Blueprinty customers average 4.133 patents over five years versus 3.473 for non-customers. This comparison is correlational: customers are not randomly selected, and differences in firm age or region may explain part of the gap. The regression analysis tests whether the association remains after controlling for these factors.\nFirm age by Blueprinty customer status\n\n\n\niscustomer\nN\nMean age\nMedian age\nSD\n\n\n\n\n0\n1019\n26.10\n25.50\n6.95\n\n\n1\n481\n26.90\n26.50\n7.81\n\n\n\nCustomer and non-customer firms have similar age distributions: customers are slightly older on average (26.90 vs 26.10 years) and have a slightly higher median age (26.5 vs 25.5). Age is therefore included as a control in the regression.\nRegion by Blueprinty customer status (counts)\n\n\n\nRegion\nNon-customer (0)\nCustomer (1)\nTotal\n\n\n\n\nMidwest\n187\n37\n224\n\n\nNortheast\n273\n328\n601\n\n\nNorthwest\n158\n29\n187\n\n\nSouth\n156\n35\n191\n\n\nSouthwest\n245\n52\n297\n\n\nTotal\n1019\n481\n1500\n\n\n\nCustomer status varies by region, with a large share of customers located in the Northeast. Because region is related to adoption, it is controlled for in the regression to reduce confounding."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#poisson-model-specification-and-estimation",
    "href": "Maximum_Likelihood_Estimation.html#poisson-model-specification-and-estimation",
    "title": "Blueprinty Case Study",
    "section": "Poisson Model Specification and Estimation",
    "text": "Poisson Model Specification and Estimation\nWe model the number of patents awarded to each firm over a fixed five-year period. Because patents is a nonnegative count outcome, a Poisson model provides a natural starting point.\nLet (Y_i) denote the number of patents awarded to firm (i) in the last five years. We assume:\n[ Y_i () ]\nwhere () is the expected number of patents per firm over the five-year window.\nFor a sample of (n) independent firms with observed counts (y_1, y_2, , y_n), the log-likelihood function is:\n[ (y_1,,y_n) = -n+ ({i=1}^n y_i)() - {i=1}^n (y_i!) ]\nWe estimate () using maximum likelihood (MLE). This baseline model provides a benchmark before adding firm characteristics (e.g., Blueprinty usage, firm age, and region) in a regression framework.\n\nPoisson log-likelihood and the MLE\nFigure 2. Poisson Log-Likelihood vs.¬†Lambda\n\nThis figure shows the Poisson log-likelihood as a function of \\(\\lambda\\) for the observed patent counts. The log-likelihood is maximized at the sample mean, so the Poisson MLE is:\n\\[\n\\hat{\\lambda} = \\bar{Y}.\n\\]\nFor independent observations \\(y_1,\\ldots,y_n\\), the Poisson log-likelihood is:\n\\[\n\\ell(\\lambda)\n= -n\\lambda\n+ \\left(\\sum_{i=1}^n y_i\\right)\\log(\\lambda)\n- \\sum_{i=1}^n \\log(y_i!).\n\\]\nDifferentiating and setting the derivative to zero gives:\n\\[\n\\frac{\\partial \\ell}{\\partial \\lambda}\n= -n + \\frac{\\sum_{i=1}^n y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n}\\sum_{i=1}^n y_i = \\bar{y}.\n\\]\nIn this dataset, \\(\\hat{\\lambda}=3.6847\\), matching \\(\\bar{y}=3.6847\\) up to rounding."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#poisson-regression-model",
    "href": "Maximum_Likelihood_Estimation.html#poisson-regression-model",
    "title": "Blueprinty Case Study",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nNext, we model five-year patent counts using a Poisson regression. Let \\(Y_i\\) be the number of patents awarded to firm \\(i\\) over the last five years:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\qquad \\log(\\lambda_i) = X_i^\\top \\beta,\n\\]\nequivalently \\(\\lambda_i = \\exp(X_i^\\top \\beta)\\). The covariate vector \\(X_i\\) includes firm age, age squared, region indicators, and Blueprinty usage (iscustomer).\nPoisson regression (GLM) results\n\n\n\nVariable\nCoefficient\nStd. Error\nz\np-value\n\n\n\n\nIntercept\n1.3447\n0.038\n35.059\n&lt;0.001\n\n\nNortheast (vs Midwest)\n0.0292\n0.044\n0.669\n0.504\n\n\nNorthwest (vs Midwest)\n-0.0176\n0.054\n-0.327\n0.744\n\n\nSouth (vs Midwest)\n0.0566\n0.053\n1.074\n0.283\n\n\nSouthwest (vs Midwest)\n0.0506\n0.047\n1.072\n0.284\n\n\nBlueprinty customer (iscustomer)\n0.2076\n0.031\n6.719\n&lt;0.001\n\n\nAge (centered) age_c\n-0.0080\n0.002\n-3.843\n&lt;0.001\n\n\nAge(^2) (centered) age_c2\n-0.0030\n0.000\n-11.513\n&lt;0.001\n\n\n\nNotes: Reference region is Midwest. Age is centered at the sample mean; age_c2 = age_c^2.\nHolding firm age and region constant, Blueprinty customers have higher expected patent counts. The coefficient on iscustomer is 0.2076 (p &lt; 0.001), implying an expected increase of about (e^{0.2076}-1 %) in patents for customers relative to non-customers.\nAge shows a concave pattern: the negative linear and squared centered terms indicate that expected patent output declines with age and falls faster at higher ages (relative to the sample mean). Regional indicators are not statistically significant, suggesting no meaningful regional differences once firm characteristics are controlled for.\nFigure 3. Poisson Regression Coefficients (95% CI)\n\nThe figure shows Poisson regression coefficient estimates with 95% confidence intervals (dashed line at 0 indicates ‚Äúno effect‚Äù on the log scale). The iscustomer estimate is positive and its confidence interval does not cross zero, indicating a statistically significant association between Blueprinty usage and higher expected patent counts. Regional coefficients cluster near zero and their intervals overlap zero, suggesting no meaningful regional differences after controlling for other variables. The negative squared-age term indicates a concave relationship between age and patent output: patenting increases with age but at a diminishing rate.\n\nPoisson Regression Results\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nStd. Error\nz value\np-value\n2.5%\n97.5%\n\n\n\n\nconst\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\nage\n0.1486\n0.0139\n10.7162\n&lt;0.001\n0.1214\n0.1758\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n&lt;0.001\n-0.0035\n-0.0025\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n&lt;0.001\n0.1470\n0.2681\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n\nNote: Coefficients are on the log scale. Exponentiating a coefficient gives the incidence rate ratio (IRR).\nFigure X plots each coefficient estimate with its 95% confidence interval. The dashed vertical line at 0 represents no association on the log scale. Coefficients whose intervals do not cross 0 are statistically distinguishable from 0 at the 5% level.\nThe iscustomer effect is clearly positive and statistically significant, indicating higher expected patent counts for Blueprinty customers after controlling for age and region. Regional effects are small and not statistically significant. The positive age term and negative age_squared term indicate a concave relationship: expected patenting increases with age but the marginal gain declines at higher ages."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#model-implications",
    "href": "Maximum_Likelihood_Estimation.html#model-implications",
    "title": "Blueprinty Case Study",
    "section": "Model Implications",
    "text": "Model Implications\nModel fit and assumptions The Pearson chi-square statistic relative to degrees of freedom is approximately (2070/1492 ), suggesting mild overdispersion. As a robustness check, we report results using robust (sandwich) standard errors; the main conclusion for iscustomer is unchanged.\nFigure 4. Predicted Patents by Blueprinty Usage\n\nThis figure translates the Poisson regression into predicted five-year patent counts at representative firm ages. Across all ages shown, Blueprinty customers are predicted to produce more patents than comparable non-customers (holding other covariates fixed). The gap is largest in the mid-age range, consistent with the nonlinear age pattern captured by the positive age and negative age_squared terms."
  },
  {
    "objectID": "Maximum_Likelihood_Estimation.html#conclusion",
    "href": "Maximum_Likelihood_Estimation.html#conclusion",
    "title": "Blueprinty Case Study",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, these findings reinforce the value of Blueprinty‚Äôs software as a meaningful contributor to patent productivity. At the same time, they highlight the role of firm maturity in shaping innovation outcomes. While regional variation appears limited, the analysis underscores the importance of targeting firms at the right stage of development and aligning product value with their innovation capacity. These insights can guide Blueprinty‚Äôs strategic messaging and outreach efforts, particularly when engaging established firms seeking to strengthen their patent portfolios."
  },
  {
    "objectID": "AB_Testing.html",
    "href": "AB_Testing.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "AB_Testing.html#introduction",
    "href": "AB_Testing.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse."
  },
  {
    "objectID": "AB_Testing.html#objective",
    "href": "AB_Testing.html#objective",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Objective",
    "text": "Objective\nThis project replicates and interprets the findings from Karlan and List‚Äôs (2007) field experiment on charitable giving. Using the original dataset, I examine how the presence and structure of a matching donation offer influence both the likelihood of giving and the amount contributed.\nThis project involves:\n\nAnalyzing treatment effects using t-tests, regression models, and probit analysis\nComparing donation behavior across different match ratios and suggested ask levels\nConducting simulations to illustrate the Law of Large Numbers and the Central Limit Theorem\nPresenting results in a clear, reproducible format using Quarto and Python\n\nThe overall goal is to better understand the behavioral response to charitable incentives and how small changes in message framing can impact donor behavior."
  },
  {
    "objectID": "AB_Testing.html#description-of-the-experiment",
    "href": "AB_Testing.html#description-of-the-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Description of the Experiment",
    "text": "Description of the Experiment\nIn their 2007 study published in the American Economic Review, Dean Karlan and John List conducted a large-scale natural field experiment to test the effectiveness of matching donations in a real-world fundraising campaign. Over 50,000 previous donors to a U.S. nonprofit were mailed fundraising letters and randomly assigned to one of two groups:\n\nControl group: received a standard fundraising letter with no special offer\nTreatment group: received a similar letter but was offered a matching donation, meaning their contribution would be matched by another donor\n\nWithin the treatment group, individuals were further randomized into subgroups based on:\n\nMatch ratio: \\(1\\!:\\!1\\), \\(2\\!:\\!1\\), or \\(3\\!:\\!1\\)\nMaximum match amount: \\(25{,}000\\), \\(50{,}000\\), \\(100{,}000\\), or unstated\nSuggested donation amount: equal to 1.25√ó or 1.5√ó the donor‚Äôs previous contribution\n\nThe randomized design allows for causal analysis of how these variations influence both the decision to donate and the amount given. This experiment provides a powerful example of how field experiments can be used to study economic behavior in natural settings."
  },
  {
    "objectID": "AB_Testing.html#data-description",
    "href": "AB_Testing.html#data-description",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data Description",
    "text": "Data Description\nWe begin by loading the dataset provided by Karlan and List (2007), which contains detailed records from their fundraising field experiment.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nSummary Statistics by Treatment Status\nFigure 1. Donation Rate by Group \nThe treatment group has a higher donation rate (2.20%) than the control group (1.79%). Sample sizes are shown above each bar.\nFigure 2. Average Donation Amount by Group \nAverage donations, including non-donors, are higher in the treatment group ($0.97) than in the control group ($0.81), suggesting that the matching offer increased total giving on average. Sample sizes are shown above each bar."
  },
  {
    "objectID": "AB_Testing.html#balance-test",
    "href": "AB_Testing.html#balance-test",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Balance Test",
    "text": "Balance Test\nTo assess whether the randomization produced comparable treatment and control groups, we tested balance on several pre-treatment characteristics: months since last donation (mrm2), years since first donation, number of prior donations (freq), and a gender indicator (female).\n\nT-Test and Linear Regression for Each Variable\n\n\n\nVariable\nTreatment Coefficient\np-value\n\n\n\n\nmrm2\n0.0137\n0.905\n\n\nyears\n‚àí0.0575\n0.270\n\n\nfreq\n‚àí0.0120\n0.912\n\n\nfemale\n‚àí0.0075\n0.079\n\n\n\nNone of the estimated treatment effects are statistically significant at the 5% level, indicating balance across observable characteristics. These results are consistent with the balance checks reported in Table 1 of Karlan and List (2007).\n\n\nBalance Check for mrm2\n\n\n\nStatistic\nValue\n\n\n\n\nTreatment coefficient\n0.0137\n\n\np-value (t-test)\n0.9049\n\n\np-value (regression)\n0.9050\n\n\n95% confidence interval\n[‚àí0.211, 0.238]\n\n\n\nThe estimated difference in months since last donation between treatment and control groups is small and statistically insignificant, indicating comparable donation recency prior to treatment.\n\n\nAdditional Balance Tests\n\n\nBalance Test ‚Äî Key Estimates\n\n\n\nVariable\nTreatment coef\nt-stat\np-value\n95% CI\n\n\n\n\nmrm2\n0.0137\n0.119\n0.905\n[-0.211, 0.238]\n\n\nyears\n‚àí0.0575\n‚àí1.103\n0.270\n[-0.160, 0.045]\n\n\nfreq\n‚àí0.0120\n‚àí0.111\n0.912\n[-0.224, 0.200]\n\n\nfemale\n‚àí0.0075\n‚àí1.758\n0.079\n[-0.016, 0.001]\n\n\n\nNone of the estimated treatment effects are statistically significant at the 5% level; the gender indicator is borderline (p = 0.079) but the estimated difference is small."
  },
  {
    "objectID": "AB_Testing.html#experimental-results",
    "href": "AB_Testing.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nFigure 3. Donation Rate by Treatment Status \nThe treatment group exhibits a higher donation rate than the control group (2.20% vs.¬†1.79%). The difference is statistically significant at the 1% level.\n\nTreatment Effect on Donation Rates\nWe estimate the effect of the matching donation treatment on the probability of giving using both a two-sample t-test and a bivariate linear regression. Both approaches indicate a statistically significant difference in donation rates between the treatment and control groups.\nThe regression results imply that assignment to the treatment increases the probability of donating by approximately 0.42 percentage points (p = 0.002). Relative to the control group‚Äôs donation rate of about 1.8%, this corresponds to a roughly 22% increase in the likelihood of giving.\nAlthough the absolute effect is small, the estimate is precise and consistent across methods, indicating that offering a matching donation increases participation in charitable giving.\n\n\n\n\n\n\n\n\n\nOutcome: Donated (gave)\nEstimate\np-value\n95% Confidence Interval\n\n\n\n\nTreatment effect\n0.0042\n0.002\n[0.002, 0.007]\n\n\n\n\n\nProbit Regression\nAs a robustness check for the binary outcome, we estimate a Probit model and report the average marginal effect. The treatment increases the probability of donating by 0.43 percentage points (p = 0.002), which closely matches the linear probability model estimate.\n\n\n\nModel\nEffect on Pr(Donate)\np-value\n95% CI\n\n\n\n\nProbit (AME)\n0.0043\n0.002\n[0.002, 0.007]"
  },
  {
    "objectID": "AB_Testing.html#differences-between-match-rates",
    "href": "AB_Testing.html#differences-between-match-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Differences between Match Rates",
    "text": "Differences between Match Rates\nWe examine whether larger match ratios within the treatment group lead to higher donation rates by comparing 1:1, 2:1, and 3:1 matching offers. Pairwise t-tests indicate no statistically significant differences in donation rates across match ratios.\nRelative to a 1:1 match, increasing the ratio to 2:1 raises the donation rate by approximately 0.19 percentage points (p = 0.335), while moving from 2:1 to 3:1 increases the rate by only 0.01 percentage points (p = 0.310). Neither difference is statistically significant.\nThese results suggest that higher match ratios do not meaningfully increase participation beyond the presence of a match itself, consistent with the findings in Karlan and List (2007).\n\n\n\nComparison\nChange in Donation Rate (pp)\np-value\n\n\n\n\n2:1 vs 1:1\n+0.19\n0.335\n\n\n3:1 vs 2:1\n+0.01\n0.310\n\n\n\n\n\nEffect of Match Ratio on Donation Rates\n\n\n\nMatch Ratio\nChange in Donation Rate (pp)\np-value\n\n\n\n\n2:1 vs 1:1\n+0.19\n0.338\n\n\n3:1 vs 1:1\n+0.20\n0.313\n\n\n\n\n\n\nDonation Rates by Match Ratio\nAmong treated individuals, donation rates are similar across match ratios. The response rate is 2.07% under a 1:1 match, 2.26% under a 2:1 match, and 2.27% under a 3:1 match.\nThe increase in donation rates beyond a 1:1 match is small‚Äî0.19 percentage points for a 2:1 match and 0.01 percentage points for a 3:1 match‚Äîand not statistically significant, consistent with the formal tests reported earlier. This pattern aligns with the findings of Karlan and List (2007), which show that increasing the match ratio does not materially increase participation once a match is offered.\n\n\n\nMatch Ratio\nDonation Rate\n\n\n\n\n1:1\n0.0207 (2.07%)\n\n\n2:1\n0.0226 (2.26%)\n\n\n3:1\n0.0227 (2.27%)\n\n\n\n\n\n\nDonation Amount\nWe next examine whether the matching donation treatment affects the amount donated among individuals who chose to give, focusing on the intensive margin of charitable giving.\nA two-sample t-test and a linear regression both estimate a small difference in donation amounts between treatment and control donors; however, the effects are not statistically significant at conventional levels (p ‚âà 0.06). The estimated magnitudes are modest.\nOverall, these results suggest that while offering a matching donation increases participation, it does not meaningfully affect the amount donated conditional on giving.\n\n\nDistribution of Donation Amounts Among Donors\nFigure 4. Distribution of Donation Amounts by Treatment Status  Donation amounts in both the control and treatment groups are right-skewed, with most contributions concentrated at lower values and a small number of large donations. Mean donation amounts are similar across groups ($45.54 for the control group and $43.87 for the treatment group).\n\nFigure 5. Sampling Distribution of Treatment‚ÄìControl Differences (Simulated)  The figure shows the distribution of treatment‚Äìcontrol differences in donation rates across 10,000 simulated experiments with 500 observations per group. The distribution is approximately normal and centered near 0.004, the difference implied by the assumed donation probabilities. The dashed line indicates the mean simulated difference, and the dotted line marks the null of no treatment effect.\n\n\n\nLaw of Large Numbers\nFigure 6. Cumulative estimate of the treatment‚Äìcontrol difference in mean donation amounts\n\nFigure 6 plots the cumulative difference in mean donation amounts between the treatment and control groups as observations are added. When the sample size is small, the estimated difference fluctuates substantially. As the number of observations increases, the cumulative estimate stabilizes and converges toward the full-sample difference, illustrated by the horizontal dashed line.\n\n\n\nSampling Variability and Sample Size\nFigure 7. Sampling distributions of treatment‚Äìcontrol differences by sample size\n\nFigure 7 shows simulated sampling distributions of treatment‚Äìcontrol differences in donation rates for sample sizes of 50, 200, 500, and 1,000. With small samples, the distribution is wide. As sample size increases, the distributions concentrate around their mean, indicating greater precision."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Finance & Business Analytics\nüìß ctweety15@gmail.com üìû 858-888-1353 üìÑ Resume"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nWith a background in business analytics and accounting, I bring hands-on experience analyzing data, identifying process gaps, and supporting data-driven improvements through clear communication and cross-functional collaboration. My work has focused on turning complex, multi-source data into practical insights that support reporting accuracy and informed decision-making.\nIn recent analytics projects, I have analyzed operational and business data to uncover inconsistencies, trends, and improvement opportunities affecting downstream reporting and analysis. I have supported projects end-to-end by clarifying requirements, validating outputs through testing, documenting findings, and presenting results in a clear, actionable way to both technical and non-technical stakeholders. Across roles, I have worked closely with business, analytics, and technical teams to ensure analysis aligned with real user needs and project goals.\nI am comfortable working in ambiguous environments, learning new domains quickly, and balancing multiple priorities. I work regularly with Excel, SQL, Python, Tableau, and Power BI, which allows me to move fluidly between data analysis and business context. My background in client-facing and team-based roles has also strengthened my ability to communicate clearly, collaborate effectively, and contribute thoughtfully to shared outcomes.\nI am currently pursuing an M.S. in Business Analytics at UC San Diego and am seeking internship or full-time opportunities where analytical rigor and practical problem-solving can support real-world business decisions."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego ‚Äî Rady School of Management\nMaster of Science, Business Analytics\nJul 2024 ‚Äì Mar 2026\nNorthern Illinois University\nBachelor of Science, Accountancy\nAug 2021 ‚Äì Aug 2023\nAsia University\nBachelor of Science, Accounting and Information Systems\nSep 2019 ‚Äì Aug 2023"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nHighspring\nContent Analyst Jul 2025 ‚Äì Dec 2025 ¬∑ 6 mos\nHP\nData Analyst\nMar 2025 ‚Äì Jun 2025 ¬∑ 4 mos\nHBC Tax Inc.\nTax Associate\nJan 2023 ‚Äì May 2023 ¬∑ 5 mos\nNorthern Illinois University\nResearch Assistant\nMay 2022 ‚Äì Aug 2022 ¬∑ 4 mos"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "Projects",
    "text": "Projects\n\nPredictive Analysis for Intuit QuickBooks Upgrade\nDatabase Management\nA/B Testing\nMaximum Likelihood Estimation (MLE)\nMultinomial Logit (MNL) & Conjoint Analysis\nKey Drivers Analysis"
  },
  {
    "objectID": "about.html#focus-areas",
    "href": "about.html#focus-areas",
    "title": "About",
    "section": "Focus Areas",
    "text": "Focus Areas\n\nFinancial, business, and operational data analysis\n\nData validation, reconciliation, and reporting accuracy\n\nReporting automation and dashboard development\n\nExperimentation and A/B testing\n\nCommunicating insights to support operational and strategic decisions"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tweety Chang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "Multinomial_Logit_Model.html",
    "href": "Multinomial_Logit_Model.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "Multinomial_Logit_Model.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer‚Äôs decision as the selection of the product that provides the most utility, and we‚Äôll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "Multinomial_Logit_Model.html#simulate-conjoint-data",
    "href": "Multinomial_Logit_Model.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a ‚Äúno choice‚Äù option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom itertools import product\n\nnp.random.seed(123)\n\n# Attribute Levels \nbrand_levels = [\"N\", \"P\", \"H\"]        \nad_levels = [\"Yes\", \"No\"]             \nprice_levels = np.arange(4, 33, 4)    \n\n# All Possible Profiles \nprofiles = pd.DataFrame(\n    list(product(brand_levels, ad_levels, price_levels)),\n    columns=[\"brand\", \"ad\", \"price\"]\n)\n\n# Part-Worth Utilities \nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}        \na_util = {\"Yes\": -0.8, \"No\": 0.0}           \nprice_util = lambda p: -0.1 * p               \n\n# Simulation Parameters \nn_peeps = 100     \nn_tasks = 10      \nn_alts = 3      \n\n# Simulate One Respondent‚Äôs Data \ndef simulate_one(respondent_id: int) -&gt; pd.DataFrame:\n    tasks = []\n\n    for task_no in range(1, n_tasks + 1):\n        dat = profiles.sample(n=n_alts).copy()\n        dat.insert(0, \"task\", task_no)\n        dat.insert(0, \"resp\", respondent_id)\n\n        # Deterministic utility\n        dat[\"v\"] = (\n            dat[\"brand\"].map(b_util)\n            + dat[\"ad\"].map(a_util)\n            + price_util(dat[\"price\"])\n        )\n\n        # Gumbel-distributed noise\n        e = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        dat[\"u\"] = dat[\"v\"] + e\n\n        # Determine choice\n        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n\n        tasks.append(dat)\n\n    return pd.concat(tasks, ignore_index=True)\n\n# Simulate All Respondents\nconjoint_data = pd.concat(\n    [simulate_one(i) for i in range(1, n_peeps + 1)],\n    ignore_index=True\n)\n\n# Keep Only Observable Data \nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Display Result Summary \nrows, cols = conjoint_data.shape\nprint(f\"The simulated dataset contains {rows} rows and {cols} columns \"\n      f\"({n_peeps} respondents √ó {n_tasks} tasks √ó {n_alts} alternatives).\")\n\n# Peek at Data \nprint(conjoint_data.head())\n\nThe simulated dataset contains 3000 rows and 6 columns (100 respondents √ó 10 tasks √ó 3 alternatives).\n   resp  task brand   ad  price  choice\n0     1     1     P  Yes     12       0\n1     1     1     N   No     24       0\n2     1     1     P   No     12       1\n3     1     2     H   No     12       0\n4     1     2     P  Yes     24       0"
  },
  {
    "objectID": "Multinomial_Logit_Model.html#preparing-the-data-for-estimation",
    "href": "Multinomial_Logit_Model.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe ‚Äúhard part‚Äù of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n   resp  task  choice brand   ad  price  brand_N  brand_P  ad_yes  task_id\n0     1     1       1     N  Yes     28        1        0       1        0\n1     1     1       0     H  Yes     16        0        0       1        0\n2     1     1       0     P  Yes     16        0        1       1        0\n3     1     2       0     N  Yes     32        1        0       1        1\n4     1     2       1     P  Yes     16        0        1       1        1\n\n\nEach task_id represents a choice task with three alternatives. For example, task_id = 0 (resp = 1, task = 1) includes three options from brands N, H, and P, all with ads and different prices. The first row (brand = N, price = 28) has choice = 1, indicating it was selected. Dummy variables for brand and ad presence are created, with Hulu and ad-free as baselines. The data is now formatted for model estimation."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#estimation-via-maximum-likelihood",
    "href": "Multinomial_Logit_Model.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\nX = df[[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]].values\ny = df[\"choice\"].values\ngroups = df[\"task_id\"].values\n\n# MNL log-likelihood\ndef mnl_log_likelihood(beta, X, y, task_ids):\n    v = X @ beta\n    exp_v = np.exp(v)\n    df_v = pd.DataFrame({\"task_id\": task_ids, \"exp_v\": exp_v})\n    denom = df_v.groupby(\"task_id\")[\"exp_v\"].transform(\"sum\").values\n    log_probs = v - np.log(denom)\n    return -np.sum(y * log_probs)\n\n# MLE estimation\nbeta_init = np.zeros(X.shape[1])\nresult = minimize(\n    mnl_log_likelihood,\n    beta_init,\n    args=(X, y, groups),\n    method=\"BFGS\"\n)\nbeta_hat = result.x\nse = np.sqrt(np.diag(result.hess_inv))\nz = 1.96\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n# MLE results\nmle_summary = pd.DataFrame({\n    \"MLE Coefficient\": beta_hat,\n    \"Std. Error\": se,\n    \"95% CI Lower\": ci_lower,\n    \"95% CI Upper\": ci_upper\n}, index=[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLE Coefficient\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.941195\n0.109961\n0.725672\n1.156718\n\n\nbrand_P\n0.501616\n0.120088\n0.266242\n0.736989\n\n\nad_yes\n-0.731994\n0.088279\n-0.905021\n-0.558968\n\n\nprice\n-0.099480\n0.006363\n-0.111951\n-0.087010\n\n\n\n\n\n\n\nThe MLE estimates are:\n\nbrand_N: 0.94\n\nbrand_P: 0.50\n\nad_yes: -0.73\n\nprice: -0.099\n\nStandard errors range from 0.006 to 0.12, and all 95% confidence intervals exclude zero. The model converged successfully and parameters were estimated with high precision."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#estimation-via-bayesian-methods",
    "href": "Multinomial_Logit_Model.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Log-prior: N(0, 5^2) for binary vars, N(0, 1^2) for price\ndef log_prior(beta):\n    return -0.5 * (np.sum(beta[:3]**2 / 25) + beta[3]**2)\n\n# Log-posterior: log-likelihood + log-prior\ndef log_posterior(beta, X, y, task_ids):\n    v = X @ beta\n    exp_v = np.exp(v)\n    df_v = pd.DataFrame({\"task_id\": task_ids, \"exp_v\": exp_v, \"v\": v, \"y\": y})\n    denom = df_v.groupby(\"task_id\")[\"exp_v\"].transform(\"sum\").values\n    log_probs = v - np.log(denom)\n    ll = np.sum(y * log_probs)\n    return ll + log_prior(beta)\n\n# MCMC sampler\ndef metropolis_sampler(X, y, task_ids, n_iter=11000, burn_in=1000):\n    n_params = X.shape[1]\n    samples = np.zeros((n_iter, n_params))\n    beta_current = np.zeros(n_params)\n    log_post_current = log_posterior(beta_current, X, y, task_ids)\n\n    for i in range(1, n_iter):\n        beta_proposal = beta_current + np.random.normal(0, proposal_sd)\n        log_post_proposal = log_posterior(beta_proposal, X, y, task_ids)\n        log_accept_ratio = log_post_proposal - log_post_current\n\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta_current = beta_proposal\n            log_post_current = log_post_proposal\n\n        samples[i] = beta_current\n\n    return samples[burn_in:]\n\n# Run sampler and summarize\nposterior_samples = metropolis_sampler(X, y, groups)\n\nposterior_summary = pd.DataFrame({\n    \"Posterior Mean\": posterior_samples.mean(axis=0),\n    \"Posterior Std\": posterior_samples.std(axis=0),\n    \"95% CI Lower\": np.percentile(posterior_samples, 2.5, axis=0),\n    \"95% CI Upper\": np.percentile(posterior_samples, 97.5, axis=0)\n}, index=[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior Mean\nPosterior Std\n95% CI Lower\n95% CI Upper\n\n\n\n\nbrand_N\n0.954482\n0.111946\n0.737799\n1.172539\n\n\nbrand_P\n0.506766\n0.112751\n0.285853\n0.728708\n\n\nad_yes\n-0.732992\n0.088405\n-0.910828\n-0.566985\n\n\nprice\n-0.099719\n0.006344\n-0.112362\n-0.087510\n\n\n\n\n\n\n\nThe posterior means are:\n\nbrand_N: 0.95\n\nbrand_P: 0.50\n\nad_yes: -0.74\n\nprice: -0.100\n\nPosterior standard deviations closely match the MLE standard errors (within ¬±0.001). The 95% credible intervals are nearly identical to the MLE confidence intervals. These results reflect strong agreement between the two methods given the large sample size and weakly informative priors.\n\n\n\n\n\nPosterior Histogram for brand_N\n\n\n\n\nThe posterior histogram is approximately normal and centered near 0.95. The distribution is symmetric and unimodal, with most density between 0.75 and 1.15. This indicates a well-identified parameter with low uncertainty.\n\n\n\n\n\nTrace Plot for brand_N\n\n\n\n\nThe trace plot shows consistent sampling around a stable mean with no visible drift or trends. The values fluctuate tightly across the 10,000 post-burn-in iterations, suggesting good mixing and convergence of the Metropolis-Hastings sampler.\n\n\n\n\n\n\nSimulating the Data\n\n\n\n\n\n\nbayes_summary = posterior_summary.copy()\nbayes_summary.columns = [\"Bayes Mean\", \"Bayes Std. Dev.\", \"Bayes CI Lower\", \"Bayes CI Upper\"]\n\nmle_summary.columns = [\"MLE Mean\", \"MLE Std. Error\", \"MLE CI Lower\", \"MLE CI Upper\"]\n\ncomparison_df = pd.concat([bayes_summary, mle_summary], axis=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Mean\nBayes Std. Dev.\nBayes CI Lower\nBayes CI Upper\nMLE Mean\nMLE Std. Error\nMLE CI Lower\nMLE CI Upper\n\n\n\n\nbrand_N\n0.954482\n0.111946\n0.737799\n1.172539\n0.941195\n0.109961\n0.725672\n1.156718\n\n\nbrand_P\n0.506766\n0.112751\n0.285853\n0.728708\n0.501616\n0.120088\n0.266242\n0.736989\n\n\nad_yes\n-0.732992\n0.088405\n-0.910828\n-0.566985\n-0.731994\n0.088279\n-0.905021\n-0.558968\n\n\nprice\n-0.099719\n0.006344\n-0.112362\n-0.087510\n-0.099480\n0.006363\n-0.111951\n-0.087010\n\n\n\n\n\n\n\nThe Bayesian posterior means closely align with the MLE point estimates across all four parameters. Differences are minimal:\n\nbrand_N: Posterior mean is 0.95; MLE is 0.94. Both intervals fully overlap.\nbrand_P: Posterior mean is 0.51 vs.¬†MLE of 0.50.\nad_yes: Both methods estimate the effect around -0.73, with nearly identical uncertainty.\nprice: Posterior mean is -0.0997; MLE is -0.0995, with near-identical intervals.\n\nPosterior standard deviations match MLE standard errors within 0.002 across all parameters. Given the large sample and weak priors, the Bayesian and MLE results are functionally equivalent and reinforce the same conclusions: consumers prefer Netflix and Prime, dislike ads, and are price sensitive."
  },
  {
    "objectID": "Multinomial_Logit_Model.html#discussion",
    "href": "Multinomial_Logit_Model.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf this were real data, the parameter estimates would reveal consumers‚Äô underlying preferences.\n\nŒ≤_Netflix &gt; Œ≤_Prime implies that, all else equal, consumers prefer Netflix to Prime Video. This reflects a higher perceived utility from Netflix‚Äôs offering.\nŒ≤_price &lt; 0 is consistent with economic theory: as price increases, utility decreases, making the product less likely to be chosen.\n\nThe direction and magnitude of all estimates are reasonable. Consumers dislike ads, show brand preferences, and are price-sensitive‚Äîpatterns expected in digital subscription markets.\nTo simulate or estimate a multi-level MNL model, we must account for individual differences in preferences. Unlike the standard MNL model, which assumes a single set of coefficients shared by all respondents, a hierarchical model allows each respondent to have their own set of parameters.\n\nKey changes: Instead of using one fixed Œ≤, assign each respondent a personal Œ≤‚Çñ drawn from a population distribution: [ _i (, ) ]\nEstimation:\nEstimate both the respondent-level Œ≤·µ¢s and the population-level parameters (Œº, Œ£). This requires hierarchical modeling techniques, often implemented via Bayesian MCMC or mixed logit estimation.\n\nThis approach better reflects real-world data by capturing preference heterogeneity across individuals."
  },
  {
    "objectID": "Key_Drivers_Analysis.html",
    "href": "Key_Drivers_Analysis.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "Introduction\nThis analysis applies the K-Means clustering algorithm to the Palmer Penguins dataset, using two key morphological measurements: bill length and flipper length. The goal is to explore whether natural groupings of penguins can be uncovered without using species labels, and how well these groupings align with biological categories.\nUnlike supervised learning, clustering is an unsupervised machine learning method, meaning it identifies structure in the data without relying on labeled outcomes. K-Means is one of the most common techniques for this purpose, partitioning observations into k clusters based on feature similarity.\nIn this project, we implement K-Means both from scratch and using scikit-learn, visualize the clustering process across iterations, compare our custom implementation to the built-in version, evaluate different values of k using WCSS and silhouette scores to identify the optimal number of clusters.\nThis exercise provides both technical practice in unsupervised learning and biological insight into how penguin species differ morphologically.\n\n\nPalmer Penguins\nThe Palmer Penguins dataset provides biological measurements for three penguin species native to islands in the Palmer Archipelago, Antarctica: Ad√©lie, Chinstrap, and Gentoo.\nEach row in the dataset corresponds to a single penguin and includes variables such as:\n\nspecies: species name (Ad√©lie, Chinstrap, or Gentoo)\nisland: island where the penguin was observed\nbill_length_mm: bill length (mm)\nbill_depth_mm: bill depth (mm)\nflipper_length_mm: flipper length (mm)\nbody_mass_g: body mass (g)\nsex: biological sex (if known)\nyear: year of observation\n\nFor this analysis, we focus on two numerical features: Bill Length (bill_length_mm) , Flipper Length (flipper_length_mm)\nThese measurements are used as inputs to clustering algorithms to explore natural groupings in the data without relying on species labels.\n\n\n\n\n\n\nData\n\n\n\n\n\n\npenguins = pd.read_csv(\"/Users/qqtweety/Downloads/mgta495/palmer_penguins.csv\")\npenguins.head()\n\npenguins_clean = penguins[['bill_length_mm', 'flipper_length_mm']].dropna().reset_index(drop=True)\nX = penguins_clean.values\n\n\n\n\n\n\n\n\n\n\nDef plot clusters\n\n\n\n\n\n\ndef plot_clusters(X, centroids, labels, title):\n    plt.figure(figsize=(8, 6))\n    for i in range(np.max(labels) + 1):\n        plt.scatter(X[labels == i, 0], X[labels == i, 1], label=f'Cluster {i+1}')\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n    plt.xlabel('Bill Length (mm)')\n    plt.ylabel('Flipper Length (mm)')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef kmeans_custom(X, k=3, max_iters=10, plot_each_step=True):\n    np.random.seed(42)\n    initial_idx = np.random.choice(X.shape[0], k, replace=False)\n    centroids = X[initial_idx]\n\n    for iteration in range(max_iters):\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        if plot_each_step:\n            plot_clusters(X, centroids, labels, f'Custom KMeans - Iteration {iteration + 1}')\n\n        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n\n    return centroids, labels\n\n\n\n\n\n# Run the custom KMeans and visualize\ncustom_centroids, custom_labels = kmeans_custom(X, k=3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Run the built-in KMeans for comparison\nsk_model = KMeans(n_clusters=3, random_state=42)\nsk_labels = sk_model.fit_predict(X)\nsk_centroids = sk_model.cluster_centers_\n\n# Plot result from sklearn\nplot_clusters(X, sk_centroids, sk_labels, \"Built-in KMeans Final Result\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow centroid comparison\n\n\n\n\n\n\nprint(\"Custom Centroids:\\n\", custom_centroids)\nprint(\"\\nBuilt-in Centroids:\\n\", sk_centroids)\n\nCustom Centroids:\n [[ 38.45304348 187.05217391]\n [ 47.6296     216.92      ]\n [ 45.95483871 196.7311828 ]]\n\nBuilt-in Centroids:\n [[ 39.56013986 188.13986014]\n [ 48.14375    219.29166667]\n [ 46.49680851 201.76595745]]\n\n\n\n\n\nInterpretation of Results\nThe custom and built-in KMeans algorithms produced comparable clustering outcomes. The centroids differed by only 1‚Äì5 mm per dimension, confirming convergence to similar solutions.\nThe built-in method placed some centroids slightly farther along the flipper-length axis, likely due to finer convergence or sensitivity to outliers.\nCluster interpretation suggests: Cluster 1 groups penguins with shorter bills and flippers, likely Ad√©lie. Cluster 2 includes those with the longest bills and flippers, likely Gentoo. Cluster 3 covers the intermediate range, possibly Chinstrap.\nDespite random initialization, the small Euclidean distances between corresponding centroids reflect stable clustering across both implementations.\n\n\n\n\n\n\nWCSS\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n# Store metrics for different K values\nks = range(2, 8)\nwcss = []  # within-cluster sum of squares (inertia)\nsilhouette_scores = []\n\nfor k in ks:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)\n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\n\n\n\n\n\nWCSS\n\n# Plot WCSS and Silhouette Score\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nax1.plot(ks, wcss, marker='o')\nax1.set_title('Within-Cluster Sum of Squares (WCSS)')\nax1.set_xlabel('Number of Clusters (k)')\nax1.set_ylabel('WCSS')\nax2.plot(ks, silhouette_scores, marker='o')\nax2.set_title('Silhouette Score')\nax2.set_xlabel('Number of Clusters (k)')\nax2.set_ylabel('Silhouette Score')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nK-Means Clustering\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Fit KMeans for K=3\nkmeans_3 = KMeans(n_clusters=3, random_state=42)\nlabels_3 = kmeans_3.fit_predict(X)\ncentroids_3 = kmeans_3.cluster_centers_\n\n# Plotting function\ndef plot_kmeans_clusters(X, labels, centroids):\n    plt.figure(figsize=(8, 6))\n    colors = ['red', 'gold', 'magenta']\n    for i in range(3):\n        plt.scatter(X[labels == i, 0], X[labels == i, 1], s=10, color=colors[i], alpha=0.7)\n        plt.scatter(centroids[i, 0], centroids[i, 1], c='black', s=100, edgecolors='black', linewidths=1.5)\n    plt.title('K-Means Clustering (K = 3)')\n    plt.xlabel('Bill Length (mm)')\n    plt.ylabel('Flipper Length (mm)')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\nplot_kmeans_clusters(X, labels_3, centroids_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaw values\n\n\n\n\n\n\n# Return the raw values for interpretation\nlist(zip(ks, wcss, silhouette_scores))\n\n[(2, 20949.785311278196, np.float64(0.6117940477662409)),\n (3, 14269.555284121907, np.float64(0.45576101830851007)),\n (4, 9587.135276652694, np.float64(0.4448839684032104)),\n (5, 7597.607576867576, np.float64(0.4104355873447603)),\n (6, 6326.305140616324, np.float64(0.4137288893863444)),\n (7, 6030.078872777223, np.float64(0.36988077481143466))]\n\n\n\n\n\nEvaluation of Cluster Quantity\nTo determine the optimal number of clusters, we computed the within-cluster sum of squares (WCSS) and silhouette scores for K ranging from 2 to 7.\nMetric Summary\n\n\n\nK\nWCSS\nSilhouette Score\n\n\n\n\n2\n20949.79\n0.612\n\n\n3\n14269.56\n0.456\n\n\n4\n9587.14\n0.445\n\n\n5\n7597.61\n0.410\n\n\n6\n6326.31\n0.414\n\n\n7\n6030.08\n0.370\n\n\n\nInterpretation\n\nWCSS decreases with higher K, as expected, showing improved compactness.\nSilhouette score peaks at K=2, indicating the clearest separation between clusters.\nFrom K=3 onward, silhouette values steadily decline, suggesting diminishing cluster quality.\n\nRecommended K\nK = 2 offers the best trade-off between compactness and separation. It is the most natural grouping for this dataset based on these metrics.\n\n\n\n\n\n\n\nSet seed and parameters\n\n\n\n\n\n\nnp.random.seed(42)\nn = 100\n# Generate features\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\n# Define boundary and outcome\nboundary = np.sin(4 * x1) + x1\ny = (x2 &gt; boundary).astype(int)\n# Create DataFrame\ndat = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndat.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-0.752759\n-2.811425\n0\n\n\n1\n2.704286\n0.818462\n0\n\n\n2\n1.391964\n-1.113864\n0\n\n\n3\n0.591951\n0.051424\n0\n\n\n4\n-2.063888\n2.445399\n1\n\n\n\n\n\n\n\n\n\n\nSynthetic Dataset for K-Nearest Neighbors\nWe generated a synthetic dataset with two features, x1 and x2, uniformly drawn from [-3, 3]. The binary outcome variable y is assigned based on a non-linear boundary:\n\\[\ny =\n\\begin{cases}\n1 & \\text{if } x_2 &gt; \\sin(4x_1) + x_1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThis creates a wiggly decision boundary ideal for testing non-linear classifiers like KNN.\nExample Records\n\n\n\nx1\nx2\ny\n\n\n\n\n-0.75\n-2.81\n0\n\n\n2.70\n0.82\n0\n\n\n1.39\n-1.11\n0\n\n\n0.59\n0.05\n0\n\n\n-2.06\n2.45\n1\n\n\n\n\n\n\n\n\n\nVisualization\n\n\n\n\n\n\n# Plot the synthetic data with decision boundary\nplt.figure(figsize=(8, 6))\ncolors = ['red' if label == 0 else 'blue' for label in dat['y']]\nplt.scatter(dat['x1'], dat['x2'], c=colors, alpha=0.7, edgecolor='k', label='Data points')\nx1_sorted = np.linspace(-3, 3, 300)\nboundary_curve = np.sin(4 * x1_sorted) + x1_sorted\nplt.plot(x1_sorted, boundary_curve, color='black', linestyle='--', label='True decision boundary')\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Synthetic Data and True Decision Boundary\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Data for KNN Evaluation\nA test dataset of 100 points was generated using uniform random values for x1 and x2, drawn from the range [-3, 3]. The binary label y was assigned based on a non-linear boundary: \\[\ny =\n\\begin{cases}\n1 & \\text{if } x_2 &gt; \\sin(4x_1) + x_1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThis boundary creates a wavy separation ideal for testing flexible classifiers like K-Nearest Neighbors.\n\n\n\n\n\n\ntest dataset\n\n\n\n\n\n\n# Generate a test dataset using a different random seed\nnp.random.seed(24)\nn_test = 100\n\n# Generate test features\nx1_test = np.random.uniform(-3, 3, n_test)\nx2_test = np.random.uniform(-3, 3, n_test)\n\n# Apply the same decision boundary rule\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = pd.Series((x2_test &gt; boundary_test).astype(int), dtype='category')\n\n# Create the test DataFrame\ntest_dat = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})\ntest_dat.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n2.760104\n1.614976\n0\n\n\n1\n1.197072\n-0.152494\n0\n\n\n2\n2.999204\n0.114679\n0\n\n\n3\n-1.679596\n2.665158\n1\n\n\n4\n-0.833662\n0.861152\n1\n\n\n\n\n\n\n\n\n\n\n\n\nSample Data\n\n\n\nx1\nx2\ny\n\n\n\n\n2.76\n1.61\n0\n\n\n1.20\n-0.15\n0\n\n\n2.99\n0.11\n0\n\n\n-1.68\n2.67\n1\n\n\n-0.83\n0.86\n1\n\n\n\n\n\n\n\n\n\nPlot\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\n\n# Color-coded test points\nfor class_value, color, label in zip([0, 1], ['red', 'blue'], ['y = 0', 'y = 1']):\n    subset = test_dat[test_dat['y'] == class_value]\n    plt.scatter(subset['x1'], subset['x2'], c=color, label=label, edgecolor='k', s=50)\n\n# Plot the true decision boundary\nx1_vals = np.linspace(-3, 3, 300)\nboundary_curve = np.sin(4 * x1_vals) + x1_vals\nplt.plot(x1_vals, boundary_curve, 'k--', label='True boundary')\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Test Dataset with True Decision Boundary\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual KNN Implementation and Validation\n\n\n\n\n\n\nKNN implementation\n\n\n\n\n\n\nfrom collections import Counter\n\n# Step-by-step manual KNN implementation (k=5)\ndef knn_predict(X_train, y_train, X_test, k=5):\n    predictions = []\n    for x in X_test:\n        # Compute Euclidean distances\n        distances = np.linalg.norm(X_train - x, axis=1)\n        # Get indices of k nearest neighbors\n        nn_indices = distances.argsort()[:k]\n        # Get the most common class among nearest neighbors\n        nn_labels = y_train[nn_indices]\n        most_common = Counter(nn_labels).most_common(1)[0][0]\n        predictions.append(most_common)\n    return np.array(predictions)\n\n# Prepare data\nX_train = dat[['x1', 'x2']].values\ny_train = dat['y'].astype(int).values\nX_test = test_dat[['x1', 'x2']].values\ny_test = test_dat['y'].astype(int).values\n\n# Manual prediction\nmanual_preds = knn_predict(X_train, y_train, X_test, k=5)\n\n# Check accuracy of manual KNN\nmanual_accuracy = np.mean(manual_preds == y_test)\n\n# Compare to sklearn's KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors=5)\nknn_model.fit(X_train, y_train)\nsklearn_preds = knn_model.predict(X_test)\nsklearn_accuracy = np.mean(sklearn_preds == y_test)\n\nmanual_accuracy, sklearn_accuracy\n\n(np.float64(0.92), np.float64(0.92))\n\n\n\n\n\nWe implemented the K-Nearest Neighbors algorithm from scratch using Euclidean distance and a majority vote among the k = 5 nearest neighbors.\nTo verify the implementation, we compared it with KNeighborsClassifier from scikit-learn.\nAccuracy Comparison\n\n\n\nMethod\nAccuracy\n\n\n\n\nManual KNN\n92%\n\n\nSklearn KNN\n92%\n\n\n\nBoth methods produced identical results, confirming the correctness of the custom implementation. This validates our understanding of the KNN algorithm and the reproducibility of results using both custom and built-in tools.\n\n\n\nTuning K in K-Nearest Neighbors\n\n\n\n\n\n\nManual KNN function\n\n\n\n\n\n\ndef knn_predict(X_train, y_train, X_test, k=5):\n    predictions = []\n    for x in X_test:\n        distances = np.linalg.norm(X_train - x, axis=1)\n        nn_indices = distances.argsort()[:k]\n        nn_labels = y_train[nn_indices]\n        most_common = Counter(nn_labels).most_common(1)[0][0]\n        predictions.append(most_common)\n    return np.array(predictions)\n\n# Define training and test sets\nX_train = dat[['x1', 'x2']].values\ny_train = dat['y'].astype(int).values\nX_test = test_dat[['x1', 'x2']].values\ny_test = test_dat['y'].astype(int).values\n\n# Compute accuracy for k = 1 to 30\nk_values = range(1, 31)\naccuracies = [np.mean(knn_predict(X_train, y_train, X_test, k) == y_test) for k in k_values]\n\n\n\n\nWe evaluated classification accuracy on the test dataset for k = 1 to k = 30 using our custom KNN function.\n\n\n\n\n\n\nTest accuracy of manual KNN for k = 1 to 30\n\n\n\n\n\n\nplt.plot(k_values, accuracies, marker='o')\nplt.title(\"KNN Accuracy on Test Data for k = 1 to 30\")\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nYogurt Data\nThe yogurt_data dataset contains information on consumer yogurt purchases. Each row represents an individual shopping trip, including the quantity of yogurt purchased and prices for five different yogurt brands.\nKey variables include:\n\nid: unique identifier for each customer\nchoice: the brand chosen during the shopping trip\nprice1 to price5: prices of the five competing yogurt brands\nquantity: number of yogurt units purchased\n\nThis dataset is commonly used for analyzing consumer choice behavior, price sensitivity, and brand preference using models like multinomial logit or KNN classification.\n\n\n\n\n\n\nData\n\n\n\n\n\n\nyogurt = pd.read_csv(\"/Users/qqtweety/Downloads/mgta495/yogurt_data.csv\")\nyogurt.head()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReshape dataset to long format suitable for LC-MNL\n\n\n\n\n\n\n# Create long-format structure\nlong_df = pd.DataFrame()\n\n# Loop through 4 alternatives per observation\nfor i in range(1, 5):\n    temp = pd.DataFrame({\n        'id': yogurt['id'],\n        'alt': i,\n        'choice': yogurt[f'y{i}'],\n        'feature': yogurt[f'f{i}']\n    })\n    long_df = pd.concat([long_df, temp], ignore_index=True)\n\n# Sort by id and alt for clarity\nlong_df = long_df.sort_values(by=['id', 'alt']).reset_index(drop=True)\nlong_df.head(8)\n\n\n\n\n\n\n\n\nid\nalt\nchoice\nfeature\n\n\n\n\n0\n1\n1\n0\n0\n\n\n1\n1\n2\n0\n0\n\n\n2\n1\n3\n0\n0\n\n\n3\n1\n4\n1\n0\n\n\n4\n2\n1\n0\n0\n\n\n5\n2\n2\n1\n0\n\n\n6\n2\n3\n0\n0\n\n\n7\n2\n4\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutput\n\n\n\n\n\n\nfrom scipy.special import logsumexp\nfrom scipy.optimize import minimize\n# Prepare data\ndf = long_df.copy()\nn_alts = df['alt'].nunique()\nn_obs = df['id'].nunique()\n\n# Pivot to get feature matrix per choice set\nX = df.pivot(index='id', columns='alt', values='feature').values  # shape: (n_obs, 4)\nY = df.pivot(index='id', columns='alt', values='choice').values   # shape: (n_obs, 4)\n\n# Log-likelihood function for latent-class MNL with 2 segments\ndef latent_class_loglik(params):\n    beta1, beta2, lamb = params[:1], params[1:2], params[2]\n    pi1 = np.exp(lamb) / (1 + np.exp(lamb))\n    pi2 = 1 - pi1\n\n    # Utilities for each segment\n    V1 = X * beta1\n    V2 = X * beta2\n\n    # Choice probabilities\n    P1 = np.exp(V1 - logsumexp(V1, axis=1, keepdims=True))\n    P2 = np.exp(V2 - logsumexp(V2, axis=1, keepdims=True))\n\n    # Segment-level likelihoods\n    LL1 = np.sum(Y * np.log(P1 + 1e-12), axis=1)\n    LL2 = np.sum(Y * np.log(P2 + 1e-12), axis=1)\n\n    # Full log-likelihood (marginalizing over segments)\n    LL = np.log(pi1 * np.exp(LL1) + pi2 * np.exp(LL2) + 1e-12)\n    return -np.sum(LL)\n\n# Initial parameter guess: [Œ≤1, Œ≤2, Œª]\ninit_params = np.array([0.1, 0.1, 0.0])\nresult = minimize(latent_class_loglik, init_params, method='BFGS')\n\nestimated_params = result.x\nbeta1_hat, beta2_hat, lambda_hat = estimated_params[0], estimated_params[1], estimated_params[2]\npi1_hat = np.exp(lambda_hat) / (1 + np.exp(lambda_hat))\npi2_hat = 1 - pi1_hat\n\nbeta1_hat, beta2_hat, pi1_hat, pi2_hat\n\n(np.float64(0.8010362006166869),\n np.float64(0.8010362006166869),\n np.float64(0.5),\n np.float64(0.5))\n\n\n\n\n\n\n\nLatent-Class MNL Estimation\nWe estimate a two-segment latent-class MNL model using the Yogurt dataset. Each segment has its own utility sensitivity parameter (Œ≤), and segment membership is probabilistic via a logistic transformation of Œª:\n[ _1 = , _2 = 1 - _1 ]\nThe unconditional choice probability is a weighted sum over segment-specific MNL probabilities:\n[ P_i(j) = _{s=1}^{2} _s P_i(j s) ]\n\n\n\n\n\n\nCreate long-format structure\n\n\n\n\n\n\nlong_data = pd.DataFrame()\n\nfor i in range(1, 5):  # 4 products\n    temp = pd.DataFrame({\n        'id': yogurt['id'],\n        'alt': i,\n        'choice': yogurt[f'y{i}'],\n        'feature': yogurt[f'f{i}'],\n        'price': yogurt[f'p{i}']\n    })\n    long_data = pd.concat([long_data, temp], ignore_index=True)\n\nlong_data = long_data.sort_values(['id', 'alt']).reset_index(drop=True)\nlong_data.head(8)\n\n\n\n\n\n\n\n\nid\nalt\nchoice\nfeature\nprice\n\n\n\n\n0\n1\n1\n0\n0\n0.108\n\n\n1\n1\n2\n0\n0\n0.081\n\n\n2\n1\n3\n0\n0\n0.061\n\n\n3\n1\n4\n1\n0\n0.079\n\n\n4\n2\n1\n0\n0\n0.108\n\n\n5\n2\n2\n1\n0\n0.098\n\n\n6\n2\n3\n0\n0\n0.064\n\n\n7\n2\n4\n0\n0\n0.075\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrepare matrices\n\n\n\n\n\n\nX_price = long_data.pivot(index='id', columns='alt', values='price').values\nX_feature = long_data.pivot(index='id', columns='alt', values='feature').values\nY = long_data.pivot(index='id', columns='alt', values='choice').values\n\n# Log-likelihood function for LC-MNL with 2 variables per segment\ndef log_likelihood(params):\n    beta1 = params[0:2]   # [price, feature] for segment 1\n    beta2 = params[2:4]   # [price, feature] for segment 2\n    lamb = params[4]\n\n    pi1 = np.exp(lamb) / (1 + np.exp(lamb))\n    pi2 = 1 - pi1\n\n    # Utilities for each segment\n    V1 = beta1[0] * X_price + beta1[1] * X_feature\n    V2 = beta2[0] * X_price + beta2[1] * X_feature\n\n    # Choice probabilities\n    P1 = np.exp(V1 - logsumexp(V1, axis=1, keepdims=True))\n    P2 = np.exp(V2 - logsumexp(V2, axis=1, keepdims=True))\n\n    # Segment-level likelihood\n    LL1 = np.sum(Y * np.log(P1 + 1e-12), axis=1)\n    LL2 = np.sum(Y * np.log(P2 + 1e-12), axis=1)\n\n    LL_total = np.log(pi1 * np.exp(LL1) + pi2 * np.exp(LL2) + 1e-12)\n    return -np.sum(LL_total)\n\n# Initial guess: [Œ≤1_price, Œ≤1_feature, Œ≤2_price, Œ≤2_feature, lambda]\ninit = np.array([-10, 1, -10, 1, 0])  # negative price sensitivity, positive feature\n\n# Estimate\nresult = minimize(log_likelihood, init, method='BFGS')\nbeta1_est = result.x[0:2]\nbeta2_est = result.x[2:4]\nlambda_est = result.x[4]\npi1_est = np.exp(lambda_est) / (1 + np.exp(lambda_est))\npi2_est = 1 - pi1_est\n\nbeta1_est, beta2_est, pi1_est, pi2_est\n\n(array([12.91301699, 14.64227543]),\n array([  9.18211689, -12.17223547]),\n np.float64(0.42723347735078737),\n np.float64(0.5727665226492127))\n\n\n\n\n\n\n\nLatent-Class Multinomial Logit (LC-MNL)\nThe yogurt dataset includes anonymized consumer IDs (id), binary indicators of purchased products (y1‚Äìy4), whether products were advertised in-store (f1‚Äìf4), and price-per-ounce values (p1‚Äìp4).\nEach row represents a consumer‚Äôs single purchase occasion across four yogurt options. For example, consumer 1 selected yogurt 4 at a price of 0.079/oz, with no items advertised. Consumers 2‚Äì7 all chose yogurt 2.\nTo estimate a latent-class multinomial logit model, the dataset was reshaped from wide to long format, creating one row per consumer‚Äìproduct pair. Key variables used were: - choice: 1 if the product was selected, 0 otherwise - price: per-ounce price - feature: whether the item was advertised\nWe estimated a two-segment LC-MNL model using price and feature as predictors of utility.\nSegment-Level Estimates\n\n\n\nSegment\nŒ≤price\nŒ≤feature\nœÄ (Share)\n\n\n\n\n1\n12.91\n14.64\n42.7%\n\n\n2\n9.18\n‚àí12.17\n57.3%\n\n\n\nInterpretation\n\nSegment 1 is highly responsive to both lower prices and in-store advertising.\nSegment 2 exhibits lower price sensitivity and reacts negatively to advertising.\nSegment proportions (42.7% vs.¬†57.3%) indicate heterogeneous preference patterns among consumers.\n\nThese results suggest the presence of two distinct market segments: one promotion-sensitive, the other resistant to marketing signals."
  }
]